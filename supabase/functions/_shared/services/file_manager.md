# FileManagerService

## Overview

The `FileManagerService` is a critical component within the `supabase/functions/_shared/` directory. Its primary purpose is to provide a centralized and standardized way to manage all file operations related to the Dialectic feature. This service ensures that:

1.  **Consistent File Pathing**: All files are stored in Supabase Storage using a predefined, logical directory structure. This structure is essential for organization, future export features, and potentially for direct user interaction with the file system (e.g., via GitHub integration). The paths are generated by the `constructStoragePath` utility, driven by the `PathContext`.
2.  **Database Synchronization**: Every file uploaded to storage has a corresponding metadata record created in the appropriate database table (`dialectic_project_resources` or `dialectic_contributions`). This link is vital for file discoverability, managing permissions (via RLS on the tables), and associating files with their respective projects, sessions, stages, and AI contributions.
3.  **Simplified File Operations**: It abstracts the complexities of direct Supabase Storage calls and database inserts/updates related to file metadata, providing a simpler API for other backend services.
4.  **Atomic Operations (Best Effort)**: When registering a file, it first uploads to storage and then creates the database record. If the database insert fails, it attempts to clean up the orphaned file from storage.

Using `FileManagerService` for all file-related tasks is **mandatory** to maintain data integrity and consistency across the application.

## Core Concepts

### `FileType`

An enum that defines the type of file being handled. This type dictates the storage path construction and the target database table for metadata.

```typescript
export enum FileType {
  // General Project Files
  ProjectReadme = 'project_readme',
  InitialUserPrompt = 'initial_user_prompt',
  ProjectSettingsFile = 'project_settings_file',
  GeneralResource = 'general_resource',
  ProjectExportZip = 'project_export_zip',

  // User-Managed Workflow Files
  PendingFile = 'pending_file',
  CurrentFile = 'current_file',
  CompleteFile = 'complete_file',
  UserFeedback = 'user_feedback',

  // Core Generation Artifacts
  ModelContributionMain = 'model_contribution_main',
  ModelContributionRawJson = 'model_contribution_raw_json',
  ContributionDocument = 'contribution_document',
  SeedPrompt = 'seed_prompt',
  
  // Document-Centric Generation Artifacts
  PlannerPrompt = 'planner_prompt',
  TurnPrompt = 'turn_prompt',
  HeaderContext = 'header_context',
  AssembledDocumentJson = 'assembled_document_json',
  RenderedDocument = 'rendered_document',
  
  // Intermediate Synthesis Artifacts
  PairwiseSynthesisChunk = 'pairwise_synthesis_chunk',
  ReducedSynthesis = 'reduced_synthesis',
  Synthesis = 'synthesis',
  RagContextSummary = 'rag_context_summary',

  // Thesis Document Keys
  business_case = 'business_case',
  feature_spec = 'feature_spec',
  technical_approach = 'technical_approach',
  success_metrics = 'success_metrics',

  // Antithesis Document Keys
  business_case_critique = 'business_case_critique',
  technical_feasibility_assessment = 'technical_feasibility_assessment',
  risk_register = 'risk_register',
  non_functional_requirements = 'non_functional_requirements',
  dependency_map = 'dependency_map',
  comparison_vector = 'comparison_vector',

  // Synthesis Document Keys
  header_context_pairwise = 'header_context_pairwise',
  synthesis_pairwise_business_case = 'synthesis_pairwise_business_case',
  synthesis_pairwise_feature_spec = 'synthesis_pairwise_feature_spec',
  synthesis_pairwise_technical_approach = 'synthesis_pairwise_technical_approach',
  synthesis_pairwise_success_metrics = 'synthesis_pairwise_success_metrics',
  synthesis_document_business_case = 'synthesis_document_business_case',
  synthesis_document_feature_spec = 'synthesis_document_feature_spec',
  synthesis_document_technical_approach = 'synthesis_document_technical_approach',
  synthesis_document_success_metrics = 'synthesis_document_success_metrics',
  SynthesisHeaderContext = 'synthesis_header_context',
  prd = 'prd',
  system_architecture_overview = 'system_architecture',
  tech_stack_recommendations = 'tech_stack',

  // Parenthesis Document Keys
  trd = 'trd',
  master_plan = 'master_plan',
  milestone_schema = 'milestone_schema',

  // Paralysis Document Keys
  updated_master_plan = 'updated_master_plan',
  actionable_checklist = 'actionable_checklist',
  advisor_recommendations = 'advisor_recommendations',
}
```

### `PathContext`

An object providing the necessary context to construct a unique and correct storage path for a file.

```typescript
export interface PathContext {
  branchKey?: string | null;
  parallelGroup?: number | null;
  projectId: string;
  fileType: FileType;
  sessionId?: string;
  iteration?: number;
  stageSlug?: string;
  contributionType?: ContributionType | null;
  modelSlug?: string;
  attemptCount?: number;
  originalFileName?: string;
  sourceModelSlugs?: string[];
  sourceAnchorType?: string;
  sourceAnchorModelSlug?: string;
  sourceAttemptCount?: number;
  pairedModelSlug?: string;
  isContinuation?: boolean;
  turnIndex?: number;
  documentKey?: string;
  stepName?: string;
}
```

### `UploadContext`

An object containing all information needed by `uploadAndRegisterFile` to store a file and its metadata.

```typescript
export interface UploadContext {
  pathContext: PathContext;
  fileContent: Buffer | ArrayBuffer | string;
  mimeType: string;
  sizeBytes: number;
  userId: string | null;
  description: string;
  resourceTypeForDb?: string;
  contributionMetadata?: ContributionMetadata;
  feedbackTypeForDb?: string;
  resourceDescriptionForDb?: Json | null;
}

export interface ContributionMetadata {
  sessionId: string;
  modelIdUsed: string; // FK to ai_providers.id
  modelNameDisplay: string; // For dialectic_contributions.model_name
  stageSlug: string;
  iterationNumber: number;
  rawJsonResponseContent: Json; // The actual JSON string content for the raw AI response.
  target_contribution_id?: string;
  document_relationships?: Json | null;
  isIntermediate?: boolean;
  tokensUsedInput?: number;
  tokensUsedOutput?: number;
  processingTimeMs?: number;
  source_prompt_resource_id?: string; // Path to the prompt that generated this contribution
  citations?: Json | null;
  contributionType?: ContributionType | null;
  errorDetails?: string | null;
  promptTemplateIdUsed?: string | null;
  editVersion?: number;
  isLatestEdit?: boolean;
  originalModelContributionId?: string | null;
  isContinuation?: boolean;
  turnIndex?: number;
}
```

### File Tree Generated by File Manager (Document-Centric)

The following structure represents the target state for the document-centric architecture.

```
{repo_root}/
└── {project_name_slug}/
    ├── project_readme.md      (Optional high-level project description, goals, defined by user or initial setup, *Generated at project finish, not start, not yet implemented*)
    ├── {user_prompt}.md (the initial prompt submitted by the user to begin the project generated by createProject, whether provided as a file or text string, *Generated at project start, implemented*)
    ├── project_settings.json (The json object includes keys for the dialectic_domain row, dialectic_process_template, dialectic_stage_transitions, dialectic_stages, dialectic_process_associations, domain_specific_prompt_overlays, and system_prompt used for the project where the key is the table and the value is an object containing the values of the row, *Generated on project finish, not project start, not yet implemented*)
    ├── {export_project_file}.zip (a zip file of the entire project for the user to download generated by exportProject)
    ├── general_resource (all optional)
    │    ├── `{deployment_context}` (where/how the solution will be implemented), 
    │    ├── `{domain_standards}` (domain-specific quality standards and best practices), 
    │    ├── `{success_criteria}` (measurable outcomes that define success), 
    │    ├── `{constraint_boundaries}` (non-negotiable requirements and limitations), 
    │    ├── `{stakeholder_considerations}` (who will be affected and how),
    │    ├── `{reference_documents}` (user-provided reference materials and existing assets), 
    │    └── `{compliance_requirements}` (regulatory, legal, or organizational compliance mandates)    
    ├── Pending/          (System-managed folder populated as the final step of the Paralysis stage)
    │   └── ...                     (When the user begins their work, they move the first file they're going to work on from Pending to Current)
    ├── Current/          (User-managed folder for the file they are actively working on for this project)
    │   └── ...                     (This is the file the user is currently working on, drawn from Pending)
    ├── Complete/         (User-managed folder for the files they have already completed for this project)       
    │   └── ...                     (When the user finishes all the items in the Current file, they move it to Complete, and move the next Pending file into Current)
    └── session_{session_id_short}/  (Each distinct run of the dialectic process)
        └── iteration_{N}/        (N being the iteration number, e.g., "iteration_1")
            ├── 1_thesis/
            │   ├── _work/
            │   │   ├── prompts/
            │   │   │   ├── {model_slug}_{n}_planner_prompt.md
            │   │   │   └── {model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            │   │   ├── context/
            │   │   │   └── {model_slug}_{n}_header_context.json
            │   │   └── assembled_json/
            │   │       └── {model_slug}_{n}_{document_key}_assembled.json
            │   ├── raw_responses/
            │   │   ├── {model_slug}_{n}_planner_raw.json
            │   │   └── {model_slug}_{n}_{document_key}[_continuation_{c}]_raw.json
            │   ├── documents/
            │   │   └── {model_slug}_{n}_{document_key}.md
            │   ├── seed_prompt.md
            │   └── user_feedback_thesis.md
            ├── 2_antithesis/
            │   ├── _work/
            │   │   ├── prompts/
            │   │   │   └── {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            │   │   ├── context/
            │   │   │   └── {model_slug}_critiquing_{source_model_slug}_{n}_header_context.json
            │   │   └── assembled_json/
            │   │       └── {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}_assembled.json
            │   ├── raw_responses/
            │   │   └── {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}[_continuation_{c}]_raw.json
            │   ├── documents/
            │   │   └── {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}.md
            │   ├── seed_prompt.md
            │   └── user_feedback_antithesis.md
            └── ... (Structure repeats for 3_synthesis, 4_parenthesis, 5_paralysis)
```

## Public Methods

### `constructor(supabaseClient: SupabaseClient<Database>)`

Initializes a new instance of the `FileManagerService`.

*   **Parameters**:
    *   `supabaseClient`: An initialized Supabase client instance.
*   **Throws**:
    *   Error if the `SB_CONTENT_STORAGE_BUCKET` environment variable is not set.

### `async uploadAndRegisterFile(context: UploadContext): Promise<FileManagerResponse>`

Uploads a file to Supabase Storage and creates a corresponding metadata record in either `dialectic_project_resources` or `dialectic_contributions` table based on `context.pathContext.fileType`. If `context.pathContext.fileType` is `'model_contribution_main'` and `context.contributionMetadata.rawJsonResponseContent` is provided, it will also attempt to upload the raw JSON response to a corresponding path.

*   **Parameters**:
    *   `context`: `UploadContext` - The full context for the file upload.
*   **Returns**: `Promise<FileManagerResponse>`
    *   `FileManagerResponse`: An object `{ record: YourTableRecord | null, error: { message: string, details?: string } | null }`.
        *   `record`: The newly created database record from `dialectic_project_resources` or `dialectic_contributions`.
        *   `error`: An error object if the operation failed at any stage (storage upload, database insert).

### `async getFileSignedUrl(fileId: string, table: 'dialectic_project_resources' | 'dialectic_contributions' | 'dialectic_feedback'): Promise<{ signedUrl: string | null; error: Error | null }>`

Retrieves a temporary, short-lived signed URL for a file stored in Supabase Storage, allowing secure, direct client-side access (e.g., for downloads or viewing).

*   **Parameters**:
    *   `fileId`: `string` - The UUID of the file record in the database.
    *   `table`: `'dialectic_project_resources' | 'dialectic_contributions' | 'dialectic_feedback'` - The table where the file's metadata is stored.
*   **Returns**: `Promise<{ signedUrl: string | null; error: Error | null }>`
    *   `signedUrl`: The temporary URL if successful.
    *   `error`: An error object if the file record is not found or URL generation fails.

### `async assembleAndSaveFinalDocument(rootContributionId: string): Promise<{ finalPath: string | null; error: Error | null }>`

Assembles a chain of contribution chunks into a single final document and uploads it, overwriting the content of the root contribution file. This method is crucial for completing "continuation" jobs where a model generates a long document in several parts.

It works by:
1.  Fetching the root contribution to identify the session.
2.  Fetching all contributions for that session.
3.  Traversing the chain of contributions starting from the root, ordering them via the `target_contribution_id` field.
4.  Downloading the content of each chunk in order.
5.  Concatenating the content into a single string.
6.  Uploading the final string to the storage path of the original root contribution, effectively replacing the initial chunk with the complete document.

*   **Parameters**:
    *   `rootContributionId`: `string` - The UUID of the very first contribution record in the continuation chain.
*   **Returns**: `Promise<{ finalPath: string | null; error: Error | null }>`
    *   `finalPath`: The storage path of the final, assembled document if successful.
    *   `error`: An error object if any part of the process fails (fetching, downloading, concatenating, or uploading).

## Architecture and Testing

To ensure the `FileManagerService` is reliable and easy to test, its implementation follows the Dependency Inversion Principle.

### `IFileManager` Interface

The core of the architecture is the `IFileManager` interface (defined in `supabase/functions/_shared/types/file_manager.types.ts`). This interface defines the public contract for any file manager implementation.

```typescript
// From: supabase/functions/_shared/types/file_manager.types.ts
export interface IFileManager {
  uploadAndRegisterFile(context: UploadContext): Promise<FileManagerResponse>;
  assembleAndSaveFinalDocument(rootContributionId: string): Promise<{ finalPath: string | null; error: Error | null; }>;
  getFileSignedUrl(fileId: string, table: 'dialectic_project_resources' | 'dialectic_contributions' | 'dialectic_feedback'): Promise<{ signedUrl: string | null; error: Error | null }>;
}
```

By depending on this abstraction rather than the concrete `FileManagerService` class, other services can easily swap the real implementation with a mock for testing purposes.

### `MockFileManagerService`

For unit testing services that use the file manager, a mock implementation, `MockFileManagerService`, is provided in `supabase/functions/_shared/services/file_manager.mock.ts`.

This mock class implements the `IFileManager` interface. Its methods are Deno `spy` objects, which allows for:

1.  **Configuring Return Values**: You can set up the mock to return specific success or error responses for different test scenarios.
2.  **Asserting Calls**: You can verify that the file manager's methods were called with the correct arguments and the expected number of times.

#### Example Usage in a Test:

```typescript
// In a test file (e.g., executeModelCallAndSave.test.ts)
import { MockFileManagerService } from '../_shared/services/file_manager.mock.ts';
import { assertEquals } from 'https://deno.land/std/testing/asserts.ts';

Deno.test('some function should call assembleAndSaveFinalDocument on stop', async () => {
  // 1. Setup
  const fileManager = new MockFileManagerService();
  const deps = { fileManager }; // Dependencies for the function under test

  const rootId = 'some-root-id';
  // ... logic to call your service/function which should trigger the assembly
  await myFunctionThatUsesFileManager(deps, rootId);

  // 2. Assert
  const assembleSpy = fileManager.assembleAndSaveFinalDocument;
  assertEquals(assembleSpy.calls.length, 1);
  assertEquals(assembleSpy.calls[0].args[0], rootId);
});
```

## Usage Examples

It's crucial that other backend services (Edge Functions) use `FileManagerService` for any operation that involves writing files related to the Dialectic feature.

### 1. Project Creation (`createProject.ts`)

When a user creates a new project and optionally uploads an initial prompt file:

```typescript
// In supabase/functions/dialectic-service/createProject.ts

// ... (setup, payload parsing)
if (promptFile) {
  const fileManager = new FileManagerService(dbAdminClient);
  const fileBuffer = await promptFile.arrayBuffer();

  const uploadResult = await fileManager.uploadAndRegisterFile({
    pathContext: {
      projectId: newProjectData.id,
      fileType: 'initial_user_prompt',
      originalFileName: promptFile.name,
    },
    fileContent: Buffer.from(fileBuffer),
    mimeType: promptFile.type,
    sizeBytes: promptFile.size,
    userId: user.id,
    description: 'Initial project prompt file',
  });

  if (uploadResult.error || !uploadResult.record) {
    // Handle error, potentially roll back project creation
    console.error("Error uploading initial prompt file:", uploadResult.error);
    // ...
  } else {
    // Link uploadResult.record.id to dialectic_projects.initial_prompt_resource_id
  }
}
// ...
```

### 2. Session Start (`startSession.ts`)

When a new Dialectic session is initiated, the fully assembled seed prompt is saved using the `FileManagerService` via the `PromptAssembler` service.

```typescript
// In supabase/functions/dialectic-service/startSession.ts (Conceptual)

// The PromptAssembler is now responsible for handling prompt construction and persistence.
const promptAssembler = new PromptAssembler(dbClient, { fileManager });

// It is called to create and save the seed prompt artifact.
const assembledPrompt = await promptAssembler.assembleSeedPrompt(
    project,
    newSessionRecord,
    stageContext,
    initialUserPrompt,
    1 // iteration number
);

if (assembledPrompt.error) {
    // Handle error
}

// The resulting `source_prompt_resource_id` is then passed along to the next
// step in the process, which will eventually use it in `executeModelCallAndSave`.
const sourcePromptResourceId = assembledPrompt.source_prompt_resource_id;

// ... proceed to call the dialectic-worker with the job payload ...
```

### 3. Saving AI Model Contributions (`executeModelCallAndSave.ts`)

When an AI model generates content for a stage, `executeModelCallAndSave` in the `dialectic-worker` uses the `FileManagerService` to save the output.

```typescript
// In supabase/functions/dialectic-worker/executeModelCallAndSave.ts (simplified)

// ... (after receiving aiResponse from callUnifiedAIModel)
const fileManager = new FileManagerService(dbClient, { constructStoragePath });

const contributionUpload = await fileManager.uploadAndRegisterFile({
  pathContext: {
    projectId: sessionDetails.project_id,
    sessionId: sessionDetails.id,
    iteration: currentIterationNumber,
    stageSlug: stage.slug,
    modelSlug: sanitizeForPath(providerDetails.api_identifier || providerDetails.name),
    fileType: 'model_contribution_main',
    // ... other path properties like documentKey
  },
  fileContent: aiResponse.content, // The main Markdown/text output
  mimeType: aiResponse.contentType || 'text/markdown',
  sizeBytes: new TextEncoder().encode(aiResponse.content).length,
  userId: null, // AI contributions are system-generated
  description: `Contribution for stage ${stage.slug} from model ${providerDetails.name}`,
  contributionMetadata: {
    sessionId: sessionDetails.id,
    rawJsonResponseContent: aiResponse.rawProviderResponse,
    modelIdUsed: modelIdForCall,
    modelNameDisplay: providerDetails.name,
    stageSlug: stage.slug,
    iterationNumber: currentIterationNumber,
    tokensUsedInput: aiResponse.inputTokens,
    tokensUsedOutput: aiResponse.outputTokens,
    processingTimeMs: aiResponse.processingTimeMs,
    // The link to the prompt resource is now passed directly.
    source_prompt_resource_id: promptConstructionPayload.source_prompt_resource_id,
    // ... other metadata like citations, contributionType, etc.
  }
});

if (contributionUpload.error || !contributionUpload.record) {
  // Handle failed contribution saving
} else {
  // `contributionUpload.record` is the new dialectic_contributions row.
  // The raw JSON is uploaded by FileManagerService itself if rawJsonResponseContent is provided.
}
// ...
```

### 4. Cloning a Project (Conceptual) (`cloneProject.ts`)

When cloning a project, `FileManagerService` would be used to duplicate each file from the source project into the new project's structure, creating new database records for each. The logic for handling contributions must be updated to copy the prompt relationship.

```typescript
// In supabase/functions/dialectic-service/cloneProject.ts (conceptual)

// ... (create new project record for `newProjectId`)
const fileManager = new FileManagerService(dbClient, { constructStoragePath });

// ... (Loop for cloning dialectic_project_resources remains the same)

// 4. Repeat for `dialectic_contributions`
for (const originalContrib of sourceContributions) {
    // ... (download original contribution content)

    // When re-uploading, the contribution metadata must be updated.
    const newContributionMetadata: ContributionMetadata = {
        // ... (copy metadata like modelIdUsed, stageSlug, etc.)
        
        // Instead of rebuilding a seed_prompt_url, we now copy the foreign key
        // to the *newly cloned* prompt resource. This requires mapping old resource IDs to new ones.
        source_prompt_resource_id: idMap.get(originalContrib.source_prompt_resource_id),

        // ... (other metadata)
    };

    await fileManager.uploadAndRegisterFile({
      pathContext: {
        projectId: newProjectId,
        sessionId: newSessionId, // A new session ID for the cloned project
        // ... other new path context
      },
      // ... file content and other properties
      contributionMetadata: newContributionMetadata,
    });
}
// ...
```

## Important Considerations

*   **Central Authority**: Treat `FileManagerService` as the sole authority for creating file records and determining storage paths. Directly manipulating Supabase Storage or the file metadata tables (`dialectic_project_resources`, `dialectic_contributions`) outside of this service will lead to inconsistencies and bugs.
*   **Environment Variables**: The service relies on `SB_CONTENT_STORAGE_BUCKET` being correctly set in your Supabase project's environment variables.
*   **Error Handling**: Always check the `error` field in the response from `uploadAndRegisterFile`. Implement appropriate rollback or compensation logic in the calling service if an error occurs (e.g., deleting a newly created project record if its initial prompt file fails to upload).
*   **Database Schema**: The `FileManagerService` assumes the structure of `dialectic_project_resources` and `dialectic_contributions` tables matches the fields it attempts to populate. Ensure your database migrations align with these expectations (see `types_db.ts`).
*   **Testing**: When writing unit tests for services that depend on file operations, always inject `MockFileManagerService` to isolate your tests from the actual database and storage.

By adhering to the `FileManagerService`, developers can ensure a robust, maintainable, and consistent file management system for the Dialectic feature. 