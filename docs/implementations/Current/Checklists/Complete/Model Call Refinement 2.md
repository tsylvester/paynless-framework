# Model Call Refinement 2: Complex Job Processing

This document provides a complete, verified, and end-to-end implementation plan for complex job management logic into the AI chat service. This feature will allow the system to handle requests that are too large for the input to fix in the model's input window, or too complex for the model to complete in a single pass.

## Legend

*   `[ ]` 1. Unstarted work step. Each work step will be uniquely named for easy reference. We begin with 1.
    *   `[ ]` 1.a. Work steps will be nested as shown. Substeps use characters, as is typical with legal documents.
        *   `[ ]` 1. a. i. Nesting can be as deep as logically required, using roman numerals, according to standard legal document numbering processes.
*   `[âœ…]` Represents a completed step or nested set.
*   `[ðŸš§]` Represents an incomplete or partially completed step or nested set.
*   `[â¸ï¸]` Represents a paused step where a discovery has been made that requires backtracking or further clarification.
*   `[â“]` Represents an uncertainty that must be resolved before continuing.
*   `[ðŸš«]` Represents a blocked, halted, or stopped step or has an unresolved problem or prior dependency to resolve before continuing.

## Component Types and Labels

The implementation plan uses the following labels to categorize work steps:

*   `[DB]` Database Schema Change (Migration)
*   `[RLS]` Row-Level Security Policy
*   `[BE]` Backend Logic (Edge Function / RLS / Helpers / Seed Data)
*   `[API]` API Client Library (`@paynless/api` - includes interface definition in `interface.ts`, implementation in `adapter.ts`, and mocks in `mocks.ts`)
*   `[STORE]` State Management (`@paynless/store` - includes interface definition, actions, reducers/slices, selectors, and mocks)
*   `[UI]` Frontend Component (e.g., in `apps/web`, following component structure rules)
*   `[CLI]` Command Line Interface component/feature
*   `[IDE]` IDE Plugin component/feature
*   `[TEST-UNIT]` Unit Test Implementation/Update
*   `[TEST-INT]` Integration Test Implementation/Update (API-Backend, Store-Component, RLS)
*   `[TEST-E2E]` End-to-End Test Implementation/Update
*   `[DOCS]` Documentation Update (READMEs, API docs, user guides)
*   `[REFACTOR]` Code Refactoring Step
*   `[PROMPT]` System Prompt Engineering/Management
*   `[CONFIG]` Configuration changes (e.g., environment variables, service configurations)
*   `[COMMIT]` Checkpoint for Git Commit (aligns with "feat:", "test:", "fix:", "docs:", "refactor:" conventions)
*   `[DEPLOY]` Checkpoint for Deployment consideration after a major phase or feature set is complete and tested.

---

## File Structure for Supabase Storage and Export Tools

{repo_root}/  (Root of the user's GitHub repository)
â””â”€â”€ {project_name_slug}/
    â”œâ”€â”€ project_readme.md      (Optional high-level project description, goals, defined by user or initial setup, *Generated at project finish, not start, not yet implemented*)
    â”œâ”€â”€ {user_prompt}.md (the initial prompt submitted by the user to begin the project generated by createProject, whether provided as a file or text string, *Generated at project start, implemented*)
    â”œâ”€â”€ project_settings.json (The json object includes keys for the dialectic_domain row, dialectic_process_template, dialectic_stage_transitions, dialectic_stages, dialectic_process_associations, domain_specific_prompt_overlays, and system_prompt used for the project where the key is the table and the value is an object containing the values of the row, *Generated on project finish, not project start, not yet implemented*)
    â”œâ”€â”€ {export_project_file}.zip (a zip file of the entire project for the user to download generated by exportProject)
    â”œâ”€â”€ general_resource (all optional)
    â”‚    â”œâ”€â”€ `{deployment_context}` (where/how the solution will be implemented), 
    â”‚    â”œâ”€â”€ `{domain_standards}` (domain-specific quality standards and best practices), 
    â”‚    â”œâ”€â”€ `{success_criteria}` (measurable outcomes that define success), 
    â”‚    â”œâ”€â”€ `{constraint_boundaries}` (non-negotiable requirements and limitations), 
    â”‚    â”œâ”€â”€ `{stakeholder_considerations}` (who will be affected and how),
    â”‚    â”œâ”€â”€ `{reference_documents}` (user-provided reference materials and existing assets), 
    â”‚    â””â”€â”€ `{compliance_requirements}` (regulatory, legal, or organizational compliance mandates)    
    â”œâ”€â”€ Pending/          (System-managed folder populated as the final step of the Paralysis stage)
    â”‚   â””â”€â”€ ...                     (When the user begins their work, they move the first file they're going to work on from Pending to Current)
    â”œâ”€â”€ Current/          (User-managed folder for the file they are actively working on for this project)
    â”‚   â””â”€â”€ ...                     (This is the file the user is currently working on, drawn from Pending)
    â”œâ”€â”€ Complete/         (User-managed folder for the files they have already completed for this project)       
    â”‚   â””â”€â”€ ...                     (When the user finishes all the items in the Current file, they move it to Complete, and move the next Pending file into Current)
    â””â”€â”€ session_{session_id_short}/  (Each distinct run of the dialectic process)
        â””â”€â”€ iteration_{N}/        (N being the iteration number, e.g., "iteration_1")
            â”œâ”€â”€ 1_thesis/
            â”‚   â”œâ”€â”€ raw_responses
            â”‚   â”‚   â””â”€â”€{model_name_slug}_{n}_thesis_raw.json
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_name_slug}_{n}_thesis.md (Contains YAML frontmatter + AI response, appends a count so a single model can provide multiple contributions)
            â”‚   â”œâ”€â”€ ... (other models' hypothesis outputs)
            â”‚   â”œâ”€â”€ user_feedback_hypothesis.md   (User's feedback on this stage)
            â”‚   â””â”€â”€ documents/                      (Optional refined documents, e.g., PRDs from each model)
            â”‚       â””â”€â”€ (generated from .json object located at Database['dialectic_stages']['row']['expected_output_artifacts'])
            â”œâ”€â”€ 2_antithesis/
            â”‚   â”œâ”€â”€ raw_responses
            â”‚       â””â”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_antithesis_raw.json
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_antithesis.md
            â”‚   â”œâ”€â”€ ...
            â”‚   â”œâ”€â”€ user_feedback_antithesis.md
            â”‚   â””â”€â”€ documents/                    (Optional refined documents, e.g., PRDs from each model)
            â”‚       â””â”€â”€ (generated from .json object located at Database['dialectic_stages']['row']['expected_output_artifacts'])                
            â”œâ”€â”€ 3_synthesis/
            â”‚   â”œâ”€â”€ _work/          (Storage for intermediate, machine-generated artifacts that are not final outputs)
            â””â”€â”€ raw_responses/
            â”‚   â”œâ”€â”€ {model_slug}_from_{source_model_slugs}_{n}_pairwise_synthesis_chunk_raw.json
            â”‚   â”œâ”€â”€ {model_slug}_reducing_{source_contribution_id_short}_{n}_reduced_synthesis_raw.json
            â”‚   â””â”€â”€ {model_slug}_{n}_final_synthesis_raw.json
            â”œâ”€â”€ _work/
            â”‚   â”œâ”€â”€ {model_slug}_from_{source_model_slugs}_{n}_pairwise_synthesis_chunk.md
            â”‚   â””â”€â”€ {model_slug}_reducing_{source_contribution_id_short}_{n}_reduced_synthesis.md
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_{n}_final_synthesis.md
            â”‚   â”œâ”€â”€ ...
            â”‚   â”œâ”€â”€ user_feedback_synthesis.md
            â”‚   â””â”€â”€ documents/                      (Optional refined documents, e.g., PRDs from each model)
            â”‚       â””â”€â”€ (generated from .json object located at Database['dialectic_stages']['row']['expected_output_artifacts'])
            â”œâ”€â”€ 4_parenthesis/
            â”‚   â”œâ”€â”€ raw_responses
            â”‚   â”‚   â””â”€â”€{model_name_slug}_{n}_{stage_slug}_raw.json
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_name_slug}_{n}_{stage_slug}.md
            â”‚   â”œâ”€â”€ ...
            â”‚   â”œâ”€â”€ user_feedback_parenthesis.md
            â”‚   â””â”€â”€ documents/                      (Optional refined documents, e.g., PRDs from each model)
            â”‚       â””â”€â”€ (generated from .json object located at Database['dialectic_stages']['row']['expected_output_artifacts'])
            â””â”€â”€ 5_paralysis/
                â”œâ”€â”€ raw_responses
                â”‚   â””â”€â”€{model_name_slug}_{n}_{stage_slug}_raw.json
                â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
                â”œâ”€â”€ {model_name_slug}_{n}_{stage_slug}.md
                â”œâ”€â”€ ...
                â””â”€â”€ documents/                      (Optional refined documents, e.g., PRDs from each model)
                    â””â”€â”€ (generated from .json object located at Database['dialectic_stages']['row']['expected_output_artifacts'])

---

### Phase 9: [REFACTOR] Implement Tiered Context Window Management

This phase implements the critical, tiered strategy for managing model context window limitations without resorting to naive summarization, as defined in the "Computable Determinant" architecture.

#### 25. [BE] [DB] Create and Integrate Prerequisite "Combination" Job Logic

*   `[âœ…]` 25.a. **Enhance Jobs Table for Prerequisite Tracking:**
    *   `[DB]` **Action:** In a new migration, add a `prerequisite_job_id` (nullable, UUID, foreign key to `dialectic_generation_jobs.id`) to the `dialectic_generation_jobs` table.
    *   `[DB]` **Action:** Add a new job status: `'waiting_for_prerequisite'`.
*   `[âœ…]` 25.b. **Implement Combination Prompt Strategy:**
    *   `[BE]` `[PROMPT]` **Action:** In the `system_prompts` table, create a new entry for the combination job. It should be along the lines of: *"You are a document synthesis agent. Combine the following documents into a single, coherent text. You must preserve every unique fact, requirement, argument, and detail. Eliminate only redundant phrasing or conversational filler."*
*   `[âœ…]` 25.c. **[REFACTOR] Implement a Modular, Reusable Model Execution Utility:**
    *   `[âœ…]` 25.c.i. **Goal:** To refactor the shared logic for calling AI models and saving results into a single, reusable utility, adhering to DRY and SRP principles.
    *   `[âœ…]` 25.c.ii. **Create Generic `executeModelCallAndSave` Utility:**
        *   `[BE]` `[TEST-UNIT]` **(RED)** Create a new test file: `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`. Write a failing test that calls a mock of this utility, providing a prepared prompt and context. Assert that the AI adapter is called and that the `FileManager` is invoked to save the result.
        *   `[BE]` `[REFACTOR]` **(GREEN)** Create the file: `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`. Move the common logic from `processSimpleJob.ts` (the part that calls `callUnifiedAIModel`, handles the response, and then calls the `FileManager` to save the contribution) into this new function.
        *   `[TEST-UNIT]` **(PROVE)** Prove the unit tests for the new utility pass.
    *   `[âœ…]` 25.c.iii. **Refactor `processSimpleJob` into a "Preparer":**
        *   `[BE]` `[TEST-UNIT]` **(RED)** Update the tests in `processSimpleJob.test.ts`. They should no longer assert that the AI adapter or `FileManager` are called directly. Instead, they should assert that `processSimpleJob` correctly performs its setup (using `PromptAssembler` with stage recipes) and then calls the new `executeModelCallAndSave` utility with the correctly prepared parameters.
        *   `[BE]` `[REFACTOR]` **(GREEN)** Refactor `processSimpleJob.ts`. Remove the logic that was just moved and replace it with a call to the new utility. Its sole responsibility is now preparing the context for a stage-based job.
        *   `[TEST-UNIT]` **(PROVE)** Prove the refactored `processSimpleJob` tests pass.
    *   `[âœ…]` 25.c.iv. **Implement `processCombinationJob` as a "Preparer":**
        *   `[BE]` `[TEST-UNIT]` **(RED)** Create a new test file: `supabase/functions/dialectic-worker/processCombinationJob.test.ts`. Write a failing test that asserts this new function correctly prepares the context for a utility combination job (fetches the specific 'Tier 2 Document Combiner' prompt, gets documents from the payload) and then calls `executeModelCallAndSave`.
        *   `[BE]` `[REFACTOR]` **(GREEN)** Create the file: `supabase/functions/dialectic-worker/processCombinationJob.ts`. Implement the simple setup logic and have it call the shared `executeModelCallAndSave` utility.
        *   `[TEST-UNIT]` **(PROVE)** Prove the `processCombinationJob` tests pass.
    *   `[âœ…]` 25.c.v. **Update the Main Worker Router:**
        *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/processJob.ts`.
        *   `[BE]` `[REFACTOR]` **Action:** The main router must be updated to delegate to the new `processCombinationJob` module when a job with `job_type: 'combine'` is received.
*   `[âœ…]` 25.d. **Implement Orchestration for Prerequisite Jobs:**
    *   `[DB]` **File:** The `handle_child_job_completion()` PostgreSQL function.
    *   `[DB]` `[REFACTOR]` **Action:** The trigger function must be enhanced to be a more generic `handle_job_completion()` orchestrator. When a job completes, it must check both of the following:
        1.  **Parent/Child:** Does this job have a `parent_job_id`? If so, check if all siblings are complete to wake the parent.
        2.  **Prerequisite/Waiting:** Does any other job list this job's `id` in its `prerequisite_job_id` field? If so, update the waiting job's status from `'waiting_for_prerequisite'` to `'pending'`.

#### 26. [BE] [REFACTOR] Integrate Tiered Context Window Management

This step implements a hybrid, two-layer defense mechanism to manage model context windows, ensuring both efficient orchestration and absolute safety against API errors. The strategy involves an initial check at the "planning" stage and a final validation at the "execution" stage.

*   `[âœ…]` 26.a. **Create `ContextWindowError` Custom Error:**
    *   `[BE]` **File:** `supabase/functions/_shared/utils/errors.ts`.
    *   `[BE]` **Action:** Create and export a new custom error class, `ContextWindowError`, that extends the base `Error` class. This allows for specific `catch` blocks in the worker logic.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/_shared/utils/errors.ts
        export class ContextWindowError extends Error {
          constructor(message: string) {
            super(message);
            this.name = 'ContextWindowError';
          }
        }
        ```

*   `[âœ…]` 26.b. **Implement Tier 2 Orchestration in the Complex Job Planner:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/task_isolator.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** The `planComplexStage` function will be modified to perform a pre-emptive check on the collected source documents *before* it attempts to plan child jobs.
    *   **Logic:**
        1.  **Location:** The new logic will be placed after the `validSourceDocuments` array is populated.
        2.  **Token Estimation:**
            *   Import `countTokens` from `supabase/functions/_shared/utils/tokenizer_utils.ts`.
            *   Fetch the `ai_providers` record for the `parentJob.payload.model_id` to get its `max_input_tokens` and `tokenization_strategy`.
            *   Map the `validSourceDocuments` to the `MessageForTokenCounting` format (`[{ role: 'user', content: doc.content }]`).
            *   Calculate the `estimatedTokens` for the entire collection of documents.
        3.  **Tier 1 (Context is OK):** If `estimatedTokens` is less than `max_input_tokens`, the function proceeds with its existing logic to plan and return child jobs.
        4.  **Tier 2 (Context is Too Large):** If `estimatedTokens` exceeds `max_input_tokens`:
            *   The function will create a *new* prerequisite job.
            *   **New Job Payload:**
                *   `job_type`: `'combine'`
                *   `payload`: Contains the `resource_ids` of all documents in `validSourceDocuments`. This gives the `processCombinationJob` worker the information it needs to fetch the content later.
                *   Inherit necessary fields from the `parentJob` (e.g., `sessionId`, `projectId`, `user_id`).
            *   **Database Operations:**
                *   Use `dbClient` to `insert` the new "combine" job into `dialectic_generation_jobs`.
                *   Use `dbClient` to `update` the original `parentJob`, setting its `status` to `'waiting_for_prerequisite'` and `prerequisite_job_id` to the ID of the new combine job.
            *   The function will then log this action and `return []`, halting the planning process for the current parent job until the prerequisite is met.

*   `[âœ…]` 26.c. **Implement Final Validation in the Model Executor:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** This function will be modified to act as the final safeguard, validating the token count of the *fully rendered prompt* just before making the API call.
    *   **Logic:**
        1.  **Location:** The new logic will be placed immediately before the call to `deps.callUnifiedAIModel`.
        2.  **Token Calculation:**
            *   Import `countTokens` from `supabase/functions/_shared/utils/tokenizer_utils.ts`.
            *   Construct the `MessageForTokenCounting` array from the `renderedPrompt.content` and `previousContent`.
            *   Calculate the `finalTokenCount` using the `providerDetails` (which is an `AiModelExtendedConfig`).
        3.  **Validation:**
            *   If `finalTokenCount` exceeds `providerDetails.max_input_tokens`, the function will `throw new ContextWindowError(...)` with a detailed message. This is a hard failure, as the context is too large even after potential planning and combination. The check should account for a small buffer if necessary (e.g., `max_input_tokens * 0.98`).

*   `[âœ…]` 26.d. **Update Worker Error Handling:**
    *   `[BE]` `[REFACTOR]` **Files:** `supabase/functions/dialectic-worker/processSimpleJob.ts` and `supabase/functions/dialectic-worker/processComplexJob.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** The top-level `try/catch` block in these worker files must be updated to specifically handle the `ContextWindowError`.
    *   **Logic:**
        1.  Add `import { ContextWindowError } from '../_shared/utils/errors.ts';`.
        2.  Inside the `catch (e)` block, add a specific check: `if (e instanceof ContextWindowError) { ... }`.
        3.  **On Catch:**
            *   Log the specific error.
            *   Update the job's status to `'failed'`.
            *   Set the `error_details` field with a clear message explaining that the context window was exceeded and no strategy could resolve it.
            *   Use the `notificationService` to dispatch a user-facing failure notification. This ensures the user is informed about why the job could not be completed.

---

### Phase 10: [REFACTOR] Implement Reusable, Database-Driven Step Prompts

This phase refactors the worker to use pre-defined, reusable prompt templates for each step within a complex stage, making the system more modular and easier to maintain.

#### 27. [DB] [BE] Formalize Step Recipes and Prompts

*   `[âœ…]` 27.a. **Create Database Migration/Seed File:**
    *   `[DB]` **Action:** Create a new migration or seed file.
    *   `[DB]` **Action:** Seed the `system_prompts` table with specific, reusable prompt templates for each step of a complex stage.
        *   **Example 1 (`synthesis_step1_pairwise`):** *"As an expert synthesizer, your task is to analyze the following user prompt, an original thesis written to address it, and a single antithesis that critiques the thesis. Combine the thesis and antithesis into a more complete and accurate response that is more fit-for-purpose against the original user prompt. Preserve all critical details."*
        *   **Example 2 (`synthesis_step2_combine`):** *"As an expert editor, your task is to analyze the following user prompt and a set of preliminary syntheses. Combine these documents into a single, unified synthesis that is maximally fit-for-purpose against the original user prompt. You must eliminate redundancy and conflicting information while ensuring every unique and critical detail is preserved."*
*   `[âœ…]` 27.b. **Populate `synthesis` Stage with Recipe Data:**
    *   `[DB]` **Action:** Create a new, single-purpose migration file to populate the `synthesis` stage with its formal, multi-step recipe. This enables the worker to follow a pre-defined plan instead of using hardcoded logic.
    *   `[DB]` **Action:** In the new migration file, write an `UPDATE` statement for the `dialectic_stages` table, targeting the row where `slug = 'synthesis'`.
    *   `[DB]` **Implementation Detail:** The `UPDATE` statement will set the `input_artifact_rules` JSONB value to a new structure containing a `steps` array. Each object within this array will define a step, including a `prompt_template_name` that references the prompts seeded in step `27.a`. This action effectively applies the new recipe schema to the `synthesis` stage.
*   `[âœ…]` 27.c. **[BE] [REFACTOR] Update Job Enqueuer for Recipe Awareness:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-service/generateContribution.ts`
    *   `[BE]` `[REFACTOR]` **Action:** The logic must be updated to first fetch the stage recipe before creating the parent job. This is critical for populating the `step_info` object correctly.
    *   **Implementation Detail:**
        ```typescript
        // In generateContribution.ts, inside the main try block:

        // 1. Fetch the recipe for the stage
        const { data: stageDef, error: recipeError } = await dbClient
            .from('dialectic_stages')
            .select('input_artifact_rules')
            .eq('slug', stageSlug)
            .single();

        if (recipeError || !stageDef) {
            // Handle error: couldn't find the stage definition
            return { success: false, error: { message: `Could not find recipe for stage ${stageSlug}.`, status: 500 } };
        }
        
        // 2. Calculate total steps from the recipe
        const totalSteps = (stageDef.input_artifact_rules)?.steps?.length || 1;

        // 3. Inside the loop for creating jobs, construct the formal payload:
        const jobPayload: Json = {
            // ...existing context like projectId, sessionId, model_id
            job_type: 'plan',
            step_info: {
                current_step: 1,
                total_steps: totalSteps,
                status: 'pending',
            }
        };

        // 4. Insert the job with this new payload
        // ...
        ```

#### 28. [BE] [TEST-UNIT] Implement Core Granularity Strategy Functions

This step implements the core "Strategy" pattern for the complex job planner. It decouples the orchestration logic in `processComplexJob` from the specific logic of how to break down a task by creating a set of modular, testable "planner" functions. Each function aligns with a `granularity_strategy` defined in the stage recipes. The implementation will follow a strict Test-Driven Development (TDD) methodology.

*   `[âœ…]` 28.a. **Create Granularity Strategy Module & Directory Structure:**
    *   `[BE]` **Action:** Create a new directory: `supabase/functions/dialectic-worker/strategies/`. This will serve as the home for all planner-related logic.
    *   `[BE]` **Action:** Inside the new directory, create another directory: `planners/`. This will contain the individual planner function files.
    *   `[BE]` **Action:** Create the main strategy registry file: `supabase/functions/dialectic-worker/strategies/granularity.strategies.ts`.
    *   `[BE]` **Action:** In this file, define and export the `granularityStrategyMap` and the `getGranularityPlanner` function as specified in `A Computable Determinant for Task Isolation.md`. This map will associate strategy strings (e.g., `'pairwise_by_origin'`) with the actual planner function implementations.
    *   **Implementation Detail:**
        ```typescript
        // In granularity.strategies.ts
        import { planPairwiseByOrigin, planPerSourceDocument, planAllToOne, planPerSourceGroup } from './planners';

        export const granularityStrategyMap = {
          'per_source_document': planPerSourceDocument,
          'pairwise_by_origin': planPairwiseByOrigin,
          'per_source_group': planPerSourceGroup,
          'all_to_one': planAllToOne,
        };

        export function getGranularityPlanner(strategyId: string) {
            return granularityStrategyMap[strategyId] || planPerSourceDocument; // Default strategy
        }
        ```

*   `[âœ…]` 28.b. **Implement `planPairwiseByOrigin` Strategy (Map):**
    *   `[BE]` `[TEST-UNIT]` **(RED)** In the `strategies/planners/` directory, create a new test file: `planPairwiseByOrigin.test.ts`. Write a failing unit test that defines the function's contract.
        *   **Test Case:** Provide a mock set of source documents (e.g., 2 `thesis` contributions and 3 related `antithesis` contributions) and a mock parent job context.
        *   **Assertion:** Assert that the planner function returns the correct number of child job payloads and that each payload is correctly formed (e.g., `job_type: 'execute'`, correct `prompt_template_name`, and correctly paired `thesis_id` and `antithesis_id` in the `inputs`).
    *   `[BE]` `[TEST-UNIT]` **(RED)** Create helper utility tests. For example, in a new `strategy.helpers.test.ts` file, write failing tests for `groupSourceDocumentsByType` and `findRelatedContributions`.
    *   `[BE]` `[REFACTOR]` **(GREEN)** Create `strategy.helpers.ts` and implement the helper functions to make the tests pass. These helpers will be responsible for sorting input documents by type and finding related documents based on `source_contribution_id`.
    *   `[BE]` **(GREEN)** In the `strategies/planners/` directory, create `planPairwiseByOrigin.ts`. Implement the planner function logic as described in the documentation, using the tested helpers. The function will loop through `thesis` documents, find their corresponding `antithesis` documents, and generate a child job payload for each pair.
    *   `[TEST-UNIT]` **(PROVE)** Prove that all unit tests in `planPairwiseByOrigin.test.ts` now pass.
    *   **Implementation Detail (Skeleton):**
        ```typescript
        // In a file like /strategies/planners/planPairwiseByOrigin.ts
        export function planPairwiseByOrigin(sourceDocs: SourceDocuments, parentJobPayload: ParentJobPayload, recipeStep: RecipeStep): ChildJobPayload[] {
            const childJobs: ChildJobPayload = [];
            const { theses, antitheses } = groupSourceDocuments(sourceDocs); // Utility to sort inputs

            for (const thesis of theses) {
                // Find antitheses derived from this thesis
                const relatedAntitheses = findRelated(antitheses, thesis.id);
                for (const antithesis of relatedAntitheses) {
                    const newPayload = {
                        // ... core context from parent
                        job_type: 'execute',
                        // The planner now gets the prompt name directly from the recipe!
                        prompt_template_name: recipeStep.prompt_template_name,
                        inputs: {
                            thesis_id: thesis.resource_id,
                            antithesis_id: antithesis.resource_id,
                        }
                    };
                    childJobs.push(newPayload);
                }
            }
            return childJobs;
        }
        ```

*   `[âœ…]` 28.c. **Implement `planPerSourceDocument` Strategy (Map):**
    *   `[BE]` `[TEST-UNIT]` **(RED)** Create `planPerSourceDocument.test.ts`. Write a failing test that provides a list of source documents and asserts that the function returns a child job for each one.
    *   `[BE]` **(GREEN)** Create `planPerSourceDocument.ts`. Implement the simple logic to loop through the inputs and create a job for each.
    *   `[TEST-UNIT]` **(PROVE)** Prove the test passes.

*   `[âœ…]` 28.d. **Implement `planPerSourceGroup` Strategy (Reduce):**
    *   `[BE]` `[TEST-UNIT]` **(RED)** Create `planPerSourceGroup.test.ts`. Write a failing test.
        *   **Test Case:** Provide a list of intermediate artifacts (e.g., `pairwise_synthesis_chunk`) that share a common `source_contribution_id`.
        *   **Assertion:** Assert that the function groups these artifacts correctly and generates a single child job for each group, with all grouped `resource_id`s in the `inputs`.
    *   `[BE]` **(GREEN)** Create `planPerSourceGroup.ts`. Implement the logic to group documents by `source_contribution_id` and generate one job per group.
    *   `[TEST-UNIT]` **(PROVE)** Prove the test passes.

*   `[âœ…]` 28.e. **Implement `planAllToOne` Strategy (Reduce):**
    *   `[BE]` `[TEST-UNIT]` **(RED)** Create `planAllToOne.test.ts`. Write a failing test that provides a list of inputs and asserts that the function returns exactly one child job containing all input `resource_id`s.
    *   `[BE]` **(GREEN)** Create `planAllToOne.ts`. Implement the straightforward logic.
    *   `[TEST-UNIT]` **(PROVE)** Prove the test passes.

*   `[âœ…]` 28.f. **[COMMIT] `feat(worker): implement granularity strategy planners`**
    *   **Action:** Commit the completed, fully tested granularity strategy module.

#### 29. [BE] [REFACTOR] Enhance Planner for Multi-Step Recipe Execution

This step refactors the complex job worker to be driven by a formal, multi-step recipe defined in the database. The implementation maintains strict type safety from end to end by defining clear interfaces and using TypeScript's type guard system, completely avoiding type casting.

##### Phase 29.1: Establish the Type-Safe Foundation

Before modifying logic, we must define the new data structures that will drive the multi-step process.

*   `[âœ…]` 29.a. **Define Recipe and Step Interfaces:**
    *   `[BE]` **File:** `supabase/functions/dialectic-service/dialectic.interface.ts`.
    *   `[BE]` **Action:** Add new interfaces to represent the recipe structure stored in the `dialectic_stages.input_artifact_rules` JSONB column.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/dialectic-service/dialectic.interface.ts

        /**
         * Describes a single step within a multi-step job recipe.
         */
        export interface DialecticRecipeStep {
            step: number;
            name: string;
            prompt_template_name: string;
            inputs_required: {
                type: string;
                stage_slug?: string; // e.g., 'thesis' for antithesis inputs
            }[];
            granularity_strategy: 'per_source_document' | 'pairwise_by_origin' | 'per_source_group' | 'all_to_one';
            output_type: string; // e.g., 'pairwise_synthesis_chunk'
        }

        /**
         * Describes the complete recipe for a complex, multi-step stage.
         */
        export interface DialecticStageRecipe {
            processing_strategy: {
                type: 'task_isolation';
            };
            steps: DialecticRecipeStep[];
        }
        ```
    *   `[BE]` **File:** `supabase/functions/_shared/utils/type_guards.ts`.
    *   `[BE]` **Action:** Add a corresponding type guard to safely validate the recipe structure at runtime.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/_shared/utils/type_guards.ts
        export function isDialecticStageRecipe(value: unknown): value is DialecticStageRecipe {
            const recipe = value;
            return (
                recipe?.processing_strategy?.type === 'task_isolation' &&
                Array.isArray(recipe.steps) &&
                recipe.steps.every(
                    (step) =>
                        typeof step.step === 'number' &&
                        typeof step.prompt_template_name === 'string' &&
                        typeof step.granularity_strategy === 'string' &&
                        typeof step.output_type === 'string' &&
                        Array.isArray(step.inputs_required)
                )
            );
        }
        ```

*   `[âœ…]` 29.b. **Update Job Payload Types:**
    *   `[BE]` **File:** `supabase/functions/dialectic-service/dialectic.interface.ts`.
    *   `[BE]` **Action:** Introduce new, strictly-typed payloads for parent (`'plan'`) jobs and child (`'execute'`) jobs, along with a `step_info` tracker. Update the main `DialecticJobPayload` to be a discriminated union of all possible payload types.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/dialectic-service/dialectic.interface.ts

        /**
         * Tracks the progress of a multi-step job.
         */
        export interface DialecticStepInfo {
            current_step: number;
            total_steps: number;
        }

        /**
         * The base payload containing information common to all job types.
         */
        export interface DialecticBaseJobPayload extends Omit<GenerateContributionsPayload, 'selectedModelIds'> {
            model_id: string; // Individual model ID for this specific job
        }

        /**
         * The payload for a parent job that plans steps based on a recipe.
         */
        export interface DialecticPlanJobPayload extends DialecticBaseJobPayload {
            job_type: 'plan';
            step_info: DialecticStepInfo;
        }

        /**
         * The payload for a child job that executes a single model call.
         */
        export interface DialecticExecuteJobPayload extends DialecticBaseJobPayload {
            job_type: 'execute';
            step_info: DialecticStepInfo; // Pass down for context
            prompt_template_name: string;
            output_type: string; // The type of artifact this job will produce
            inputs: {
                // Key-value store for resource_ids needed by the prompt
                [key: string]: string; 
            };
        }
        
        // Update the main union type
        export type DialecticJobPayload =
            | DialecticSimpleJobPayload // Assuming this exists for non-complex jobs
            | DialecticPlanJobPayload
            | DialecticExecuteJobPayload
            | DialecticCombinationJobPayload;
        ```
    *   `[BE]` **File:** `supabase/functions/_shared/utils/type_guards.ts`.
    *   `[BE]` **Action:** Create type guards for the new payloads.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/_shared/utils/type_guards.ts
        export function isDialecticPlanJobPayload(payload: unknown): payload is DialecticPlanJobPayload {
            const p: DialecticPlanJobPayload = payload;
            return p?.job_type === 'plan' && typeof p.step_info?.current_step === 'number';
        }

        export function isDialecticExecuteJobPayload(payload: unknown): payload is DialecticExecuteJobPayload {
            const p = payload as DialecticExecuteJobPayload;
            return p?.job_type === 'execute' && typeof p.prompt_template_name === 'string' && typeof p.inputs === 'object';
        }
        ```

##### Phase 29.2: Implement the Recipe-Driven Logic

*   `[âœ…]` 29.c. **Refactor `processComplexJob` as the Orchestrator:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/processComplexJob.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** Refactor `processComplexJob` to be the master orchestrator. It is responsible for reading the job's `step_info`, fetching the stage recipe, determining the current step, and delegating to the planner.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/dialectic-worker/processComplexJob.ts
        export async function processComplexJob(
            dbClient: SupabaseClient<Database>,
            // The intersection type asserts this job has a plannable payload
            job: DialecticJobRow & { payload: DialecticJobPayload },
            // ... deps
        ): Promise<void> {
            // Use the type guard to safely narrow the payload
            if (!isDialecticPlanJobPayload(job.payload)) {
                // This is a logic error, the job router should not send other job types here.
                throw new Error(`Job ${job.id} has an invalid payload for complex processing.`);
            }
            
            // From here, `job.payload` is a strongly-typed `DialecticPlanJobPayload`
            const { step_info, stageSlug } = job.payload;
            deps.logger.info(`[processComplexJob] Processing step ${step_info.current_step}/${step_info.total_steps} for job ${job.id}`);

            // 1. Fetch the recipe and validate its structure with a type guard
            const { data: stageData } = await dbClient.from('dialectic_stages').select('input_artifact_rules').eq('slug', stageSlug!).single();
            if (!isDialecticStageRecipe(stageData?.input_artifact_rules)) {
                throw new Error(`Stage '${stageSlug}' has an invalid or missing recipe.`);
            }
            const recipe = stageData.input_artifact_rules;
            
            // 2. Determine the current step's recipe
            const currentRecipeStep = recipe.steps.find(s => s.step === step_info.current_step);
            if (!currentRecipeStep) {
                throw new Error(`Could not find recipe for step ${step_info.current_step}.`);
            }

            // 3. Delegate to the planner to create child jobs for this specific step
            const childJobsToInsert = await deps.planComplexStage(dbClient, job, deps, currentRecipeStep);
            
            // ... (rest of the logic to insert child jobs and update parent status to 'waiting_for_children')
        }
        ```
*   `[âœ…]` 29.d. **Refactor `task_isolator.ts` as the Step Planner:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/task_isolator.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** The `planComplexStage` function is refactored to be a pure "step planner". It no longer orchestrates but is instead called *by* the orchestrator (`processComplexJob`) to plan a single step based on the provided recipe.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/dialectic-worker/task_isolator.ts
        export async function planComplexStage(
            dbClient: SupabaseClient<Database>,
            parentJob: DialecticJobRow & { payload: DialecticPlanJobPayload },
            deps: IPlanComplexJobDeps,
            // The specific recipe for the current step is now passed in
            recipeStep: DialecticRecipeStep,
        ): Promise<ChildJobInsert[]> { // ChildJobInsert is the type for a new DB row
            
            // 1. Use recipeStep.inputs_required to query for source documents.
            const sourceDocuments = await findSourceDocuments(dbClient, parentJob.payload.projectId, recipeStep.inputs_required);

            // 2. Get the correct planner function using the strategy from the recipe.
            const planner = getGranularityPlanner(recipeStep.granularity_strategy);

            // 3. Execute the planner to get the strongly-typed child job payloads.
            const childJobPayloads: DialecticExecuteJobPayload[] = planner(sourceDocuments, parentJob.payload, recipeStep);

            // 4. Map to full job rows for DB insertion, maintaining type safety.
            const childJobsToInsert = childJobPayloads.map(payload => ({
                parent_job_id: parentJob.id,
                // ... other fields from parent ...
                status: 'pending',
                payload: payload, // `payload` is already a valid, typed `DialecticExecuteJobPayload`
            }));
            
            return childJobsToInsert;
        }
        ```
*   `[âœ…]` 29.e. **Ensure Original Prompt is Always Included:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/_shared/prompt-assembler.ts`.
    *   `[BE]` **Action:** The `gatherContext` utility must be updated to always fetch the project's original user prompt for any job belonging to a complex stage (e.g., `'synthesis'`). This prompt text must be passed to the prompt renderer as a distinct, top-level variable (`original_user_request`) to ensure it's available for injection into any step's prompt template.
*   `[âœ…]` 29.f. **Tag Intermediate Artifacts in the Executor:**
    *   `[BE]` **File:** `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`.
    *   `[BE]` **Action:** When saving the output of a child job, the worker must use the `output_type` from the strongly-typed payload to tag the new artifact in `dialectic_project_resources`.
    *   **Implementation Detail:**
        ```typescript
        // In supabase/functions/dialectic-worker/executeModelCallAndSave.ts
        export async function executeModelCallAndSave(
            // The job here is a child job with a strongly-typed 'execute' payload
            job: DialecticJobRow & { payload: DialecticExecuteJobPayload },
            // ... other parameters
        ) {
            // ... logic to call the AI model ...
            const modelOutput = await deps.callUnifiedAIModel(...);

            // When saving the result, the output_type comes directly from the typed payload.
            await fileManager.uploadAndRegisterFile({
                // ... other context
                resourceTypeForDb: job.payload.output_type,
                description: `Intermediate artifact for ${job.id}, step ${job.payload.step_info.current_step}`,
            });
        }
        ```
*   `[âœ…]` 29.g. **Update Parent Job Orchestration Logic:**
    *   `[DB]` `[BE]` **File:** The `handle_job_completion()` PostgreSQL function and `supabase/functions/dialectic-worker/processComplexJob.ts`.
    *   `[BE]` **Action:** The state machine is orchestrated as follows:
        1.  When all child jobs for a step complete, the `handle_job_completion()` trigger wakes the parent job by setting its status to `'pending_next_step'`.
        2.  The `processComplexJob` worker picks up this parent job. Its first action is to check for this status.
        3.  If the status is `pending_next_step`, it increments `payload.step_info.current_step` and updates the job in the database.
        4.  It then proceeds with the planning logic. If `current_step > total_steps`, it marks the parent job as `'completed'`. Otherwise, it re-calls the planner for the new step.
*   `[âœ…]` 29.h. **Define Multi-Step Error Handling Strategy:**
    *   `[BE]` `[REFACTOR]` **Action:** If any child job (type `'execute'`) fails permanently, it must report its failure to the parent. The `handle_job_completion` trigger will detect this child failure and immediately set the parent job's `status` to `'failed'`, also marking `step_info.status` as `'failed'`. This fail-fast approach prevents the process from getting stuck waiting for a job that will never complete.

---

### Phase 11: [REFACTOR] Implement `_work` Directory for Intermediate Artifacts

This phase addresses a critical filename collision issue discovered during integration testing of multi-step stages (`synthesis`). It introduces a `_work` subdirectory to house intermediate artifacts, ensuring final, user-facing deliverables have clean, predictable paths while guaranteeing unique paths for all generated files.

#### 30. [BE] [ARCH] Implement `_work` Directory Strategy

*   `[âœ…]` 30.a. **Update Core Type Definitions:**
    *   `[TYPES]` **File:** `supabase/functions/_shared/types/file_manager.types.ts`.
    *   `[TYPES]` **Action:** Add a new optional boolean property, `isIntermediate?: boolean`, to the `PathContext` interface. This flag will signal to the path constructor that the file is a temporary or intermediate artifact.

*   `[âœ…]` 30.b. **Modify Path Constructor Logic:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/_shared/utils/path_constructor.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** Update the `constructStoragePath` function. When it builds the path for a `FileType` of `'model_contribution_main'`, it must check for the `isIntermediate` flag in the `PathContext`.
        *   If `isIntermediate` is `true`, it must insert the `_work/` segment into the storage path immediately after the stage directory (e.g., `.../3_synthesis/_work/`).
        *   If `isIntermediate` is `false` or undefined, the path should be constructed as before, placing the file in the stage's root.
    *   `[TEST-UNIT]` **(RED)** Update `path_constructor.test.ts` to include test cases for this new logic. Test both scenarios (`isIntermediate: true` and `isIntermediate: false`) to ensure the `_work` directory is added correctly and only when required.
    *   `[TEST-UNIT]` **(GREEN)** Prove the updated `path_constructor` passes all tests.

*   `[âœ…]` 30.c. **Update Planners to Signal Intermediate Artifacts:**
    *   `[BE]` `[REFACTOR]` **Files:** All relevant granularity strategy planners (`planPairwiseByOrigin.ts`, `planPerSourceGroup.ts`, `planAllToOne.ts`).
    *   `[BE]` `[REFACTOR]` **Action:** When these planners construct the `PathContext` for a child job's payload, they must now determine if the `output_type` is an intermediate artifact.
        *   The logic will be: if the `output_type` is `'pairwise_synthesis_chunk'` or `'reduced_synthesis'`, set `isIntermediate: true` in the `PathContext`.
        *   If the `output_type` is `'synthesis'` (the final deliverable), `isIntermediate` should be `false` or omitted.
    *   `[TEST-UNIT]` **(RED/GREEN)** Update the unit tests for each planner to assert that the `isIntermediate` flag is correctly set in the generated job payloads based on the `output_type` of the recipe step.

*   `[âœ…]` 30.d. **Update Documentation:**
    *   `[DOCS]` **File:** `docs/implementations/Current/Checklists/Current/AI Dialectic Implementation Plan Phase 2.md`.
    *   `[DOCS]` **Action:** Update the "File Structure for Supabase Storage and Export Tools" diagram and description to include the new `_work` subdirectory within the `synthesis` stage, explaining its purpose.

*   `[âœ…]` 30.e. **[COMMIT] `feat(worker): implement _work directory for intermediate artifacts`**

#### 31. [TEST-INT] Create the End-to-End Antithesis and Synthesis Pipeline Tests

**Goal:** To create a single, comprehensive integration test file that validates the entire multi-stage, multi-step asynchronous workflow. This test will simulate the process from the end of the `Thesis` stage through the planning and execution of the `Antithesis` stage, and finally through the multi-step "map-reduce" process of the `Synthesis` stage. It will validate the core mechanics of recipe-driven planning, child job execution, and database trigger-based orchestration.

*   `[âœ…]` 31.a. **Enhance Existing Integration Test File:**
    *   `[TEST-INT]` **File:** `supabase/integration_tests/services/dialectic_pipeline.integration.test.ts`.
    *   `[TEST-INT]` **Action:** This test will be extended. The existing logic that successfully completes the `Thesis` stage and prepares for `Antithesis` will serve as the foundation. We will add new, sequential test steps within the same `Deno.test` block to continue the workflow.

*   `[âœ…]` 31.b. **Test Setup and Seeding:**
    *   `[TEST-INT]` **Action:** The existing test setup is already seeding the necessary users, projects, and AI models. Ensure that the `dialectic_stages` table is seeded with the correct, detailed `input_artifact_rules` (recipes) for both the `antithesis` and `synthesis` stages, as defined in `A Computable Determinant for Task Isolation.md`.
        *   The `antithesis` recipe should be a single-step plan with a `granularity_strategy` of `'per_source_document'`.
        *   The `synthesis` recipe must be the formal, multi-step recipe with three steps using `'pairwise_by_origin'`, `'per_source_group'`, and `'all_to_one'` strategies, respectively.

*   `[âœ…]` 31.c. **Test Execution: Antithesis Stage (Single-Step Complex Job)**
    *   `[TEST-INT]` **Action:** Building on the state where `Thesis` is complete and `antithesis` is pending:
        1.  **Invoke Planner:** Call the `dialectic-service` to start the `antithesis` stage. This will create one parent `'plan'` job for each model.
        2.  **Execute Planner:** Call `executePendingDialecticJobs()`.
        3.  **Assert Planning:**
            *   Assert that the parent jobs transition to `'waiting_for_children'`.
            *   Assert that the correct number of child `'execute'` jobs are created. Given 2 `thesis` contributions and 2 models, this should result in 4 child jobs (`2 theses * 2 models`), each with a `parent_job_id`.
        4.  **Execute Child Jobs:** Call `executePendingDialecticJobs()` again to process the newly created child jobs.
        5.  **Assert Orchestration & Completion:**
            *   Assert that all child jobs are marked `'completed'`.
            *   Assert that the database trigger fires and updates the parent jobs to `'completed'`.
            *   Query the `dialectic_contributions` table and assert that exactly 4 `antithesis` contributions now exist.

*   `[âœ…]` 31.d. **Test Execution: Synthesis Stage (Multi-Step Map-Reduce Job)**
    *   `[TEST-INT]` **Action:** The test will now simulate the entire multi-step synthesis process:
        1.  **Submit Antithesis Feedback:** Invoke the `submitStageResponses` function to simulate user feedback on the `antithesis` contributions. Assert that the session status advances and the seed prompt for the `synthesis` stage is created.
        2.  **Invoke Synthesis Planner (Step 1):**
            *   Invoke the `dialectic-service` to start the `synthesis` stage. Assert the parent `'plan'` job is created (one per model).
            *   Call `executePendingDialecticJobs()`.
            *   Assert parent job is `'waiting_for_children'` and child jobs for **Step 1 (`pairwise_by_origin`)** are created. Given 2 `thesis` and 4 `antithesis` (2 per thesis), this should create 4 child jobs (`2 thesis * 2 antithesis/thesis`).
        3.  **Simulate Step 1 Completion & Wake Parent:**
            *   Call `executePendingDialecticJobs()` to complete the Step 1 child jobs.
            *   Poll the database to assert the trigger fires and the parent job's status becomes `'pending_next_step'`.
        4.  **Invoke Synthesis Planner (Step 2):**
            *   Call `executePendingDialecticJobs()` again. The worker will pick up the parent job, increment its `step_info.current_step` to 2, and re-delegate to the planner.
            *   Assert the parent job returns to `'waiting_for_children'` and child jobs for **Step 2 (`per_source_group`)** are created. Given 4 `pairwise_synthesis_chunk` artifacts grouped by 2 original theses, this should result in 2 child jobs.
        5.  **Simulate Step 2 Completion & Wake Parent:**
            *   Repeat the process: execute the Step 2 child jobs and assert the parent job returns to `'pending_next_step'`.
        6.  **Invoke Synthesis Planner (Step 3):**
            *   Call `executePendingDialecticJobs()` one last time. The worker will increment the step to 3.
            *   Assert the parent job returns to `'waiting_for_children'` and child jobs for **Step 3 (`all_to_one`)** are created. Given 2 `reduced_synthesis` artifacts, this will create 2 child jobs (one per model, each taking all reduced syntheses as input).
        7.  **Simulate Final Step Completion:**
            *   Execute the final child jobs.
            *   Assert that the database trigger now marks the parent job as `'completed'`, as `current_step` now equals `total_steps`.

*   `[âœ…]` 31.e. **Final Assertions:**
    *   `[TEST-INT]` **Action:** Query the `dialectic_generation_jobs` table and assert that all parent and child jobs for the `synthesis` stage are `'completed'`.
    *   `[TEST-INT]` **Action:** Query the `dialectic_contributions` table and storage. Verify that the correct number of intermediate artifacts (`pairwise_synthesis_chunk`, `reduced_synthesis`) and the final `synthesis` contributions were created correctly and are linked to the session.

---

### Phase 12: [ARCH] [REFACTOR] Implement Convergent Dialectic and Sophisticated RAG Pipeline

**Design Philosophy:** This phase addresses two critical insights. First, the dialectic process must converge towards a single output in later stages. Second, the RAG pipeline enabling this must be sophisticated enough to prevent the "averaging effect" and preserve unique, novel details.

#### 32. [BE] [DB] [REFACTOR] Implement a Dynamic, On-Demand RAG Service

This section refactors the RAG pipeline to be a dynamic, on-demand "context compression" service. Instead of being reflexively tied to specific stages, RAG will be invoked by any part of the system that prepares data for a model call whenever it detects that the proposed input context exceeds the model's token limit. This creates a more robust, efficient, and architecturally sound system.

*   `[âœ…]` 32.a. **Setup the Vector Database:**
    *   `[DB]` **Tool:** Supabase SQL Editor.
    *   `[DB]` **Action:** Enable the `vector` extension in your Supabase project (`create extension vector;`).
    *   `[DB]` **Action:** Create a new table, `dialectic_memory`, to store text chunks and their embeddings. It should include columns like `id` (UUID), `session_id` (FK to `dialectic_sessions`), `source_contribution_id` (FK to `dialectic_contributions`), `content` (text), `metadata` (JSONB), and `embedding` (vector(1536)) -- assuming OpenAI's `text-embedding-3-small`.

*   `[âœ…]` 32.b. **Implement the Indexing Service with Semantic Chunking:**
    *   `[BE]` **Recommendation:** Use a library like `langchain-js` (which has Deno support) to simplify text splitting and embedding calls.
    *   `[BE]` **File:** `supabase/functions/_shared/services/indexing_service.ts`.
    *   `[BE]` **Action:** Create a new service that:
        1.  Takes a document's text and metadata.
        2.  Uses a text splitter (e.g., `RecursiveCharacterTextSplitter`) configured for **semantic chunking**. The goal is to split documents at points of topical shift to create focused, potent vectors that preserve unique ideas rather than using naive fixed-size chunks.
        3.  Calls an embedding model API (e.g., OpenAI's `text-embedding-3-small`) for each chunk.
        4.  Saves the chunk's content, metadata, and the resulting embedding vector into the `dialectic_memory` table.

*   `[âœ…]` 32.c. **Refactor Indexing to be a "Just-in-Time" Process:**
    *   `[BE]` `[REFACTOR]` **Action:** The current approach of reflexively indexing all documents from certain stages is inefficient. Indexing should be performed dynamically, on-demand, only when a RAG operation is actually required.
    *   **Implementation - Step 1: Remove Reflexive Indexing:**
        *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-service/submitStageResponses.ts`.
        *   `[BE]` `[REFACTOR]` **Action:** Delete the entire logic block that triggers the `IndexingService` based on the current stage slug. Indexing should no longer be a part of the stage transition process.
    *   **Implementation - Step 2: Implement Just-in-Time Indexing in `RagService`:**
        *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/_shared/services/rag_service.ts`.
        *   `[BE]` `[REFACTOR]` **Action:** Enhance the `getContextForModel` method. Before this method executes its multi-query retrieval logic, it must first ensure that all source documents it needs to query have been indexed.
        *   `[BE]` **Logic:**
            1.  The service receives a list of source document IDs.
            2.  It queries the `dialectic_memory` table to see which chunks corresponding to these document IDs already exist.
            3.  For any document IDs that do *not* have corresponding chunks in the vector store, the `RagService` will invoke the `IndexingService` to chunk and embed them.
            4.  Only once all required documents are confirmed to be in the vector store will the `RagService` proceed with its retrieval, re-ranking, and assembly logic.

*   `[âœ…]` 32.d. **Create a Central, Reusable `RagService` with Advanced Retrieval:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/_shared/services/rag_service.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** Create a new, dedicated `RagService` to encapsulate all logic for retrieving and synthesizing context. Its primary method, `getContextForModel`, will accept a collection of source document texts/IDs and the target `AiModelExtendedConfig`.
    *   **Implementation - Step 1: Hybrid Search RPC:**
        *   `[DB]` **Action:** Create an RPC function in Supabase, e.g., `match_dialectic_chunks`. This function is critical and must support **hybrid search**. It will accept a query and return results based on a combination of vector similarity (`<->` operator) and traditional keyword matching (e.g., `to_tsvector`). This ensures both semantically relevant and literally exact information is retrieved, preventing important but non-standard terms from being missed by vector search alone.
    *   **Implementation - Step 2: Multi-Query, Re-ranking, and Assembly Logic:**
        *   `[BE]` **Action:** Inside the `getContextForModel` method, implement a sophisticated, multi-step retrieval process:
        1.  **Generate Multiple Queries:** Programmatically generate several queries to capture different "angles" of the required information. For example: a broad synthesis query, a query specifically asking for "novel or unique approaches", and another asking for "conflicting recommendations".
        2.  **Retrieve Superset:** Call the `match_dialectic_chunks` RPC for *each* generated query to gather a superset of potentially relevant chunks from the `dialectic_memory` table.
        3.  **Re-rank for Diversity:** Use a **re-ranking algorithm like Maximal Marginal Relevance (MMR)** on the combined result set. This is crucial for preventing the "averaging effect." MMR will select a final list of chunks that are both highly relevant to the queries and semantically diverse, ensuring that unique, outlier perspectives are not drowned out by more common, redundant themes.
        4.  **Assemble Final Prompt:** Create the final, compact prompt containing the re-ranked, diverse chunks. This string is the distilled, token-constrained context that will be returned by the service.

*   `[âœ…]` 32.e. **Refactor `task_isolator.ts` for Dynamic RAG-based Planning:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/task_isolator.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** Modify the `planComplexStage` function to be RAG-aware.
    *   **Logic:**
        1.  After gathering source documents, perform the token estimation check as defined in `Phase 9, Step 26.b`.
        2.  **If tokens are within limits:** Proceed with the existing job planning logic.
        3.  **If tokens exceed limits:**
            *   **Remove** the logic that creates prerequisite `'combine'` jobs.
            *   **Instead**, invoke the new `RagService` with the oversized collection of documents.
            *   Use the single, RAG-generated context string to create a single child job. This is more efficient and eliminates the need for complex prerequisite job orchestration.

*   `[âœ…]` 32.f. **Refactor `prompt-assembler.ts` for Correct, Granular, Dynamic RAG-based Assembly:**
    *   `[BE]` `[REFACTOR]` **Goal:** To refactor the `PromptAssembler` to dynamically invoke the `RagService` with a granular, complete set of individual source documents. This is a critical correction to ensure the RAG service can perform its intended function of semantic chunking, multi-query retrieval, and diversity re-ranking, preventing the "averaging effect" that would occur if all context were pre-combined into a single string.
    *   `[TEST-UNIT]` **(RED)** **Update `prompt-assembler.test.ts`:**
        *   **Action:** Add/update a `describe` block for "Dynamic Granular RAG Invocation".
        *   **Action:** Create a mock for the `RagService` that can be spied on.
        *   **Action:** Modify mocks for `gatherInputsForStage` to return an array of `SourceDocument` objects (a new internal type defined below), not a single string.
        *   **Test Case 1 (Context Under Limit):**
            *   **Setup:** Mock `gatherInputsForStage` to return a small array of `SourceDocument`s. Mock `countTokens` to return a value below the token limit.
            *   **Execution:** Call `gatherContext`.
            *   **Assertion:** Assert that `ragService.getContextForModel` was **NOT** called. Assert that the `dynamicContextVariables` are correctly formatted by combining the individual `SourceDocument` objects.
        *   **Test Case 2 (Context Over Limit):**
            *   **Setup:** Mock `gatherInputsForStage` to return a large array of `SourceDocument`s. Mock `countTokens` to return a value exceeding the `minTokenLimit`. Provide the mock `RagService`.
            *   **Execution:** Call `gatherContext`.
            *   **Assertion:** Assert that `ragService.getContextForModel` **WAS** called exactly once with an array of `IRagSourceDocument` objects, preserving the individuality of each source. Assert that the returned `dynamicContextVariables.prior_stage_ai_outputs` contains the compressed string from the mock RAG service.
    *   `[BE]` `[REFACTOR]` **(GREEN)** **Step 1: Refactor `gatherInputsForStage` to Return Individual Documents:**
        *   **File:** `supabase/functions/_shared/prompt-assembler.ts`.
        *   **Action:** Create a new internal interface, `SourceDocument`, to represent each distinct piece of content (contribution or feedback), including its `id`, `type`, `content`, and formatting `metadata`.
        *   **Action:** Change the return type of `gatherInputsForStage` from `Promise<{ priorStageContributions: string; priorStageFeedback: string }>` to `Promise<SourceDocument[]>`.
        *   **Action:** Modify the function's logic. Instead of concatenating content into strings, it will now create and return an array of `SourceDocument` objects.
    *   `[BE]` `[REFACTOR]` **(GREEN)** **Step 2: Implement Correct RAG Logic in `gatherContext`:**
        *   **File:** `supabase/functions/_shared/prompt-assembler.ts`.
        *   **Action:** Update `gatherContext` to receive the `SourceDocument[]` array.
        *   **Action:** Implement the token estimation logic by combining the content from the `SourceDocument` array for an accurate measurement.
        *   **Action:** Implement the conditional RAG logic:
            1.  **If `estimatedTokens > minTokenLimit`:**
                *   Map the `SourceDocument[]` to the `IRagSourceDocument[]` format (`{ id, content }`).
                *   Invoke `this.ragService.getContextForModel` with this granular array of documents.
                *   Use the compressed string from the RAG service result as `prior_stage_ai_outputs`.
            2.  **If `estimatedTokens <= minTokenLimit` (the "else" case):**
                *   Iterate through the `SourceDocument[]` array to correctly format the `prior_stage_ai_outputs` and `prior_stage_user_feedback` strings, preserving headers and original structure.
    *   `[BE]` `[REFACTOR]` **(GREEN)** **Step 3: Update `prompt-assembler.mock.ts`:**
        *   **File:** `supabase/functions/_shared/prompt-assembler.mock.ts`.
        *   **Action:** Update the `MockPromptAssembler` constructor and all spied method signatures (`assemble`, `gatherContext`) to match the new dependencies and parameters of the real `PromptAssembler` class, including `ragServiceDeps`, `modelConfigForTokenization`, and `minTokenLimit`.
    *   `[BE]` `[REFACTOR]` **(GREEN)** **Step 4: Verify Caller Service (`submitStageResponses.ts`):**
        *   **File:** `supabase/functions/dialectic-service/submitStageResponses.ts`.
        *   **Action:** Confirm that the instantiation of `PromptAssembler` and the call to `assembler.assemble` are correct and provide all necessary dependencies and parameters, which was already done correctly. No changes are expected here, but verification is key.
    *   `[TEST-UNIT]` **(PROVE)** Prove that all unit tests in `prompt-assembler.test.ts` now pass, validating both the RAG-enabled and non-RAG paths with granular data.

*   `[âœ…]` 32.g. **Confirm `executeModelCallAndSave.ts` as a Final Safeguard:**
    *   `[BE]` `[REFACTOR]` **File:** `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`.
    *   `[BE]` `[REFACTOR]` **Action:** The plan explicitly confirms that the token check in this executor function remains the final, fatal safeguard.
    *   **Logic:** If the token count of the fully rendered prompt still exceeds the model's limit, it signifies an upstream bug in token estimation or prompt rendering. The function **must** `throw new ContextWindowError`. It will **not** attempt a last-second RAG call, as this would hide upstream issues and lead to unpredictable behavior.

*   `[âœ…]` 32.h. **[REFACTOR] Unify Worker Dependency Injection:**
    *   `[BE]` `[REFACTOR]` **Goal:** To resolve a type mismatch in the main `processJob` router by consolidating the disparate `ProcessSimpleJobDeps` and `IPlanComplexJobDeps` interfaces into a single, unified dependency object. This will ensure type safety and provide all processors with the services they need.
    *   `[BE]` `[REFACTOR]` **Step 1: Unify Dependency Interfaces:**
        *   **File:** `supabase/functions/dialectic-service/dialectic.interface.ts`.
        *   **Action:** Locate the `ProcessSimpleJobDeps` interface. Rename it to `IDialecticJobDeps`.
        *   **Action:** Add the properties from `IPlanComplexJobDeps` (`ragService`, `fileManager`, `countTokens`, `getAiProviderConfig`) to the new `IDialecticJobDeps` interface. Make properties that are not used by all consumers optional (e.g., `ragService?: IRagService`).
        *   **File:** `supabase/functions/dialectic-worker/processComplexJob.ts`.
        *   **Action:** Delete the now-redundant `IPlanComplexJobDeps` interface definition.
    *   `[BE]` `[REFACTOR]` **Step 2: Update Worker Entry Point:**
        *   **File:** `supabase/functions/dialectic-worker/index.ts`.
        *   **Action:** In the main `Service` function, instantiate the new services required for complex jobs: `RagService` and `FileManager`.
        *   **Action:** Update the `deps` object to include these new service instances.
        *   **Action:** Update all type references from `ProcessSimpleJobDeps` to the new `IDialecticJobDeps`.
    *   `[BE]` `[REFACTOR]` **Step 3: Update Job Router and Processors:**
        *   **File:** `supabase/functions/dialectic-worker/processJob.ts`.
        *   **Action:** Change the `processJob` function signature to accept `deps: IDialecticJobDeps`.
        *   **Action:** In the `task_isolation` branch, the `complexDeps` object can now be correctly populated from the main `deps` object, resolving the original linter errors.
        *   **File:** `supabase/functions/dialectic-worker/processComplexJob.ts`.
        *   **Action:** Update the function signature to use `IDialecticJobDeps` instead of `IPlanComplexJobDeps`.
        *   **File:** `supabase/functions/dialectic-worker/processSimpleJob.ts`.
        *   **Action:** Update the function signature to use `IDialecticJobDeps`.
    *   `[TEST-UNIT]` `[TEST-INT]` **Step 4: Update All Tests:**
        *   **Action:** Systematically go through the list of test files that use the old dependency types (`executeModelCallAndSave.test.ts`, `index.test.ts`, `processJob.test.ts`, `processSimpleJob.test.ts`, `processComplexJob.test.ts`, `task_isolator.test.ts`, `dialectic_pipeline.integration.test.ts`).
        *   **Action:** For each test file, update the type imports to use the new `IDialecticJobDeps` interface.
        *   **Action:** Update all mock dependency objects (`mockDeps`) to include the new properties (`ragService`, `fileManager`, etc.), providing mock implementations or spies as needed to satisfy the updated interface.
