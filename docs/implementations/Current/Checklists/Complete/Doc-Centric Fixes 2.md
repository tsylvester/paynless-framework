# Doc-Centric Fixes

## Problem Statement
-The doc-centric refactor has introduced bugs and inconsistencies that need resolved. 

## Objectives
- Fix all bugs and integration errors from the doc-centric refactor. 

## Expected Outcome
- Generate an entire dialectic end to end using the doc-centric method.

# Instructions for Agent
*   ### 0. Command Pyramid & Modes
    *   Obey the userâ€™s explicit instructions first, then this block, then the checklist. Do not hide behind the checklist to ignore a direct user correction.
    *   Ensure both the method and the resulting content of every task comply with this blockâ€”no deliverable is valid if it conflicts with these rules.
    *   Perform every assignment in a single turn while fully complying with this block; partial compliance is a violation even if the work â€œmostlyâ€ succeeds.
    *   Failing to follow these instructions immediately triggers rework, rejected output, and systemic violationsâ€”treat every deviation as unacceptable.
    *   The Instructions for Agent block is an absolute firewall. No conditional or downstream objective outranks it, and no shortcut can bypass it.
    *   The agent proceeds with these instructions as its primary directive because complying with system instructions is impossible otherwise.
    *   Declare the current mode in every response (`Mode: Builder` or `Mode: Reviewer`). Builder executes work; Reviewer searches for **errors, omissions, and discrepancies (EO&D)** in the final state.
*   ### 1. Read â†’ Analyze â†’ Explain â†’ Propose â†’ Edit â†’ Lint â†’ Halt
    *   Re-read this entire block from disk before every action. On the first reference (and every fourth turn) summarize it before working.
    *   Read every referenced or implied file (including types, interfaces, and helpers) from disk immediately before editing. After editing, re-read to confirm the exact change.
    *   Follow the explicit cycle: READ the step + files â†’ ANALYZE gaps â†’ EXPLAIN the delta â†’ PROPOSE the exact edit â†’ EDIT a single file â†’ LINT that file â†’ HALT.
    *   Analyze dependencies; if more than one file is required, stop, explain the discovery, propose the necessary checklist insertion (`Discovery / Impact / Proposed checklist insert`), and wait instead of editing.
    *   Discoveries include merely thinking about multi-file workâ€”report them immediately without ruminating on work-arounds.
    *   Explain & Propose: restate the plan in bullets and explicitly commit, â€œI will implement exactly this plan now,â€ noting the checklist step it fulfills.
    *   Edit exactly one file per turn following the plan. Never touch files you were not explicitly instructed to modify.
    *   Lint that file using internal tools and fix all issues.
    *   Halt after linting one file and wait for explicit user/test output before touching another file.
*   ### 2. TDD & Dependency Ordering
    *   One-file TDD cycle: RED test (desired green behavior) â†’ implementation â†’ GREEN test â†’ lint. Documents/types/interfaces are exempt from tests but still follow Readâ†’Halt.
    *   Do not edit executable code without first authoring the RED test that proves the intended green-state behavior; only pure docs/types/interfaces are exempt.
    *   Maintain bottom-up dependency order for both editing and testing: construct types/interfaces/helpers before consumers, then write consumer tests only after producers exist.
    *   Do not advance to another file until the current fileâ€™s proof (tests or documented exemption) is complete and acknowledged.
    *   The agent never runs tests directly; rely on provided outputs or internal reasoning while keeping the application in a provable state.
    *   The agent does not run the userâ€™s terminal commands or tests; use only internal tooling and rely on provided outputs.
*   ### 3. Checklist Discipline
    *   Do not edit the checklist (or its statuses) without explicit instruction; when instructed, change only the specified portion using legal-style numbering.
    *   Execute exactly what the active checklist step instructs with no deviation or â€œcreative interpretation.â€
    *   Each numbered checklist step equals one fileâ€™s entire TDD cycle (deps â†’ types â†’ tests â†’ implementation â†’ proof). Preserve existing detail while adding new requirements.
    *   Document every edit within the checklist. If required edits are missing from the plan, explain the discovery, propose the new step, and halt instead of improvising.
    *   Never update the status of any work step (checkboxes or badges) without explicit instruction.
    *   Following a block of related checklist steps that complete a working implementation, include a commit with a proposed commit message. 
*   ### 4. Builder vs Reviewer Modes
    *   **Builder:** follow the Readâ†’â€¦â†’Halt loop precisely. If a deviation, blocker, or new requirement is discoveredâ€”or the current step simply cannot be completed as writtenâ€”explain the problem, propose the required checklist change, and halt immediately.
    *   **Reviewer:** treat prior reasoning as untrusted. Re-read relevant files/tests from scratch and produce a numbered EO&D list referencing files/sections. Ignore checklist status or RED/GREEN history unless it causes a real defect. If no EO&D are found, state â€œNo EO&D detected; residual risks: â€¦â€
*   ### 5. Strict Typing & Object Construction
    *   Use explicit types everywhere. No `any`, `as`, `as const`, inline ad-hoc types, or castsâ€”except for Supabase clients and intentionally malformed objects in error-handling tests (use dedicated helpers and keep typing strict elsewhere). Every object and variable must be typed. 
    *   Always construct full objects that satisfy existing interfaces/tuples from the relevant type file. Compose complex objects from smaller typed components; never rely on defaults, fallbacks, or backfilling to â€œhealâ€ missing data.
    *   Use type guards to prove and narrow types for the compiler when required.
    *   Never import entire libraries with *, never alias imports, never add "type" to type imports. 
    *   A ternary is not a type guard, a ternary is a default value. Default values are prohibited. 
*   ### 6. Plan Fidelity & Shortcut Ban
    *   Once a solution is described, implement exactly that solution and the userâ€™s instruction. Expedient shortcuts are forbidden without explicit approval.
    *   If you realize you deviated, stop, report it, and wait for direction. Repeating corrected violations triggers halt-and-wait immediately.
    *   If your solution to a challenge is "rewrite the entire file", you have made an error. Stop, do not rewrite the file. Explain the problem to the user and await instruction. 
    *   Do not ruminate on how to work around the "only write to one file per turn". If you are even thinking about the need to work around that limit, you have made a discovery. Stop immediately, report the discovery to the user, and await instruction. 
    *   Refactors must preserve all existing functionality unless the user explicitly authorizes removals; log and identifier fidelity is mandatory.
*   ### 7. Dependency Injection & Architecture
    *   Use explicit dependency injection everywhereâ€”pass every dependency with no hidden defaults or optional fallbacks.
    *   Build adapters/interfaces for every function and work bottom-up so dependencies compile before consumers. Preserve existing functionality, identifiers, and logging unless explicitly told otherwise.
    *   When a file exceeds 600 lines, stop and propose a logical refactoring to decompose the file into smaller parts providing clear SOC and DRY. 
*   ### 8. Testing Standards
    *   Tests assert the desired passing state (no RED/GREEN labels) and new tests are added to the end of the file. Each test covers exactly one behavior.
    *   Use real application functions/mocks, strict typing, and Deno std asserts. Tests must call out which production type/helper each mock mirrors so partial objects are not invented.
    *   Integration tests must exercise real code paths; unit tests stay isolated and mock dependencies explicitly. Never change assertions to match broken codeâ€”fix the code instead.
    *   Tests use the same types, objects, structures, and helpers as the real code, never create new fixtures only for tests - a test that relies on imaginary types or fixtures is invalid. 
    *   Prove the functional gap, the implemented fix, and regressions through tests before moving on; never assume success without proof.
*   ### 9. Logging, Defaults, and Error Handling
    *   Do not add or remove logging unless the user explicitly instructs you to do so.
    *   Adding console logs solely for troubleshooting is exempt from TDD and checklist obligations, but the exemption applies only to the logging statements themselves.
    *   Believe failing tests, linter flags, and user-reported errors literally; fix the stated condition before chasing deeper causes.
    *   If the user flags instruction noncompliance, acknowledge, halt, and wait for explicit directionâ€”do not self-remediate in a way that risks further violations.
*   ### 10. Linting & Proof
    *   After each edit, lint the touched file and resolve every warning/error. Record lint/test evidence in the response (e.g., â€œLint: clean via internal tool; Tests: not run per instructionsâ€).
    *   Evaluate if a linter error can be resolved in-file, or out-of-file. Only resolve in-file linter errors, then report the out-of-file errors and await instruction. 
    *   Testing may produce unresolvable linter errors. Do not silence them with @es flags, create an empty target function, or other work-arounds. The linter error is sometimes itself proof of the RED state of the test. 
    *   Completion proof requires a lint-clean file plus GREEN test evidence (or documented exemption for types/docs).
*   ### 11. Reporting & Traceability
    *   Every response must include: mode declaration, confirmation that this block was re-read, plan bullets (Builder) or EO&D findings (Reviewer), checklist step references, and lint/test evidence.
    *   If tests were not run (per instruction), explicitly state why and list residual risks. If no EO&D are found, state that along with remaining risks.
    *   The agent uses only its own tools and never the userâ€™s terminal.
*   ### 12. Output Constraints
    *   Never output large code blocks (entire files or multi-function dumps) in chat unless the user explicitly requests them.
    *   Never print an entire function and tell the user to paste it in; edit the file directly or provide the minimal diff required.
    *   Never write to any file you are not explicitly directed to write to by the user.
    *   Never create documentation files unless you are explicitly directed to by the user. 

## Checklist-Specific Editing Rules

*   THE AGENT NEVER TOUCHES THE CHECKLIST UNLESS THEY ARE EXPLICITLY INSTRUCTED TO! 
*   When editing checklists, each numbered step (1, 2, 3, etc.) represents editing ONE FILE with a complete TDD cycle.
*   Sub-steps within each numbered step use legal-style numbering (1.a, 1.b, 1.a.i, 1.a.ii, etc.) for the complete TDD cycle for that file.
*   All changes to a single file are described and performed within that file's numbered step.
*   Types files (interfaces, enums) are exempt from RED/GREEN testing requirements.
*   Each file edit includes: RED test â†’ implementation â†’ GREEN test â†’ optional refactor.
*   Steps are ordered by dependency (lowest dependencies first).
*   Preserve all existing detail and work while adding new requirements.
*   Use proper legal-style nesting for sub-steps within each file edit.
*   NEVER create multiple top-level steps for the same file edit operation.
*   Adding console logs is not required to be detailed in checklist work. 

## Example Checklist

*   `[ ]`   1. **Title** Objective
    *   `[ ]`   1.a. [DEPS] A list explaining dependencies of the function, its signature, and its return shape
        *   `[ ]` 1.a.i. eg. `function(something)` in `file.ts` provides this or that
    *   `[ ]`   1.b. [TYPES] A list strictly typing all the objects used in the function
    *   `[ ]`   1.c. [TEST-UNIT] A list explaining the test cases
        *   `[ ]` 1.c.i. Assert `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [SPACE] A list explaining the implementation requirements
        *   `[ ]` 1.d.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [TEST-UNIT] Rerun and expand test proving the function
        *   `[ ]` 1.d.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [TEST-INT] If there is a chain of functions that work together, prove it
        *   `[ ]` 1.d.i. For every cross-function interaction, assert `thisFunction(something)` in `this_file.ts` acts a certain way towards `thatFunction(other)` in `that_file.ts`
    *   `[ ]`   1.d. [CRITERIA] A list explaining the acceptence criteria to consider the work complete and correct. 
    *   `[ ]`   1.e. [COMMIT] A commit that explains the function and its proofs

*   `[ ]`   2. **Title** Objective
    *   `[ ]`   2.a. [DEPS] Low level providers are always build before high level consumers (DI/DIP)
    *   `[ ]`   2.b. [TYPES] DI/DIP and strict typing ensures unit tests can always run 
    *   `[ ]`   2.c. [TEST-UNIT] All functions matching defined external objects and acting as asserted helps ensure integration tests pass

## Legend - You must use this EXACT format. Do not modify it, adapt it, or "improve" it. The bullets, square braces, ticks, nesting, and numbering are ABSOLUTELY MANDATORY and UNALTERABLE. 

*   `[ ]` 1. Unstarted work step. Each work step will be uniquely named for easy reference. We begin with 1.
    *   `[ ]` 1.a. Work steps will be nested as shown. Substeps use characters, as is typical with legal documents.
        *   `[ ]` 1. a. i. Nesting can be as deep as logically required, using roman numerals, according to standard legal document numbering processes.
*   `[âœ…]` Represents a completed step or nested set.
*   `[ðŸš§]` Represents an incomplete or partially completed step or nested set.
*   `[â¸ï¸]` Represents a paused step where a discovery has been made that requires backtracking or further clarification.
*   `[â“]` Represents an uncertainty that must be resolved before continuing.
*   `[ðŸš«]` Represents a blocked, halted, or stopped step or has an unresolved problem or prior dependency to resolve before continuing.

## Component Types and Labels

*   `[DB]` Database Schema Change (Migration)
*   `[RLS]` Row-Level Security Policy
*   `[BE]` Backend Logic (Edge Function / RLS / Helpers / Seed Data)
*   `[API]` API Client Library (`@paynless/api` - includes interface definition in `interface.ts`, implementation in `adapter.ts`, and mocks in `mocks.ts`)
*   `[STORE]` State Management (`@paynless/store` - includes interface definition, actions, reducers/slices, selectors, and mocks)
*   `[UI]` Frontend Component (e.g., in `apps/web`, following component structure rules)
*   `[CLI]` Command Line Interface component/feature
*   `[IDE]` IDE Plugin component/feature
*   `[TEST-UNIT]` Unit Test Implementation/Update
*   `[TEST-INT]` Integration Test Implementation/Update (API-Backend, Store-Component, RLS)
*   `[TEST-E2E]` End-to-End Test Implementation/Update
*   `[DOCS]` Documentation Update (READMEs, API docs, user guides)
*   `[REFACTOR]` Code Refactoring Step
*   `[PROMPT]` System Prompt Engineering/Management
*   `[CONFIG]` Configuration changes (e.g., environment variables, service configurations)
*   `[COMMIT]` Checkpoint for Git Commit (aligns with "feat:", "test:", "fix:", "docs:", "refactor:" conventions)
*   `[DEPLOY]` Checkpoint for Deployment consideration after a major phase or feature set is complete and tested.

# Work Breakdown Structure

*   `[âœ…]` 1. **`[DB]` Fix Job Queue Continuation When First Step Completes**
    *   `[âœ…]` 1.a. `[DEPS]` After reviewing the recipe stage migration files (`*_stage.sql`) and the codebase, the complete list of job statuses that require worker invocation is: `pending` (new jobs, handled by `on_new_job_created` trigger on INSERT), `pending_next_step` (PLAN jobs ready for next recipe step after children complete, MISSING trigger), `pending_continuation` (continuation jobs created by `continueJob` function, MISSING trigger), and `retrying` (jobs being retried, handled by `on_job_retrying` trigger). Statuses that do NOT need worker invocation are: `processing` (worker already owns the job), `waiting_for_children` (internal state set by `processComplexJob`), `waiting_for_prerequisite` (internal state set by prerequisite logic), and terminal states (`completed`, `failed`, `retry_loop_failed`). The database trigger `handle_job_completion()` in `supabase/migrations/20251119160820_retrying_trigger.sql` correctly sets PLAN job status to `pending_next_step` when all child EXECUTE jobs complete (lines 234-236), and also sets jobs to `pending` when prerequisites complete (line 181). The `continueJob` function in `supabase/functions/dialectic-worker/continueJob.ts` sets jobs to `pending_continuation` (line 198). However, there is no trigger configured to invoke the `dialectic-worker` Edge Function when a job's status changes to `pending_next_step` or `pending_continuation`. The worker endpoint (`supabase/functions/dialectic-worker/index.ts`) is invoked via POST request with a job record in the request body (it does not fetch jobs itself), so it must be triggered by a database webhook when the status changes. Currently, there is a generic `invoke_dialectic_worker()` function (defined in `supabase/migrations/20250922165259_document_centric_generation.sql`, lines 59-143) that handles HTTP invocation logic, and it is used by the `on_new_job_created` trigger (fires on INSERT). There is also a separate `handle_job_retrying()` function and `on_job_retrying` trigger (lines 5-137 in `20251119160820_retrying_trigger.sql`) that specifically handle `retrying` status. Rather than creating separate triggers for each status, a better architectural approach is to create a single, generic trigger that fires on status changes and invokes the worker for any status that requires processing. The `processComplexJob` function in `supabase/functions/dialectic-worker/processComplexJob.ts` already handles `pending_next_step` status correctly (it processes ready steps when called), but the worker is never invoked when the status changes to `pending_next_step`, leaving PLAN jobs stuck in that state.
    *   `[âœ…]` 1.b. `[DOCS]` **TRIGGER AUDIT**: Document all current triggers on `dialectic_generation_jobs` table and identify competing/deprecated triggers. Current active triggers: (1) `on_new_job_created` (from `20250722033928_fix_job_trigger.sql`) - fires `AFTER INSERT`, calls `invoke_dialectic_worker()`, handles new jobs; (2) `on_job_retrying` (from `20251119160820_retrying_trigger.sql`) - fires `AFTER UPDATE OF status` when `status = 'retrying'`, calls `handle_job_retrying()`, handles retry logic and retry limit checking before invoking worker; (3) `trigger_handle_job_completion_on_update` (from `20250725182218_prerequisite_job_id.sql`) - fires `AFTER UPDATE OF status`, calls `handle_job_completion()`, handles parent/child dependencies and prerequisite logic; (4) `trigger_handle_job_completion_on_insert` (from `20250725182218_prerequisite_job_id.sql`) - fires `AFTER INSERT`, calls `handle_job_completion()`, handles parent/child dependencies on insert. Potentially deprecated: `on_job_terminal_state` (from `20250711205050_create_job_completion_trigger.sql`, line 150 drops it, replaced by `trigger_handle_job_completion_on_update`). The new generic trigger will replace `on_job_retrying` and may overlap with `on_new_job_created` for UPDATE operations (though `on_new_job_created` only fires on INSERT, so no conflict). Document that `handle_job_retrying()` contains special retry limit checking logic (checks if `attempt_count >= (max_retries + 1)` and marks job as `retry_loop_failed`) that must be preserved in the generic trigger or moved to a separate function.
    *   `[âœ…]` 1.c. `[TEST-INT]` **RED**: In `supabase/integration_tests/triggers/invoke_worker_on_status_change.trigger.test.ts`, write integration tests that prove the generic trigger preserves all existing `handle_job_retrying()` logic and provides new logic for missing state transitions.
        *   `[âœ…]` 1.c.i. Create a test file following the pattern of `handle_job_completion.trigger.test.ts` that sets up test database state (admin client, test user, project, session, stage).
        *   `[âœ…]` 1.c.ii. Write a test case that proves `pending_next_step` status DOES invoke worker (status updates to `pending_next_step` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no trigger for `pending_next_step`.
        *   `[âœ…]` 1.c.iii. Write a test case that proves `pending_continuation` status DOES invoke worker (status updates to `pending_continuation` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no trigger for `pending_continuation`.
        *   `[âœ…]` 1.c.iv. Write a test case that proves `pending` status set via UPDATE (when prerequisites complete) DOES invoke worker (status updates from `waiting_for_prerequisite` to `pending` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no UPDATE trigger for `pending` status.
        *   `[âœ…]` 1.c.v. Write a test case that proves `retrying` status with `attempt_count >= (max_retries + 1)` marks job as `retry_loop_failed` and does NOT invoke worker. This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.vi. Write a test case that proves `retrying` status with `attempt_count < (max_retries + 1)` invokes worker (creates log entry in `dialectic_trigger_logs`). This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.vii. Write a test case that proves test jobs (`is_test_job = true`) with `retrying` status do NOT invoke worker (creates log entry indicating test job skip, but no HTTP invocation attempt). This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.viii. Write a test case that proves status changes that do NOT require worker invocation (`processing`, `waiting_for_children`, `waiting_for_prerequisite`, `completed`, `failed`, `retry_loop_failed`) do NOT invoke worker. This test should pass initially and must continue to pass after the generic trigger is implemented.
        *   `[âœ…]` 1.c.ix. Write a test case that proves status updates where `OLD.status = NEW.status` do NOT invoke worker (no status change). This test must pass after the generic trigger is implemented.
    *   `[âœ…]` 1.d. `[DB]` **GREEN**: In `supabase/migrations/20251119160820_retrying_trigger.sql`, create a generic trigger that invokes the worker when job status changes to any status that requires processing.
        *   `[âœ…]` 1.d.i. Create or update a generic trigger function `invoke_worker_on_status_change()` that: (1) checks if the new status is one that requires worker invocation (`pending`, `pending_next_step`, `pending_continuation`, `retrying`), (2) only processes when status actually changes (not on every update: `OLD.status IS NULL OR OLD.status != NEW.status`), (3) skips test jobs (`COALESCE(NEW.is_test_job, false)`), (4) handles special retry limit checking for `retrying` status (if `attempt_count >= (max_retries + 1)`, mark job as `retry_loop_failed` and return early), (5) reuses the existing `invoke_dialectic_worker()` function's logic for determining worker URL (from vault secrets or local dev URL), extracting `user_jwt` from payload, building request body with job record, and invoking via `net.http_post()` using `pg_net` extension, (6) logs the invocation attempt to `dialectic_trigger_logs` table. Alternatively, update the existing `invoke_dialectic_worker()` function to accept both INSERT and UPDATE operations, and add status-checking logic to filter statuses that require invocation.
        *   `[âœ…]` 1.d.ii. Create a new trigger `on_job_status_change` that: (1) fires `AFTER UPDATE OF status`, (2) executes for each row where `NEW.status IN ('pending', 'pending_next_step', 'pending_continuation', 'retrying') AND (OLD.status IS NULL OR OLD.status != NEW.status)`, (3) calls the generic trigger function. This replaces the need for separate triggers for each status.
        *   `[âœ…]` 1.d.iii. **DEPRECATE**: Drop the `on_job_retrying` trigger and deprecate the `handle_job_retrying()` function (or keep it as a helper if retry limit checking is extracted). Add comments explaining that this generic trigger invokes the worker for any status change that requires processing, including PLAN jobs reaching `pending_next_step` status after all child jobs complete (set by `handle_job_completion` trigger), continuation jobs reaching `pending_continuation` status (set by `continueJob` function), jobs being retried (set by `retryJob` function), and jobs set to `pending` when prerequisites complete (set by `handle_job_completion` trigger). Note that `on_new_job_created` trigger remains active for handling INSERT operations, as it fires on a different event type and does not conflict with the UPDATE-based generic trigger.
    *   `[âœ…]` 1.e. `[TEST-INT]` **GREEN**: Re-run all tests from step 1.c and ensure they now pass. The tests from 1.c.ii, 1.c.iii, and 1.c.iv should now pass because the generic trigger handles these status transitions. The tests from 1.c.v, 1.c.vi, and 1.c.vii should continue to pass, proving that all existing `handle_job_retrying()` logic is preserved.

*   `[âœ…]` 2. **`[STORE]` Fix StageTabCard to Use Valid Document Artifacts Only for Completion Check**
    *   `[âœ…]` 2.a. `[DEPS]` The `selectStageProgressSummary` selector in `packages/store/src/dialecticStore.selectors.ts` (lines 661-733) counts ALL entries in `progress.documents`, including non-document artifacts like `header_context` (which has `artifact_class: 'header_context'` and `file_type: 'json'`). This causes `StageTabCard` to display incorrect counts like "Completed 1/1 documents" when only a `header_context` artifact exists, while `StageRunChecklist` correctly shows "0 of 4 documents" because it uses `selectValidMarkdownDocumentKeys` (lines 1014-1045) which filters out non-markdown artifacts. The `selectValidMarkdownDocumentKeys` selector uses `extractMarkdownDocumentKeysFromRule` (lines 933-1000) to identify valid markdown document keys from recipe steps' `outputs_required` fields, filtering based on `file_type === 'markdown'` or markdown template filenames, and excludes `header_context` artifacts. `StageTabCard` only needs `isComplete` to determine if the SubmitResponses button should be active (it doesn't need to display counts - that's handled by `StageRunChecklist`). The fix requires: (1) modifying `selectStageProgressSummary` to filter document keys using `selectValidMarkdownDocumentKeys` before counting, (2) removing the document count display from `StageTabCard` (lines 116-136 in `apps/web/src/components/dialectic/StageTabCard.tsx`), keeping only the `isComplete` check (line 121) for button activation.
    *   `[âœ…]` 2.b. `[TEST-UNIT]` **RED**: In `packages/store/src/dialecticStore.selectors.test.ts`, write a new test for `selectStageProgressSummary` that verifies it excludes non-document artifacts from counts.
        *   `[âœ…]` 2.b.i. Create a test case that sets up `stageRunProgress` state with both valid markdown document keys (e.g., `'draft_document_markdown'`) and non-document artifacts (e.g., `'HeaderContext'` with `artifact_class: 'header_context'`).
        *   `[âœ…]` 2.b.ii. Mock `selectValidMarkdownDocumentKeys` to return a Set containing only the valid markdown document keys (excluding `header_context`).
        *   `[âœ…]` 2.b.iii. Call `selectStageProgressSummary` with the test state and assert that `totalDocuments` and `completedDocuments` only count valid markdown documents, not `header_context` artifacts.
        *   `[âœ…]` 2.b.iv. Assert that `isComplete` is `false` when only `header_context` is completed but markdown documents are not, and `true` only when all valid markdown documents are completed. This test must fail because `selectStageProgressSummary` currently counts all entries in `progress.documents`.
    *   `[âœ…]` 2.c. `[STORE]` **GREEN**: In `packages/store/src/dialecticStore.selectors.ts`, modify `selectStageProgressSummary` (lines 661-733) to filter document keys using `selectValidMarkdownDocumentKeys` before counting.
        *   `[âœ…]` 2.c.i. Add `selectValidMarkdownDocumentKeys` as a dependency parameter to the selector (or use it within the selector function). The selector function signature is `(state: DialecticStateValues, sessionId: string, stageSlug: string, iterationNumber: number, modelId?: string)`, so we need to pass `stageSlug` to `selectValidMarkdownDocumentKeys(state, stageSlug)`.
        *   `[âœ…]` 2.c.ii. After getting `documentEntries` from `progress.documents` (line 689) and filtering by `modelId` (lines 691-695), filter the resulting `documentKeys` array to only include keys that exist in the Set returned by `selectValidMarkdownDocumentKeys(state, stageSlug)`.
        *   `[âœ…]` 2.c.iii. Use the filtered `documentKeys` array for all subsequent counting logic (lines 697-716) instead of the unfiltered array.
        *   `[âœ…]` 2.c.iv. Ensure the return type and values remain unchanged (the filtering only affects which documents are counted).
    *   `[âœ…]` 2.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 2.b and ensure it now passes. Also verify that existing tests for `selectStageProgressSummary` still pass.
    *   `[âœ…]` 2.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/StageTabCard.test.tsx`, write a test that verifies `StageTabCard` does not display document counts.
        *   `[âœ…]` 2.e.i. Render `StageTabCard` with mock store state that includes stage progress with valid documents.
        *   `[âœ…]` 2.e.ii. Assert that the document count display (the element with `data-testid={`stage-progress-count-${stage.slug}`}`) is NOT rendered (or does not contain count text).
        *   `[âœ…]` 2.e.iii. Assert that `isComplete` check (the element with `data-testid={`stage-progress-label-${stage.slug}`}`) is still rendered when `isComplete` is `true`. This test must fail because `StageTabCard` currently displays counts.
    *   `[âœ…]` 2.f. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/StageTabCard.tsx`, remove the document count display while keeping the completion check for button activation.
        *   `[âœ…]` 2.f.i. Remove the document count display div (lines 116-136) that shows `${progress.completedDocuments} / ${progress.totalDocuments} documents` (line 133).
        *   `[âœ…]` 2.f.ii. Keep the `isComplete` check (lines 121-128) that displays "Completed" label when `progress.isComplete` is `true`, as this is needed for visual feedback and may be used for the SubmitResponses button activation logic.
        *   `[âœ…]` 2.f.iii. Update the `hasDocuments` check (line 80) if needed - it currently checks `progress.totalDocuments > 0`, which should still work correctly after the selector fix (it will now reflect valid documents only). Alternatively, check `progress.isComplete` directly if that's the only requirement.
        *   `[âœ…]` 2.f.iv. Update any TypeScript types or interfaces if the `StageProgressSnapshotSummary` interface (lines 19-23) needs modification (it may not need changes if the selector still returns the same structure, just with filtered counts).
    *   `[âœ…]` 2.g. `[TEST-UNIT]` **GREEN**: Re-run the test from step 2.e and ensure it now passes. Also verify that existing `StageTabCard` tests still pass.
    *   `[âœ…]` 2.h. `[LINT]` Run the linter for all modified files (`packages/store/src/dialecticStore.selectors.ts`, `apps/web/src/components/dialectic/StageTabCard.tsx`) and resolve any warnings or errors.

*   `[âœ…]` 3. **`[UI]` Remove Document Progress Display from SessionContributionsDisplayCard**
    *   `[âœ…]` 3.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` is a container component whose responsibility is to contain child components that display document information, not to display document progress itself. Currently, it displays progress summary "Completed X of Y documents" (lines 474-484) using `selectStageProgressSummary`, which is incorrect because: (1) Displaying document progress is not the container's job - that responsibility belongs to child components like `StageRunChecklist` which already correctly displays progress counts; (2) The progress summary display includes non-document artifacts like `header_context` (which will be fixed by item 2's changes to `selectStageProgressSummary`, but the container shouldn't be displaying progress at all); (3) The container should only use `stageProgressSummary?.isComplete` (line 231) for button activation logic (`canSubmitStageResponses`), not for displaying progress counts. The container correctly uses `isComplete` for button activation (line 390) and `hasFailed` for error display (lines 282-293, 516-540), which are valid container responsibilities (managing UI state and error handling). The fix requires: (1) removing the progress summary display (lines 474-484) that shows "Completed X of Y documents", (2) keeping the `isComplete` check (line 231) for button activation logic, (3) keeping the `hasFailed` check (lines 282-293) for error display if needed, or verifying if error display should also be delegated to child components.
    *   `[âœ…]` 3.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx` (or create if it doesn't exist), write a test that verifies the component does not display document progress summary.
        *   `[âœ…]` 3.b.i. Render `SessionContributionsDisplayCard` with mock store state that includes `stageProgressSummary` with `completedDocuments`, `totalDocuments`, and `outstandingDocuments` properties.
        *   `[âœ…]` 3.b.ii. Assert that the progress summary display (the element containing "Completed X of Y documents" text, or the div at lines 474-484) is NOT rendered (or does not contain progress count text).
        *   `[âœ…]` 3.b.iii. Assert that the `canSubmitStageResponses` logic still works correctly (the submit button is enabled when `isComplete` is `true` and disabled when `isComplete` is `false`). This test must fail because `SessionContributionsDisplayCard` currently displays progress counts.
    *   `[âœ…]` 3.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, remove the document progress summary display while keeping the completion check for button activation.
        *   `[âœ…]` 3.c.i. Remove the progress summary display div (lines 474-484) that shows "Completed {stageProgressSummary.completedDocuments} of {stageProgressSummary.totalDocuments} documents" and "Outstanding: ..." text.
        *   `[âœ…]` 3.c.ii. Keep the `stageProgressSummary` selector usage (lines 218-229) because it's still needed for `isComplete` check (line 231) and `hasFailed` check (lines 282-283) for button activation and error handling logic.
        *   `[âœ…]` 3.c.iii. Ensure the `canSubmitStageResponses` logic (line 231) still works correctly - it uses `stageProgressSummary?.isComplete === true` which is valid container responsibility (button activation logic).
        *   `[âœ…]` 3.c.iv. Verify that error display logic (lines 282-293, 516-540) using `stageProgressSummary?.hasFailed` and `failedDocumentKeys` still works if needed, or consider if error display should also be delegated to child components. For now, keep it since error handling is a valid container responsibility.
    *   `[âœ…]` 3.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 3.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify button activation logic using `isComplete`).
    *   `[âœ…]` 3.e. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 4. **`[UI]` Fix GeneratedContributionCard to Exclude Non-Document Artifacts from Status Display**
    *   `[âœ…]` 4.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` correctly filters out non-document artifacts when rendering document content (lines 136-141 use `selectValidMarkdownDocumentKeys` to check `isValidMarkdownDocument`, and lines 345-468 only render document content when `focusedDocument && isValidMarkdownDocument` is true). However, it still displays the status badge for ALL focused documents, including non-document artifacts like `header_context` (lines 330-334). The `documentDescriptor` is retrieved from `stageRunProgress.documents?.[focusedDocument.documentKey]` (lines 193-196), which includes ALL entries in `progress.documents`, including `header_context` artifacts. The status badge should only be shown for valid markdown documents. Additionally, `StageRunChecklist` (used at line 339) should already filter out non-markdown documents after item 3's fix to `selectStageDocumentChecklist`, but we need to ensure `GeneratedContributionCard` doesn't show status for non-document artifacts even if they somehow become focused. The fix requires: (1) modifying the status badge display (lines 330-334) to only show when `isValidMarkdownDocument` is true, ensuring non-document artifacts like `header_context` don't show completion status even if they become focused.
    *   `[âœ…]` 4.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write a test that verifies the component does not display status badge for non-document artifacts.
        *   `[âœ…]` 4.b.i. Render `GeneratedContributionCard` with mock store state where `focusedDocument` has a `documentKey` that is NOT in `validMarkdownDocumentKeys` (e.g., `'HeaderContext'` with `artifact_class: 'header_context'`).
        *   `[âœ…]` 4.b.ii. Mock `stageRunProgress` to include a `header_context` entry with status `'completed'` in `documents` map.
        *   `[âœ…]` 4.b.iii. Assert that the status badge (the element with `Badge` variant="secondary" containing status text, or the element at lines 330-334) is NOT rendered when `isValidMarkdownDocument` is `false`. This test must fail because `GeneratedContributionCard` currently shows status badge for all focused documents regardless of whether they are valid markdown documents.
    *   `[âœ…]` 4.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/GeneratedContributionCard.tsx`, modify the status badge display to only show for valid markdown documents.
        *   `[âœ…]` 4.c.i. Update the status badge condition (line 330) from `{documentDescriptor && (` to `{documentDescriptor && isValidMarkdownDocument && (` to ensure the badge is only shown when the focused document is a valid markdown document.
        *   `[âœ…]` 4.c.ii. Ensure the `isValidMarkdownDocument` check (lines 136-141) still works correctly - it uses `selectValidMarkdownDocumentKeys(state, stageSlug)` (lines 129-134) to filter out non-markdown artifacts like `header_context`.
        *   `[âœ…]` 4.c.iii. Verify that document content rendering logic (lines 345-468) already correctly filters using `isValidMarkdownDocument` - no changes needed here as it already has the correct check.
        *   `[âœ…]` 4.c.iv. Ensure that `StageRunChecklist` component (line 339) will correctly filter non-markdown documents after item 3's fix to `selectStageDocumentChecklist`, but this fix ensures `GeneratedContributionCard` doesn't show status even if a non-document artifact somehow becomes focused.
    *   `[âœ…]` 4.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 4.b and ensure it now passes. Also verify that existing `GeneratedContributionCard` tests still pass (especially tests that verify status badge displays correctly for valid markdown documents).
    *   `[âœ…]` 4.e. `[LINT]` Run the linter for `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 5. **`[UI]` Change Submit Responses Button to Detect Last Stage and Disable Itself with a "Project Complete" Notice**
    *   `[âœ…]` 5.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` uses a Submit Responses button (`renderSubmitButton()` at lines 387-401) that enables when `canSubmitStageResponses` is `true` (based on `stageProgressSummary?.isComplete` at line 231). The button currently always allows submission and advancement to the next stage when enabled, but should be disabled with a "Project Complete" notice when the user is in the last stage of the dialectic process. The component already has access to `sortedStages` via `selectSortedStages` (line 110) and `processTemplate` via the store (lines 107-109). The `activeStage` is computed from `processTemplate` (lines 117-119). To detect the last stage, we can check if `activeStage.slug` matches the last stage in `sortedStages` array (similar to how `SessionInfoCard` detects `isFinalStageInProcess` at lines 83-87, which checks if there are no transitions where the current stage is the source). Alternatively, we can check if `activeStage.slug === sortedStages[sortedStages.length - 1]?.slug`. The fix requires: (1) adding a `useMemo` hook to compute `isLastStage` based on `sortedStages` and `activeStage`, (2) modifying the button to be disabled when `isLastStage` is `true`, (3) displaying a "Project Complete" notice when `isLastStage` is `true` and `canSubmitStageResponses` is `true` (indicating all documents are complete), (4) ensuring the button text or notice clearly indicates that the project is complete and no further stages are available.
    *   `[âœ…]` 5.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the Submit Responses button is disabled when in the last stage.
        *   `[âœ…]` 5.b.i. Create a test case that mocks store state with `sortedStages` containing multiple stages (e.g., `['thesis', 'antithesis', 'synthesis']`) and sets `activeStageSlug` to the last stage (`'synthesis'`).
        *   `[âœ…]` 5.b.ii. Mock `stageProgressSummary` to have `isComplete: true` (so `canSubmitStageResponses` would normally be `true`).
        *   `[âœ…]` 5.b.iii. Render `SessionContributionsDisplayCard` and assert that the Submit Responses button (the element with text "Submit Responses & Advance Stage" or `data-testid` if available) is disabled when in the last stage, even if `canSubmitStageResponses` is `true`.
        *   `[âœ…]` 5.b.iv. Assert that a "Project Complete" notice is displayed when in the last stage and all documents are complete. This test must fail because the button currently doesn't check for last stage.
    *   `[âœ…]` 5.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, add last stage detection and modify the button to be disabled with a notice when in the last stage.
        *   `[âœ…]` 5.c.i. Add a `useMemo` hook (after line 231) to compute `isLastStage` by checking if `activeStage?.slug === sortedStages[sortedStages.length - 1]?.slug`. Handle the case where `sortedStages` is empty or `activeStage` is null (return `false`).
        *   `[âœ…]` 5.c.ii. Modify `renderSubmitButton()` (lines 387-401) to disable the button when `isLastStage` is `true` by adding `|| isLastStage` to the `disabled` condition (line 390), or create a new computed value `isButtonDisabled = isSubmitting || !canSubmitStageResponses || isLastStage`.
        *   `[âœ…]` 5.c.iii. Add a conditional render after `renderSubmitButton()` (or modify the button section) that displays a "Project Complete" notice when `isLastStage && canSubmitStageResponses` is `true`. The notice should be a non-interactive element (e.g., a `Badge`, `Alert`, or styled `div`) with text like "Project Complete - All stages finished" or similar, styled appropriately to indicate completion.
        *   `[âœ…]` 5.c.iv. Ensure the button text remains "Submit Responses & Advance Stage" (line 398) - the button should just be disabled in the last stage, not change its text. The notice should be separate from the button.
        *   `[âœ…]` 5.c.v. Handle edge cases: if `sortedStages` is empty, `isLastStage` should be `false`. If `activeStage` is null, `isLastStage` should be `false`. If there's only one stage, it should be considered the last stage.
    *   `[âœ…]` 5.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 5.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify button enabling/disabling based on `canSubmitStageResponses`).
    *   `[âœ…]` 5.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the button still works correctly for non-last stages.
        *   `[âœ…]` 5.e.i. Create a test case that mocks store state with `sortedStages` containing multiple stages and sets `activeStageSlug` to a non-last stage (e.g., `'thesis'` when stages are `['thesis', 'antithesis', 'synthesis']`).
        *   `[âœ…]` 5.e.ii. Mock `stageProgressSummary` to have `isComplete: true` and assert that the Submit Responses button is enabled (not disabled) when not in the last stage.
        *   `[âœ…]` 5.e.iii. Assert that the "Project Complete" notice is NOT displayed when not in the last stage. This test should pass immediately after step 5.c if implemented correctly.
    *   `[âœ…]` 5.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 5.e and ensure it passes. This verifies that the fix doesn't break normal button behavior for non-last stages.
    *   `[âœ…]` 5.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 6. **`[UI]` Remove Conditional "Export Final" Button from SessionInfoCard**
    *   `[âœ…]` 6.a. `[DEPS]` The `SessionInfoCard` component in `apps/web/src/components/dialectic/SessionInfoCard.tsx` displays a conditional "Export Final" button (lines 287-297) that appears when `isFinalStageInProcess` is `true`. However, there is now an always-visible "Export" button (lines 247-257) that provides the same functionality, making the conditional "Export Final" button redundant and deprecated. The `isFinalStageInProcess` logic (lines 83-87) is computed based on process template transitions and is currently used in two places: (1) to hide the Submit button in the final stage (line 261: `!isFinalStageInProcess`), and (2) to show the conditional "Export Final" button (line 288: `isFinalStageInProcess && project &&`). The fix requires: (1) removing the conditional "Export Final" button (lines 287-297), (2) keeping the `isFinalStageInProcess` logic since it's still needed to hide the Submit button in the final stage (line 261), (3) removing the comment "Final stage export button" (line 287) as it's no longer relevant. The always-visible "Export" button (lines 247-257) provides the export functionality for all stages, eliminating the need for the conditional button.
    *   `[âœ…]` 6.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionInfoCard.test.tsx`, write a test that verifies the "Export Final" button is never displayed.
        *   `[âœ…]` 6.b.i. Create a test case that mocks store state with `isFinalStageInProcess` set to `true` (by setting up a project with process template transitions where the current stage has no outgoing transitions).
        *   `[âœ…]` 6.b.ii. Mock `project` to be non-null and render `SessionInfoCard`.
        *   `[âœ…]` 6.b.iii. Assert that the "Export Final" button (the element with text "Export Final" or `data-testid` if available) is NOT rendered, even when `isFinalStageInProcess` is `true`.
        *   `[âœ…]` 6.b.iv. Assert that the always-visible "Export" button (lines 247-257) is still rendered. This test must fail because the conditional "Export Final" button currently displays when `isFinalStageInProcess` is `true`.
    *   `[âœ…]` 6.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionInfoCard.tsx`, remove the conditional "Export Final" button while keeping the Submit button hiding logic.
        *   `[âœ…]` 6.c.i. Remove the conditional "Export Final" button block (lines 287-297), including the comment "Final stage export button" (line 287), the conditional check `{isFinalStageInProcess && project && (`, the `ExportProjectButton` component with "Export Final" text, and the closing `)}`.
        *   `[âœ…]` 6.c.ii. Keep the `isFinalStageInProcess` computation (lines 83-87) since it's still needed to hide the Submit button in the final stage (line 261: `!isFinalStageInProcess`). Do not remove this logic.
        *   `[âœ…]` 6.c.iii. Verify that the always-visible "Export" button (lines 247-257) remains unchanged and continues to provide export functionality for all stages.
        *   `[âœ…]` 6.c.iv. Ensure the Submit button hiding logic (lines 259-285) still works correctly - it should hide the Submit button when `isFinalStageInProcess` is `true`, which is the correct behavior since users shouldn't submit to advance when already in the final stage.
    *   `[âœ…]` 6.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 6.b and ensure it now passes. Also verify that existing `SessionInfoCard` tests still pass (especially tests that verify the always-visible "Export" button and the Submit button hiding logic).
    *   `[âœ…]` 6.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionInfoCard.test.tsx`, write a test that verifies the always-visible "Export" button is still rendered in all stages.
        *   `[âœ…]` 6.e.i. Create test cases for both final and non-final stages (by setting up projects with different process template transition configurations).
        *   `[âœ…]` 6.e.ii. Assert that the always-visible "Export" button (lines 247-257) is rendered regardless of whether `isFinalStageInProcess` is `true` or `false`. This test should pass immediately after step 6.c if the always-visible button was not modified.
    *   `[âœ…]` 6.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 6.e and ensure it passes. This verifies that the always-visible "Export" button continues to work correctly for all stages.
    *   `[âœ…]` 6.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionInfoCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 7. **`[UI]` Remove Conditional "Export Final" Button from SessionInfoCard**
    *   `[âœ…]` 7.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` displays a "Generating documents" loader (lines 510-524) when `isGenerating` is `true`. The `isGenerating` logic (lines 308-311) checks `contributionGenerationStatus === 'generating'`, which is a global state that can be `'generating'` even when: (1) no models are selected for the current session (`selectedModelIds` might be empty), (2) the Generate button hasn't been clicked (the status might persist from a previous session or stage), (3) a different session is generating (the global status applies to all sessions). The component already computes rich document state via `documentsByModel` (lines 169-216) which uses `selectStageDocumentChecklist` and includes document statuses from `stageRunProgress`, and `stageProgressSummary` (lines 218-229) which summarizes document states. Documents in `documentsByModel` have `status` fields that can be `'generating'`, `'completed'`, `'failed'`, or `'not_started'` (as defined in `packages/types/src/dialectic.types.ts`). The component already uses this pattern for `failedDocumentKeys` (lines 294-306) which filters documents by `status === 'failed'`. The fix requires: (1) replacing the global `contributionGenerationStatus === 'generating'` check in `isGenerating` (line 309) with a check that examines document statuses from `documentGroups` (derived from `documentsByModel`) to see if any documents have `status === 'generating'`, following the same pattern as `failedDocumentKeys`, (2) ensuring the check only looks at documents for the current session/stage/iteration (which is already scoped by `documentsByModel`), (3) ensuring the loader display logic (lines 510-524) correctly reflects the session-specific generation state. This approach is more consistent with existing component patterns and uses data already computed rather than requiring a new hook or selector.
    *   `[âœ…]` 7.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the loader does not display when no documents in the current session are generating.
        *   `[âœ…]` 7.b.i. Create a test case that mocks store state where `contributionGenerationStatus` is `'generating'` but all documents in `stageRunProgress` for the current session/stage/iteration have status `'completed'` or `'not_started'` (simulating a different session generating or generation completed).
        *   `[âœ…]` 7.b.ii. Mock store state where `contributionGenerationStatus` is `'generating'` but `selectedModelIds` is empty and no documents exist in `stageRunProgress` for the current session.
        *   `[âœ…]` 7.b.iii. Render `SessionContributionsDisplayCard` with the test state and assert that the "Generating documents" loader (the element with text "Generating documents" or `data-testid` if available) is NOT displayed when no documents have `status === 'generating'` in the current session's `documentsByModel`.
        *   `[âœ…]` 7.b.iv. Assert that the loader is NOT displayed when `contributionGenerationStatus` is `'generating'` but documents are not in generating state. This test must fail because the component currently uses the global `contributionGenerationStatus` which can be `'generating'` even when the current session's documents aren't generating.
    *   `[âœ…]` 7.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, replace the global generation status check with a document status check using existing computed data.
        *   `[âœ…]` 7.c.i. Add a new `useMemo` hook (after line 306, after `failedDocumentKeys` definition) to compute `hasGeneratingDocuments` by checking if any documents in `documentGroups` have `status === 'generating'`. The logic should follow the same pattern as `failedDocumentKeys`: `const hasGeneratingDocuments = useMemo(() => { return documentGroups.some(([, documents]) => documents.some((document) => document.status === 'generating')); }, [documentGroups]);`. This uses the same data source (`documentGroups`) and pattern as the existing `failedDocumentKeys` check.
        *   `[âœ…]` 7.c.ii. Modify `isGenerating` (lines 308-311) to check `hasGeneratingDocuments` instead of `contributionGenerationStatus === 'generating'`. The new logic should be: `const isGenerating = hasGeneratingDocuments && failedDocumentKeys.length === 0 && !generationError;`. This ensures the loader only shows when documents in the current session/stage/iteration are actually generating, using data already computed and scoped correctly.
        *   `[âœ…]` 7.c.iii. Remove the `contributionGenerationStatus` selector usage (lines 141-143) since it's no longer needed - the component will derive generation state from document statuses instead of the global status. Also remove `selectContributionGenerationStatus` from imports if it's not used elsewhere.
        *   `[âœ…]` 7.c.iv. Ensure the loader display logic (lines 510-524) correctly uses the updated `isGenerating` value - no changes needed here as it already uses `isGenerating`.
    *   `[âœ…]` 7.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 7.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify the loader displays correctly when generation is active).
    *   `[âœ…]` 7.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the loader displays correctly when documents in the current session are generating.
        *   `[âœ…]` 7.e.i. Create a test case that mocks store state where `stageRunProgress` for the current session/stage/iteration contains documents with `status: 'generating'` (e.g., set `draft_document_outline` document descriptor status to `'generating'`).
        *   `[âœ…]` 7.e.ii. Ensure `failedDocumentKeys` will be empty (no documents with `status === 'failed'`) and `generationError` is null.
        *   `[âœ…]` 7.e.iii. Render `SessionContributionsDisplayCard` and assert that the "Generating documents" loader IS displayed when documents in the current session have `status === 'generating'`. This test should pass immediately after step 7.c if implemented correctly.
    *   `[âœ…]` 7.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 7.e and ensure it passes. This verifies that the loader still works correctly when documents are actually generating in the current session.
    *   `[âœ…]` 7.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 8. **`[BE]` Fix Recipe Step Output Type to Use Model Contribution File Type**
    *   `[âœ…]` 8.a. `[DEPS]` Recipe steps in database migrations have `output_type: 'rendered_document'` (the final product), but planners expect `output_type` to be a `ModelContributionFileType` (what the model produces). The actual document key (e.g., `business_case`, `feature_spec`, `technical_approach`, `success_metrics`) is found in `outputs_required.documents[0].document_key`. The `getStageRecipe.ts` function in `supabase/functions/dialectic-service/getStageRecipe.ts` (lines 89-99) validates `output_type` against only 3 values (`HeaderContext`, `AssembledDocumentJson`, `RenderedDocument`), which is architecturally incorrect: `HeaderContext` and `AssembledDocumentJson` are backend-only and should never be sent to the frontend. The `OutputType` type in `supabase/functions/dialectic-service/dialectic.interface.ts` (lines 627-631) incorrectly includes these backend-only types. All planners (`planPerSourceDocument`, `planAllToOne`, `planPerSourceGroup`, `planPairwiseByOrigin`, `planPerSourceDocumentByLineage`, `planPerModel`) validate that `recipeStep.output_type` must be a `ModelContributionFileType` (using `isModelContributionFileType` check), and they use it directly to set `jobPayload.output_type` for EXECUTE jobs. The `shouldEnqueueRenderJob` function in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` checks if `outputType` matches any `document_key` in `outputs_required` that has markdown `file_type`, so `output_type` should match `document_key` for proper rendering logic. Every `document_key` is a `FileType`, and `output_type` should match the `document_key` to create semantic alignment. The fix requires: (1) defining `OutputType` as a subset of `ModelContributionFileTypes` that become rendered documents (excludes `HeaderContext`, `AssembledDocumentJson`, and other backend-only types), (2) updating all stage migration files (`20251006194531_thesis_stage.sql`, `20251006194542_antithesis_stage.sql`, `20251006194558_parenthesis_stage.sql`, `20251006194549_synthesis_stage.sql`, `20251006194605_paralysis_stage.sql`) to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'` for all EXECUTE job steps, (3) updating `getStageRecipe.ts` to validate that `output_type` is a `ModelContributionFileType` and is in the `OutputType` subset (renderable types), and pass it through as-is to the DTO (no mapping needed since `output_type` will match `document_key`), (4) filtering out PLAN job steps with `header_context` output from the DTO sent to frontend (or handling them separately) since they are backend-only.
    *   `[âœ…]` 8.b. `[TYPES]` In `supabase/functions/dialectic-service/dialectic.interface.ts`, define `OutputType` as a subset of `ModelContributionFileTypes` that become rendered documents. This excludes backend-only types like `HeaderContext`, `AssembledDocumentJson`, `ModelContributionRawJson`, `PairwiseSynthesisChunk`, `ReducedSynthesis`, `Synthesis`, `header_context_pairwise`, `SynthesisHeaderContext`, and other intermediate types that don't become user-facing rendered documents. The `OutputType` should include all `ModelContributionFileTypes` that have markdown outputs in `outputs_required` (e.g., `business_case`, `feature_spec`, `technical_approach`, `success_metrics`, `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `non_functional_requirements`, `dependency_map`, `comparison_vector`, `product_requirements`, `system_architecture`, `tech_stack`, `technical_requirements`, `master_plan`, `milestone_schema`, `updated_master_plan`, `actionable_checklist`, `advisor_recommendations`). Import `ModelContributionFileTypes` from `../_shared/types/file_manager.types.ts` and create a manually curated union type `OutputType` that includes only the renderable `ModelContributionFileTypes`. This provides type safety and semantic clarity: `OutputType` represents "FileTypes that become RenderedDocument". Replace the current `OutputType` definition (lines 627-631) with this new subset type.
    *   `[âœ…]` 8.b.i. `[TYPES]` In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, create a type guard function `isOutputType(value: ModelContributionFileTypes): value is OutputType` that validates if a `ModelContributionFileType` is in the `OutputType` subset (renderable types). This function should check against a runtime map similar to `MODEL_CONTRIBUTION_FILE_TYPES_MAP`, but only include the renderable types. Import `OutputType` from `../../dialectic-service/dialectic.interface.ts` and create `OUTPUT_TYPES_MAP: { [K in OutputType]: true }` with all renderable types, then implement `isOutputType` to check if the value exists in this map.
    *   `[âœ…]` 8.c. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-service/getStageRecipe.test.ts` (or create if it doesn't exist), write tests that verify `getStageRecipe` accepts renderable `ModelContributionFileTypes` (those in `OutputType`) for EXECUTE job steps and filters out backend-only types.
        *   `[âœ…]` 8.c.i. Create a test case that mocks a recipe step with `output_type: 'business_case'` (a renderable `ModelContributionFileType` in `OutputType`) and `job_type: 'EXECUTE'`, and assert that `getStageRecipe` returns successfully with `output_type: 'business_case'` in the response DTO.
        *   `[âœ…]` 8.c.ii. Create a test case that mocks a recipe step with `output_type: 'feature_spec'` (a renderable `ModelContributionFileType` in `OutputType`) and `job_type: 'EXECUTE'`, and assert that `getStageRecipe` returns successfully with `output_type: 'feature_spec'` in the response DTO.
        *   `[âœ…]` 8.c.iii. Create a test case that mocks a recipe step with `output_type: 'header_context'` (a backend-only `ModelContributionFileType` NOT in `OutputType`) and `job_type: 'PLAN'`, and assert that `getStageRecipe` either: (a) excludes this step from the DTO response (filters it out), or (b) returns an error indicating backend-only types cannot be sent to frontend. PLAN jobs with `header_context` are backend-only and should not appear in the frontend DTO.
        *   `[âœ…]` 8.c.iv. Create a test case that mocks a recipe step with `output_type: 'rendered_document'` (which is NOT a `ModelContributionFileType`), and assert that `getStageRecipe` returns an error. The database should never have `output_type: 'rendered_document'` after migrations are updated.
        *   `[âœ…]` 8.c.v. These tests must fail initially because `getStageRecipe.ts` validation (lines 89-99) only accepts 3 values (`HeaderContext`, `AssembledDocumentJson`, `RenderedDocument`), not the renderable `ModelContributionFileTypes`.
    *   `[âœ…]` 8.d. `[BE]` **GREEN**: In `supabase/functions/dialectic-service/getStageRecipe.ts`, update the validation logic (lines 89-99) to validate that `output_type` is a `ModelContributionFileType` and is in the `OutputType` subset (renderable types), and filter out backend-only steps from the DTO.
        *   `[âœ…]` 8.d.i. Import `ModelContributionFileTypes`, `isModelContributionFileType`, and `OutputType` from `../_shared/types/file_manager.types.ts`, `../_shared/utils/type-guards/type_guards.file_manager.ts`, and `./dialectic.interface.ts` respectively (if not already imported).
        *   `[âœ…]` 8.d.ii. Replace the restrictive validation (lines 92-99) that only accepts 3 values with: (1) a check that uses `isModelContributionFileType(rawType)` to validate that `output_type` is a valid `ModelContributionFileType`, (2) a check that uses `isOutputType(rawType)` (from step 8.b.i) to verify `rawType` is in the `OutputType` subset (renderable types). If `output_type` is a `ModelContributionFileType` but not in `OutputType`, either filter out the step (for backend-only types like `header_context`) or return an error.
        *   `[âœ…]` 8.d.iii. Set `mappedOutputType` to `rawType as OutputType` after validating it's in the `OutputType` subset. The `output_type` passes through as-is (no mapping needed) since it will match `document_key` in `outputs_required`.
        *   `[âœ…]` 8.d.iv. For PLAN job steps with `output_type: 'header_context'` (or other backend-only types), either: (a) filter them out before adding to the `normalized` array (skip adding the DTO), or (b) return an error. Option (a) is preferred - backend-only steps should not be sent to the frontend.
        *   `[âœ…]` 8.d.v. Remove the error logging that references only the 3 specific values, and update it to log any invalid `output_type` that fails the `isModelContributionFileType` check or is not in the `OutputType` subset.
    *   `[âœ…]` 8.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 8.c and ensure they now pass. Verify that: (1) renderable `ModelContributionFileTypes` pass validation and appear in the DTO, (2) backend-only types like `header_context` are filtered out from the DTO, (3) invalid types return errors. Also verify that existing `getStageRecipe` tests still pass, but note that PLAN job steps with `header_context` should no longer appear in the DTO response (they are filtered out as backend-only).
    *   `[âœ…]` 8.f. `[DB]` In `supabase/migrations/20251006194531_thesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.g.i. For template step `'thesis_generate_business_case'` (line 323): change `output_type` from `'rendered_document'` (line 329) to `'business_case'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 336).
        *   `[âœ…]` 8.g.ii. For template step `'thesis_generate_feature_spec'` (line 485): change `output_type` from `'rendered_document'` (line 491) to `'feature_spec'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 498).
        *   `[âœ…]` 8.g.iii. For template step `'thesis_generate_technical_approach'` (line 643): change `output_type` from `'rendered_document'` (line 649) to `'technical_approach'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 656).
        *   `[âœ…]` 8.g.iv. For template step `'thesis_generate_success_metrics'` (line 800): change `output_type` from `'rendered_document'` (line 806) to `'success_metrics'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 813).
        *   `[âœ…]` 8.g.v. For instance step `'thesis_generate_business_case'` (line 1060): change `output_type` from `'rendered_document'` (line 1065) to `'business_case'`.
        *   `[âœ…]` 8.g.vi. For instance step `'thesis_generate_feature_spec'` (line 1132): change `output_type` from `'rendered_document'` (line 1137) to `'feature_spec'`.
        *   `[âœ…]` 8.g.vii. For instance step `'thesis_generate_technical_approach'` (line 1200): change `output_type` from `'rendered_document'` (line 1205) to `'technical_approach'`.
        *   `[âœ…]` 8.g.viii. For instance step `'thesis_generate_success_metrics'` (line 1267): change `output_type` from `'rendered_document'` (line 1272) to `'success_metrics'`.
        *   `[âœ…]` 8.g.ix. Verify that PLAN job steps (e.g., `'thesis_build_stage_header'`) keep their `output_type: 'header_context'` unchanged, as these are correct.
    *   `[âœ…]` 8.h. `[DB]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.h.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for antithesis stage include: `'business_case_critique'`, `'technical_feasibility_assessment'`, `'risk_register'`, `'non_functional_requirements'`, `'dependency_map'`, `'comparison_vector'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.i. `[DB]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.i.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for parenthesis stage include: `'technical_requirements'`, `'master_plan'`, `'milestone_schema'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.j. `[DB]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.j.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for synthesis stage include: `'product_requirements'`, `'system_architecture'`, `'tech_stack'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.k. `[DB]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.k.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for paralysis stage include: `'updated_master_plan'`, `'actionable_checklist'`, `'advisor_recommendations'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.l. `[TEST-INT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file, write a test that verifies planners can successfully create EXECUTE jobs from recipe steps with `output_type` set to actual `ModelContributionFileTypes` (e.g., `'business_case'`) when using real recipe steps from the updated database.
        *   `[âœ…]` 8.l.i. Create a test case that sets up a PLAN job and fetches real recipe steps from the database (after migrations are updated in steps 8.f-8.k).
        *   `[âœ…]` 8.l.ii. Find an EXECUTE job recipe step (e.g., `'thesis_generate_business_case'`) and verify it has `output_type: 'business_case'` (not `'rendered_document'`).
        *   `[âœ…]` 8.l.iii. Call the planner function (e.g., `planPerSourceDocument`) with the recipe step and source documents, and assert that it successfully creates child EXECUTE job payloads with `output_type: 'business_case'`.
        *   `[âœ…]` 8.l.iv. Assert that the `isModelContributionFileType` validation in the planner passes (no error is thrown).
        *   `[âœ…]` 8.l.v. Verify that `output_type` in the recipe step matches `document_key` in `outputs_required.documents[0].document_key` (e.g., both are `'business_case'`), ensuring semantic alignment. This test verifies the end-to-end flow: database has correct `output_type` matching `document_key`, `getStageRecipe` returns it correctly, and planners can use it to create EXECUTE jobs.
    *   `[âœ…]` 8.m. `[TEST-INT]` **GREEN**: Re-run the test from step 8.l and ensure it passes. This verifies that the complete end-to-end flow works correctly after all changes (types updated, source code updated, database migrations applied).
    *   `[âœ…]` 8.n. `[LINT]` Run the linter for all modified files (`supabase/functions/dialectic-service/dialectic.interface.ts`, `supabase/functions/dialectic-service/getStageRecipe.ts`) and resolve any warnings or errors.

*   `[âœ…]` 9. **`[BE]` Fix assembleTurnPrompt to Query Header Context by Contribution ID Instead of Storage Path**
    *   `[âœ…]` 9.a. `[DEPS]` The `assembleTurnPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` (lines 47-51, 78-82) requires `payload.header_context_resource_id` (a storage path string) and hardcodes bucket `"SB_CONTENT_STORAGE_BUCKET"` to download header context. However, planners (e.g., `planPerSourceDocument` in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, line 88) set `inputs[`${doc.contribution_type}_id`] = doc.id`, which for `header_context` creates `inputs.header_context_id` with the contribution ID. Header context is stored as a contribution in `dialectic_contributions` with `contribution_type = 'header_context'`. The `assemblePlannerPrompt` and `assembleSeedPrompt` functions correctly use `gatherInputsForStage` which queries contributions by metadata (stage, iteration, etc.) and gets storage details from the database record, avoiding duplication of storage paths in payloads. The fix requires: (1) updating `assembleTurnPrompt` to read `inputs.header_context_id` from `job.payload.inputs.header_context_id`, query `dialectic_contributions` by that ID to get `storage_bucket`, `storage_path`, and `file_name`, construct the full path, and download using the record's bucket (matching the pattern in `gatherInputsForStage` lines 104-185), (2) removing the `header_context_resource_id` requirement from the payload precondition check (line 47-51), (3) removing the hardcoded bucket usage, ensuring the function uses the database as the single source of truth for storage details.
    *   `[âœ…]` 9.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, write tests that verify `assembleTurnPrompt` queries header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 9.b.i. Create a test case that mocks a job payload with `inputs.header_context_id` set to a contribution ID (e.g., `"contrib-123"`), and mocks a database query to `dialectic_contributions` that returns a contribution record with `storage_bucket: "dialectic_contributions"`, `storage_path: "path/to/header"`, `file_name: "header_context.json"`, and `contribution_type: "header_context"`.
        *   `[âœ…]` 9.b.ii. Mock `downloadFromStorage` to be called with the contribution's `storage_bucket` and the constructed path (`${storage_path}/${file_name}`) instead of hardcoded `"SB_CONTENT_STORAGE_BUCKET"` and `payload.header_context_resource_id`.
        *   `[âœ…]` 9.b.iii. Assert that `assembleTurnPrompt` successfully queries the contribution by ID, constructs the storage path from the record, and downloads using the record's bucket. This test must fail because `assembleTurnPrompt` currently requires `header_context_resource_id` in payload and uses hardcoded bucket.
        *   `[âœ…]` 9.b.iv. Create a test case that verifies `assembleTurnPrompt` throws an error when `inputs.header_context_id` is missing or invalid (contribution not found in database). This test must fail because the function currently checks for `header_context_resource_id` instead.
        *   `[âœ…]` 9.b.v. Create a test case that verifies `assembleTurnPrompt` throws an error when the contribution record is missing `storage_bucket`, `storage_path`, or `file_name`. This test must fail initially but should pass after implementation.
    *   `[âœ…]` 9.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts`, update the function to query header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 9.c.i. Remove the precondition check for `payload.header_context_resource_id` (lines 47-51) that throws an error when missing.
        *   `[âœ…]` 9.c.ii. Add a precondition check that verifies `job.payload.inputs` exists and is a record, and that `job.payload.inputs.header_context_id` is a string (the contribution ID).
        *   `[âœ…]` 9.c.iii. Query `dialectic_contributions` by `inputs.header_context_id` to get the contribution record, selecting `storage_bucket`, `storage_path`, `file_name`, and `contribution_type` fields. Verify the contribution exists and has `contribution_type = 'header_context'`.
        *   `[âœ…]` 9.c.iv. Validate that the contribution record has `storage_bucket`, `storage_path`, and `file_name` (all non-null strings). If any are missing, throw an error indicating the contribution is missing storage details.
        *   `[âœ…]` 9.c.v. Construct the full storage path as `${contrib.storage_path}/${contrib.file_name}` (or just `contrib.storage_path` if `file_name` is empty, matching the pattern in `gatherInputsForStage` line 144-146).
        *   `[âœ…]` 9.c.vi. Replace the `downloadFromStorage` call (lines 78-82) to use `contrib.storage_bucket` instead of hardcoded `"SB_CONTENT_STORAGE_BUCKET"`, and use the constructed path instead of `job.payload.header_context_resource_id`.
        *   `[âœ…]` 9.c.vii. Ensure all error messages are updated to reference the contribution ID lookup instead of `header_context_resource_id`.
    *   `[âœ…]` 9.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 9.b and ensure they now pass. Also verify that existing `assembleTurnPrompt` tests still pass (update any tests that mock `header_context_resource_id` to instead mock `inputs.header_context_id` and the database query).
    *   `[âœ…]` 9.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` and resolve any warnings or errors.

*   `[âœ…]` 10. **`[BE]` Fix assembleContinuationPrompt to Query Header Context by Contribution ID Instead of Storage Path**
    *   `[âœ…]` 10.a. `[DEPS]` The `assembleContinuationPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts` (lines 82-89) optionally uses `payload.header_context_resource_id` (storage path) and hardcodes bucket `"dialectic_project_resources"`. However, planners (e.g., `planPerSourceDocument` in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, line 88) set `inputs[`${doc.contribution_type}_id`] = doc.id`, which for `header_context` creates `inputs.header_context_id` with the contribution ID. Header context is stored as a contribution in `dialectic_contributions` with `contribution_type = 'header_context'`. The `assemblePlannerPrompt` and `assembleSeedPrompt` functions correctly use `gatherInputsForStage` which queries contributions by metadata (stage, iteration, etc.) and gets storage details from the database record, avoiding duplication of storage paths in payloads. The fix requires: (1) updating `assembleContinuationPrompt` to optionally read `inputs.header_context_id` and query the contribution if present, (2) removing the hardcoded bucket usage and storage path requirement, ensuring the function uses the database as the single source of truth for storage details, (3) keeping the optional behavior: if `inputs.header_context_id` is missing, the function should continue without header context (no error thrown).
    *   `[âœ…]` 10.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.test.ts`, write tests that verify `assembleContinuationPrompt` optionally queries header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 10.b.i. Create a test case that mocks a job payload with `inputs.header_context_id` set to a contribution ID, and mocks a database query that returns a contribution record with storage details, and asserts that `assembleContinuationPrompt` successfully queries and downloads the header context using the contribution's bucket and path.
        *   `[âœ…]` 10.b.ii. Create a test case that verifies `assembleContinuationPrompt` works correctly when `inputs.header_context_id` is missing (header context is optional for continuation prompts), and no header context is included in the final prompt. This test should pass initially since the function already handles optional header context (line 82-106).
        *   `[âœ…]` 10.b.iii. Create a test case that verifies `assembleContinuationPrompt` throws an error when `inputs.header_context_id` is provided but the contribution is not found in the database. This test must fail because the function currently uses `header_context_resource_id` directly without querying.
        *   `[âœ…]` 10.b.iv. Create a test case that verifies `assembleContinuationPrompt` uses the contribution's `storage_bucket` instead of hardcoded `"dialectic_project_resources"` when downloading header context. This test must fail because the function currently hardcodes the bucket (line 87).
    *   `[âœ…]` 10.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts`, update the function to optionally query header context by contribution ID from `inputs` instead of using `header_context_resource_id` in payload.
        *   `[âœ…]` 10.c.i. Update the header context fetching logic (lines 82-106) to check for `job.payload.inputs?.header_context_id` instead of `job.payload?.header_context_resource_id`.
        *   `[âœ…]` 10.c.ii. If `inputs.header_context_id` exists and is a string, query `dialectic_contributions` by that ID to get the contribution record with `storage_bucket`, `storage_path`, `file_name`, and `contribution_type` fields. Verify the contribution exists and has `contribution_type = 'header_context'`.
        *   `[âœ…]` 10.c.iii. Validate that the contribution record has `storage_bucket`, `storage_path`, and `file_name` (all non-null strings). If any are missing, throw an error indicating the contribution is missing storage details.
        *   `[âœ…]` 10.c.iv. Construct the full storage path as `${contrib.storage_path}/${contrib.file_name}` (or just `contrib.storage_path` if `file_name` is empty).
        *   `[âœ…]` 10.c.v. Replace the `downloadFromStorage` call (lines 85-89) to use `contrib.storage_bucket` instead of hardcoded `"dialectic_project_resources"`, and use the constructed path instead of `headerResourceId`.
        *   `[âœ…]` 10.c.vi. Ensure all error messages are updated to reference the contribution ID lookup instead of `header_context_resource_id`.
        *   `[âœ…]` 10.c.vii. Keep the optional behavior: if `inputs.header_context_id` is missing or undefined, the function should continue without header context (no error thrown), matching the current optional behavior.
    *   `[âœ…]` 10.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 10.b and ensure they now pass. Also verify that existing `assembleContinuationPrompt` tests still pass (update any tests that mock `header_context_resource_id` to instead mock `inputs.header_context_id` and the database query).
    *   `[âœ…]` 10.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts` and resolve any warnings or errors.

*   `[âœ…]` 11. **`[BE]` Fix planAllToOne to Extract and Validate document_key**
    *   `[âœ…]` 11.a. `[DEPS]` The `planAllToOne` planner function in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 11.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, write tests that verify `planAllToOne` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 11.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'business_case'` and asserts that the created payload has `document_key: 'business_case'`.
        *   `[âœ…]` 11.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property (e.g., only has `header_context_artifact`), and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planAllToOne` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 11.b.v. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key` set to `null` or empty string, and asserts that `planAllToOne` throws an error (step outputs documents but `document_key` is invalid).
        *   `[âœ…]` 11.b.vi. Create a test case that mocks a recipe step with `outputs_required` missing or undefined, and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.vii. These tests must fail because `planAllToOne` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 11.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 11.c.i. Before creating `newPayload` (line 44), check if the step outputs documents: verify that `recipeStep.outputs_required` exists, is an object, has a `documents` property that is an array, and the array has at least one item.
        *   `[âœ…]` 11.c.ii. If the step outputs documents (condition from 11.c.i is true), extract `document_key` from `recipeStep.outputs_required.documents[0].document_key`. If `documents[0]` is missing, not an object, or missing `document_key` property, throw an error: `"planAllToOne requires recipeStep.outputs_required.documents[0].document_key but it is missing"`.
        *   `[âœ…]` 11.c.iii. If the step outputs documents, validate that `document_key` is a non-empty string. If it's null, undefined, empty string, or not a string type, throw an error: `"planAllToOne requires recipeStep.outputs_required.documents[0].document_key to be a non-empty string, but received: ${typeof document_key === 'string' ? `'${document_key}'` : String(document_key)}"`.
        *   `[âœ…]` 11.c.iv. If the step outputs documents, store the validated `document_key` in a variable. If the step does not output documents, leave `document_key` as `undefined`.
        *   `[âœ…]` 11.c.v. Add `document_key` to the `newPayload` object (after line 68, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 11.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 11.b and ensure they now pass. Also verify that existing `planAllToOne` tests still pass.
    *   `[âœ…]` 11.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 12. **`[BE]` Fix planPairwiseByOrigin to Extract and Validate document_key**
    *   `[âœ…]` 12.a. `[DEPS]` The `planPairwiseByOrigin` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 12.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, write tests that verify `planPairwiseByOrigin` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 12.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'pairwise_synthesis_chunk'` and asserts that the created payload has `document_key: 'pairwise_synthesis_chunk'`.
        *   `[âœ…]` 12.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPairwiseByOrigin` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 12.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPairwiseByOrigin` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 12.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPairwiseByOrigin` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 12.b.v. These tests must fail because `planPairwiseByOrigin` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 12.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 12.c.i. Before creating `newPayload` (line 91), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPairwiseByOrigin"` instead of `"planAllToOne"`.
        *   `[âœ…]` 12.c.ii. Add `document_key` to the `newPayload` object (after line 116, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 12.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 12.b and ensure they now pass. Also verify that existing `planPairwiseByOrigin` tests still pass.
    *   `[âœ…]` 12.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 13. **`[BE]` Fix planPerModel to Extract and Validate document_key**
    *   `[âœ…]` 13.a. `[DEPS]` The `planPerModel` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 13.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, write tests that verify `planPerModel` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 13.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'synthesis'` and asserts that the created payload has `document_key: 'synthesis'`.
        *   `[âœ…]` 13.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerModel` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 13.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerModel` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 13.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerModel` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 13.b.v. These tests must fail because `planPerModel` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 13.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 13.c.i. Before creating `newPayload` (line 75), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerModel"` instead of `"planAllToOne"`.
        *   `[âœ…]` 13.c.ii. Add `document_key` to the `newPayload` object (after line 98, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 13.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 13.b and ensure they now pass. Also verify that existing `planPerModel` tests still pass.
    *   `[âœ…]` 13.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 14. **`[BE]` Fix planPerSourceDocument to Extract and Validate document_key**
    *   `[âœ…]` 14.a. `[DEPS]` The `planPerSourceDocument` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 14.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, write tests that verify `planPerSourceDocument` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 14.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'business_case_critique'` and asserts that the created payload has `document_key: 'business_case_critique'`.
        *   `[âœ…]` 14.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceDocument` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 14.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceDocument` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 14.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceDocument` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 14.b.v. These tests must fail because `planPerSourceDocument` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 14.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 14.c.i. Before creating `newPayload` (line 94), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceDocument"` instead of `"planAllToOne"`.
        *   `[âœ…]` 14.c.ii. Add `document_key` to the `newPayload` object (after line 116, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 14.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 14.b and ensure they now pass. Also verify that existing `planPerSourceDocument` tests still pass.
    *   `[âœ…]` 14.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 15. **`[BE]` Fix planPerSourceDocumentByLineage to Extract and Validate document_key**
    *   `[âœ…]` 15.a. `[DEPS]` The `planPerSourceDocumentByLineage` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 15.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, write tests that verify `planPerSourceDocumentByLineage` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 15.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'technical_requirements'` and asserts that the created payload has `document_key: 'technical_requirements'`.
        *   `[âœ…]` 15.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceDocumentByLineage` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 15.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceDocumentByLineage` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 15.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceDocumentByLineage` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 15.b.v. These tests must fail because `planPerSourceDocumentByLineage` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 15.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 15.c.i. Before creating `newPayload` (line 70), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceDocumentByLineage"` instead of `"planAllToOne"`.
        *   `[âœ…]` 15.c.ii. Add `document_key` to the `newPayload` object (after line 99, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 15.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 15.b and ensure they now pass. Also verify that existing `planPerSourceDocumentByLineage` tests still pass.
    *   `[âœ…]` 15.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 16. **`[BE]` Fix planPerSourceGroup to Extract and Validate document_key**
    *   `[âœ…]` 16.a. `[DEPS]` The `planPerSourceGroup` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 16.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, write tests that verify `planPerSourceGroup` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 16.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'synthesis'` and asserts that the created payload has `document_key: 'synthesis'`.
        *   `[âœ…]` 16.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceGroup` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 16.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceGroup` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 16.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceGroup` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 16.b.v. These tests must fail because `planPerSourceGroup` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 16.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 16.c.i. Before creating `newPayload` (line 54), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceGroup"` instead of `"planAllToOne"`.
        *   `[âœ…]` 16.c.ii. Add `document_key` to the `newPayload` object (after line 80, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 16.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 16.b and ensure they now pass. Also verify that existing `planPerSourceGroup` tests still pass.
    *   `[âœ…]` 16.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 17. **`[TEST-INT]` Fix Integration Test to Verify End-to-End document_key Flow**
    *   `[âœ…]` 17.a. `[DEPS]` After all planners are updated (steps 11-16), write an integration test that verifies the complete end-to-end flow: planners create EXECUTE job payloads with `document_key`, and `assembleTurnPrompt` can successfully process them. The test file `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file should verify this flow.
    *   `[âœ…]` 17.b. `[TEST-INT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file, write a test that verifies EXECUTE jobs created by planners include `document_key` and can be processed by `assembleTurnPrompt`.
        *   `[âœ…]` 17.b.i. Create a test case that sets up a PLAN job, calls a planner function (e.g., `planPerSourceDocument`) with a recipe step that has `outputs_required.documents[0].document_key`, and creates child EXECUTE jobs.
        *   `[âœ…]` 17.b.ii. Assert that the created EXECUTE job payloads have `document_key` set to the expected value from `outputs_required.documents[0].document_key`.
        *   `[âœ…]` 17.b.iii. Mock `assembleTurnPrompt` and verify it receives a payload with `document_key` set correctly, and that it can successfully find the document info in `headerContext.files_to_generate` using that `document_key`.
        *   `[âœ…]` 17.b.iv. This test must fail initially if any planner is missing `document_key`, but should pass after all planners are updated (steps 11-16).
    *   `[âœ…]` 17.c. `[TEST-INT]` **GREEN**: Re-run the test from step 17.b and ensure it now passes. This verifies that the complete end-to-end flow works correctly: planners create payloads with `document_key`, and `assembleTurnPrompt` can process them successfully.
    *   `[âœ…]` 17.d. `[LINT]` Run the linter for the integration test file and resolve any warnings or errors.
    *   `[âœ…]` 17.e. `[CRITERIA]` All six planners (`planAllToOne`, `planPairwiseByOrigin`, `planPerModel`, `planPerSourceDocument`, `planPerSourceDocumentByLineage`, `planPerSourceGroup`) now extract `document_key` from `recipeStep.outputs_required.documents[0].document_key` and set it in the `DialecticExecuteJobPayload`. If `document_key` is missing, undefined, null, empty string, or not a string, each planner throws an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing. All unit tests pass, integration tests verify the end-to-end flow works, and `assembleTurnPrompt` can successfully process EXECUTE jobs created by any planner. Error messages clearly identify which planner failed and why `document_key` is missing.
    *   `[âœ…]` 17.f. `[COMMIT]` Commit message: "fix: add document_key validation to all planner EXECUTE job payloads - Extract document_key from recipeStep.outputs_required.documents[0].document_key - Throw errors immediately if document_key is missing, null, empty, or invalid (fail loud and hard, no fallbacks) - Update all six planners (planAllToOne, planPairwiseByOrigin, planPerModel, planPerSourceDocument, planPerSourceDocumentByLineage, planPerSourceGroup) - Add unit tests for each planner verifying document_key extraction and error handling - Add integration test verifying end-to-end flow with assembleTurnPrompt - Resolves missing document_key error in assembleTurnPrompt"

*   `[âœ…]` 18. **`[TYPES]` Define ContentToInclude Type, Update ContextForDocument, and Define HeaderContext Interface in dialectic.interface.ts**
    *   `[âœ…]` 18.a. `[DEPS]` The `ContextForDocument` interface in `supabase/functions/dialectic-service/dialectic.interface.ts` (line 1240) currently defines `content_to_include` as `Record<string, unknown> | Record<string, unknown>[]`, which is too generic and doesn't enforce schema consistency. The `assembleTurnPrompt.ts` file (line 9) and `assembleContinuationPrompt.ts` file (line 8) both import `HeaderContext` from `dialectic.interface.ts`, but this type is NOT exported from that file, causing a missing type definition that will cause TypeScript errors. Based on complete analysis of all recipe migrations in Recipe-Planner-Gap-Analysis.md (lines 862-900), all `content_to_include` structures are objects (conceptually `Record<string, ...>`) where values can be: string (empty string "" for placeholders, or filled strings), string[] (array of strings), boolean (for flags), number (for scores, counts), nested ContentToInclude objects (recursive), or ContentToInclude[] (array of nested objects for repeated sections). **IMPORTANT**: Because the type is recursive, TypeScript requires index signature syntax `{[key: string]: ...}` rather than `Record<string, ...>` which cannot express recursion. The only exception is Antithesis `non_functional_requirements` which uses array of strings at top level - this will be fixed in migration step 20. The unified type must support all observed structures from all 5 stage migrations, and any structure from the migration that is not aligned to other structures in other migrations must be aligned to the common, unified pattern. The `HeaderContext` interface must be defined to match the actual structure of the header_context artifact generated by PLAN jobs: `system_materials: SystemMaterials`, `header_context_artifact: HeaderContextArtifact`, and `context_for_documents: ContextForDocument[]`. It must NOT have `files_to_generate` (that's in recipe step, not header context). All three type definitions must be added to the same file in a single step to ensure type consistency and avoid import errors.
    *   `[âœ…]` 18.b. `[TYPES]` In `supabase/functions/dialectic-service/dialectic.interface.ts`, define all three types/interfaces in the correct order: `ContentToInclude` type (before `ContextForDocument` interface, around line 1230), update `ContextForDocument` interface (line 1239-1242), and define `HeaderContext` interface (after `ContextForDocument`, around line 1242).
        *   `[âœ…]` 18.b.i. Add the `ContentToInclude` type definition before the `ContextForDocument` interface definition (around line 1230, before line 1239 where `ContextForDocument` is defined). The type definition must include the complete JSDoc comment explaining all observed structures and examples from recipes (as specified in Recipe-Planner-Gap-Analysis.md Fix 2, lines 1110-1128). **IMPORTANT**: TypeScript does not permit recursive types with `Record<string, ...>` syntax. Use index signature syntax instead. The type must be exported with this exact multi-line format: `export type ContentToInclude = {` on the first line, `[key: string]:` on the second line, then each union member (`| string`, `| string[]`, `| boolean`, `| number`, `| ContentToInclude`, `| ContentToInclude[]`) on separate lines with comments, then closing with `};` on the final line. This index signature syntax is the only way TypeScript supports recursive type definitions. Do NOT use `Record<string, ...>` as it cannot express recursion.
        *   `[âœ…]` 18.b.ii. Update the `ContextForDocument` interface (lines 1239-1242) to use `ContentToInclude` instead of the generic type. Change line 1240 from `content_to_include: Record<string, unknown> | Record<string, unknown>[];` to `content_to_include: ContentToInclude;`. Ensure `ContentToInclude` is accessible (it should be in the same file after step 18.b.i).
        *   `[âœ…]` 18.b.iii. Verify that `SystemMaterials` and `HeaderContextArtifact` types/interfaces are already defined in the file. If they are missing, this step must halt and report the missing types as a blocking issue. If they exist, proceed to define `HeaderContext`.
        *   `[âœ…]` 18.b.iv. Add the `HeaderContext` interface definition exactly as specified in Recipe-Planner-Gap-Analysis.md Fix 2 (lines 1146-1157) after the `ContextForDocument` interface definition (after `ContextForDocument`). The interface definition must include the complete JSDoc comment explaining that `files_to_generate` is NOT in header_context, that it's defined in the EXECUTE recipe step's outputs_required, and that `context_for_documents` is filled by PLAN job agent with alignment details. Note that `review_metadata` is stage-specific (only Antithesis) and is not part of the base `HeaderContext` interface - it should be handled separately in stage-specific logic if needed. Ensure the interface is exported: `export interface HeaderContext { system_materials: SystemMaterials; header_context_artifact: HeaderContextArtifact; context_for_documents: ContextForDocument[]; }`
        *   `[âœ…]` 18.b.v. Add `context_for_documents?: ContextForDocument[]` as an optional field to the `DialecticPlanJobPayload` interface (around line 682-684) so planners can pass it in PLAN job payloads. This ensures `assemblePlannerPrompt` can access `context_for_documents` from the job payload. The interface currently only extends `DialecticBaseJobPayload` and has `job_type: JobType`. Update it to: `export interface DialecticPlanJobPayload extends DialecticBaseJobPayload { job_type: JobType; context_for_documents?: ContextForDocument[]; }`
    *   `[âœ…]` 18.c. `[LINT]` Run the linter for `supabase/functions/dialectic-service/dialectic.interface.ts` and resolve any warnings or errors. Verify that all three types are properly exported and can be imported by other files.

*   `[âœ…]` 19. **`[DB]` Fix Thesis Stage Migration - Verify and Fix Structure Matching Between PLAN and EXECUTE Steps**
    *   `[âœ…]` 19.a. `[DEPS]` The Thesis stage migration file `supabase/migrations/20251006194531_thesis_stage.sql` has PLAN steps with `context_for_documents` and EXECUTE steps with `files_to_generate`. According to Recipe-Planner-Gap-Analysis.md Gap 9 (lines 793-850) and Gap 10 (lines 852-963), ALL recipe migrations must ensure: (1) `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the EXECUTE steps, (2) `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. The Thesis stage has 4 EXECUTE steps: `thesis_generate_business_case`, `thesis_generate_feature_spec`, `thesis_generate_technical_approach`, and `thesis_generate_success_metrics`. Each EXECUTE step must have its `files_to_generate[].from_document_key` match the corresponding PLAN step's `context_for_documents[].document_key`, and the `content_to_include` structures must match exactly. The migration file must be verified and any mismatches must be fixed to ensure proper PLAN â†” EXECUTE structure mapping.
    *   `[âœ…]` 19.b. `[DB]` In `supabase/migrations/20251006194531_thesis_stage.sql`, verify and fix structure matching between PLAN and EXECUTE steps.
        *   `[âœ…]` 19.b.i. For the PLAN step (lines 171-251), identify all `context_for_documents` entries and their `document_key` values: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`. Document the `content_to_include` structure for each entry.
        *   `[âœ…]` 19.b.ii. For each EXECUTE step (`thesis_generate_business_case` around line 323, `thesis_generate_feature_spec` around line 485, `thesis_generate_technical_approach` around line 643, `thesis_generate_success_metrics` around line 800), verify that `files_to_generate[].from_document_key` matches the PLAN step's `context_for_documents[].document_key` for the same document. For example, `thesis_generate_business_case` EXECUTE step should have `files_to_generate` with `from_document_key: "business_case"` matching the PLAN step's `context_for_documents` entry with `document_key: "business_case"`.
        *   `[âœ…]` 19.b.iii. For each EXECUTE step, verify that `documents[].content_to_include` structure exactly matches the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. For example, if PLAN step has `{"field1": "", "field2": []}`, the EXECUTE step must have the same structure `{"field1": "", "field2": []}` (values can differ, but structure must match).
        *   `[âœ…]` 19.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after changes.
        *   `[âœ…]` 19.b.v. **Fix 7: Standardize PLAN step `content_to_include` structures**: For the PLAN step's `context_for_documents` entry for `feature_spec` (line 219), change `content_to_include` from an array `[{...}]` to an object structure that conforms to the `ContentToInclude` type. The structure should be: `{"features": [{"feature_name": "", "user_stories": []}]}` (wrapping the array in an object with a `features` key). This ensures all `content_to_include` structures are objects (not arrays at top level) as required by the `ContentToInclude` type.
        *   `[âœ…]` 19.b.vi. **Fix 7: Update corresponding EXECUTE step**: For the EXECUTE step `thesis_generate_feature_spec` (around line 485), update the `documents[].content_to_include` structure to match the standardized PLAN step structure: `{"features": [{"feature_name": "", "user_stories": []}]}`. Ensure the structure exactly matches the PLAN step.
        *   `[âœ…]` 19.b.vii. **Fix 8: Standardize all EXECUTE step `content_to_include` structures**: For all 4 EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) Structures must exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
        *   `[âœ…]` 19.b.viii. Verify that all 4 EXECUTE steps have been checked and fixed. Document any changes made in comments if needed.
    *   `[âœ…]` 19.c. `[LINT]` Run the linter for `supabase/migrations/20251006194531_thesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 20. **`[DB]` Fix Antithesis Stage Migration - Fix non_functional_requirements Structure and Verify Structure Matching**
    *   `[âœ…]` 20.a. `[DEPS]` The Antithesis stage migration file `supabase/migrations/20251006194542_antithesis_stage.sql` has a PLAN step with `context_for_documents` entry for `non_functional_requirements` that uses an array of strings at the top level (e.g., `["security", "performance", ...]`) instead of an object structure. According to Recipe-Planner-Gap-Analysis.md Gap 7 (lines 736-763) and Gap 10 (lines 852-963), all `content_to_include` structures must be objects (Record<string, ...>), not arrays at the top level. This must be changed to `{"categories": ["security", "performance", ...]}` to match the unified object pattern defined by the `ContentToInclude` type. The corresponding EXECUTE step's `documents[].content_to_include` must also be updated to match this structure. Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps: `context_for_documents[].document_key` must match `files_to_generate[].from_document_key`, and `content_to_include` structures must match exactly.
    *   `[âœ…]` 20.b. `[DB]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, fix the `non_functional_requirements` structure and verify structure matching.
        *   `[âœ…]` 20.b.i. Find the PLAN step's `context_for_documents` entry for `non_functional_requirements` (should be in the PLAN step around lines 448-577). Change the `content_to_include` from an array of strings (e.g., `["security", "performance", ...]`) to an object with a `categories` property: `{"categories": ["security", "performance", ...]}`. Preserve the actual category values, only change the structure from array to object. Ensure JSON structure remains valid.
        *   `[âœ…]` 20.b.ii. Find the EXECUTE step for `non_functional_requirements` (should be an EXECUTE step in the same file). Update the `documents[].content_to_include` to match the PLAN step structure: `{"categories": ["security", "performance", ...]}`. Ensure the structure exactly matches the PLAN step (same keys, same array structure within the object).
        *   `[âœ…]` 20.b.iii. For all other documents in the Antithesis stage (business_case_critique, technical_feasibility_assessment, risk_register, dependency_map, comparison_vector), verify that PLAN step's `context_for_documents[].document_key` matches EXECUTE step's `files_to_generate[].from_document_key` for each document. Verify that `content_to_include` structures match exactly between PLAN and EXECUTE steps for each `document_key`.
        *   `[âœ…]` 20.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after all changes.
        *   `[âœ…]` 20.b.v. **Fix 7: Standardize all PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level - `non_functional_requirements` is already fixed in 20.b.i), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 20.b.vi. **Fix 8: Standardize all EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 20.c. `[LINT]` Run the linter for `supabase/migrations/20251006194542_antithesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 21. **`[DB]` Fix Synthesis Stage Migration - Remove files_to_generate from PLAN Steps, Add to EXECUTE Steps, and Verify Structure Matching**
    *   `[âœ…]` 21.a. `[DEPS]` The Synthesis stage migration file `supabase/migrations/20251006194549_synthesis_stage.sql` has multiple issues identified in Recipe-Planner-Gap-Analysis.md: (1) Synthesis Pairwise PLAN step (lines 247-368, specifically lines 350-367) incorrectly includes `files_to_generate`, (2) Synthesis Final Header PLAN step (lines 779-938, specifically lines 924-937) incorrectly includes `files_to_generate`, (3) Synthesis Pairwise EXECUTE step (lines 390-420) is missing `files_to_generate` - JSON ends at line 419 with just `]` closing the documents array, (4) Synthesis Final Deliverables EXECUTE steps are missing `files_to_generate`: `product_requirements` (around line 1018), `system_architecture` (around line 1070), and `tech_stack` (around line 1124). According to the conceptual model (Gap 3, Gap 6), PLAN steps should only have `context_for_documents` (which defines the document models with empty `content_to_include` objects for the agent to fill), while `files_to_generate` belongs only in EXECUTE steps (which define execution instructions). Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps.
    *   `[âœ…]` 21.b. `[DB]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, remove `files_to_generate` from PLAN steps, add to EXECUTE steps, and verify structure matching.
        *   `[âœ…]` 21.b.i. For Synthesis Pairwise PLAN step (lines 247-368): Remove the `files_to_generate` array (lines 350-367) that contains entries for `synthesis_pairwise_business_case`, `synthesis_pairwise_feature_spec`, `synthesis_pairwise_technical_approach`, and `synthesis_pairwise_success_metrics`. Remove the comma before `files_to_generate` if present. Ensure the JSON structure remains valid after removal. Verify that `context_for_documents` array remains intact with proper structure.
        *   `[âœ…]` 21.b.ii. For Synthesis Final Header PLAN step (lines 779-938): Remove the `files_to_generate` array (lines 924-937) that contains entries for `product_requirements`, `system_architecture`, and `tech_stack`. Remove the comma before `files_to_generate` if present. Ensure the JSON structure remains valid after removal. Verify that `context_for_documents` array remains intact with proper structure.
        *   `[âœ…]` 21.b.iii. For Synthesis Pairwise EXECUTE step (lines 390-420): After the `documents` array closes (line 419), add a comma and then add `files_to_generate` array with entries matching the document keys in the `documents` array. Each entry must have `from_document_key` (matching the `document_key` from the documents array) and `template_filename` (matching the `template_filename` from the documents array). The structure should be: `"files_to_generate": [{"template_filename": "synthesis_pairwise_business_case.json", "from_document_key": "synthesis_pairwise_business_case"}, {"template_filename": "synthesis_pairwise_feature_spec.json", "from_document_key": "synthesis_pairwise_feature_spec"}, {"template_filename": "synthesis_pairwise_technical_approach.json", "from_document_key": "synthesis_pairwise_technical_approach"}, {"template_filename": "synthesis_pairwise_success_metrics.json", "from_document_key": "synthesis_pairwise_success_metrics"}]`. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.iv. For Synthesis Final Deliverables EXECUTE step `product_requirements` (around line 1018): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_product_requirements_document.md", "from_document_key": "product_requirements"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.v. For Synthesis Final Deliverables EXECUTE step `system_architecture` (around line 1070): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_system_architecture.md", "from_document_key": "system_architecture"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.vi. For Synthesis Final Deliverables EXECUTE step `tech_stack` (around line 1124): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_tech_stack.md", "from_document_key": "tech_stack"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.vii. For all Synthesis PLAN and EXECUTE steps, verify that `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the corresponding EXECUTE steps. For example, Synthesis Pairwise PLAN step has `context_for_documents` with `document_key: "synthesis_pairwise_business_case"`, and the Synthesis Pairwise EXECUTE step must have `files_to_generate` with `from_document_key: "synthesis_pairwise_business_case"`. Verify this mapping for all documents: synthesis_pairwise_business_case, synthesis_pairwise_feature_spec, synthesis_pairwise_technical_approach, synthesis_pairwise_success_metrics, product_requirements, system_architecture, tech_stack.
        *   `[âœ…]` 21.b.viii. For all Synthesis PLAN and EXECUTE steps, verify that `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. Fix any mismatches by updating the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure.
        *   `[âœ…]` 21.b.ix. **Fix 7: Standardize all Synthesis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries (Pairwise and Final Header), ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 21.b.x. **Fix 8: Standardize all Synthesis EXECUTE step `content_to_include` structures**: For all EXECUTE steps (Pairwise, Document-level, and Final Deliverables), ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 21.c. `[LINT]` Run the linter for `supabase/migrations/20251006194549_synthesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid for all steps.

*   `[âœ…]` 22. **`[DB]` Fix Parenthesis Stage Migration - Verify and Fix Structure Matching Between PLAN and EXECUTE Steps**
    *   `[âœ…]` 22.a. `[DEPS]` The Parenthesis stage migration file `supabase/migrations/20251006194558_parenthesis_stage.sql` has PLAN steps with `context_for_documents` and EXECUTE steps with `files_to_generate`. According to Recipe-Planner-Gap-Analysis.md Gap 9 (lines 793-850) and Gap 10 (lines 852-963), ALL recipe migrations must ensure: (1) `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the EXECUTE steps, (2) `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. The Parenthesis stage has 3 EXECUTE steps: `parenthesis_generate_technical_requirements`, `parenthesis_generate_master_plan`, and `parenthesis_generate_milestone_schema`. Each EXECUTE step must have its `files_to_generate[].from_document_key` match the corresponding PLAN step's `context_for_documents[].document_key`, and the `content_to_include` structures must match exactly.
    *   `[âœ…]` 22.b. `[DB]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, verify and fix structure matching between PLAN and EXECUTE steps.
        *   `[âœ…]` 22.b.i. For the PLAN step (lines 271-393), identify all `context_for_documents` entries and their `document_key` values: `technical_requirements`, `master_plan`, `milestone_schema`. Document the `content_to_include` structure for each entry.
        *   `[âœ…]` 22.b.ii. For each EXECUTE step (`parenthesis_generate_technical_requirements` around line 647, `parenthesis_generate_master_plan` around line 800, `parenthesis_generate_milestone_schema` around line 950), verify that `files_to_generate[].from_document_key` matches the PLAN step's `context_for_documents[].document_key` for the same document. For example, `parenthesis_generate_technical_requirements` EXECUTE step should have `files_to_generate` with `from_document_key: "technical_requirements"` matching the PLAN step's `context_for_documents` entry with `document_key: "technical_requirements"`.
        *   `[âœ…]` 22.b.iii. For each EXECUTE step, verify that `documents[].content_to_include` structure exactly matches the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. For example, if PLAN step has `{"array_field": [], "string_field": "", "nested_object": {}}`, the EXECUTE step must have the same structure (values can differ, but structure must match).
        *   `[âœ…]` 22.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after changes.
        *   `[âœ…]` 22.b.v. **Fix 7: Standardize all Parenthesis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 22.b.vi. **Fix 8: Standardize all Parenthesis EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
        *   `[âœ…]` 22.b.vii. Verify that all 3 EXECUTE steps have been checked and fixed. Document any changes made in comments if needed.
    *   `[âœ…]` 22.c. `[LINT]` Run the linter for `supabase/migrations/20251006194558_parenthesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 23. **`[DB]` Fix Paralysis Stage Migration - Add content_to_include to EXECUTE Step and Verify Structure Matching**
    *   `[âœ…]` 23.a. `[DEPS]` The Paralysis stage migration file `supabase/migrations/20251006194605_paralysis_stage.sql` has an EXECUTE step for `actionable_checklist` (lines 458-486) where the `documents` array entry is missing the `content_to_include` field. According to Recipe-Planner-Gap-Analysis.md Gap 8 (lines 764-791), EXECUTE step `documents[].content_to_include` must exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key` to enable proper mapping. The PLAN step (lines 293-315) has `context_for_documents` with `actionable_checklist` entry that includes `content_to_include: {"action_items": ["..."]}`. The EXECUTE step must have the same structure. Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps for all documents: `context_for_documents[].document_key` must match `files_to_generate[].from_document_key`, and `content_to_include` structures must match exactly.
    *   `[âœ…]` 23.b. `[DB]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, add `content_to_include` to the EXECUTE step and verify structure matching.
        *   `[âœ…]` 23.b.i. Find the EXECUTE step for `actionable_checklist` (lines 458-486). Locate the `documents` array entry (should be around lines 461-464 based on the analysis).
        *   `[âœ…]` 23.b.ii. Add `content_to_include` field to the document entry. The structure must exactly match the PLAN step's `context_for_documents` entry for `actionable_checklist`. Based on the PLAN step (lines 293-315), it should be: `"content_to_include": {"action_items": ["..."]}`. Use the same structure as the PLAN step (empty array or placeholder values are acceptable for the migration - the actual values will be filled by the agent during execution). Ensure proper comma placement and JSON structure validity.
        *   `[âœ…]` 23.b.iii. For all documents in the Paralysis stage (actionable_checklist, updated_master_plan, advisor_recommendations), verify that PLAN step's `context_for_documents[].document_key` matches EXECUTE step's `files_to_generate[].from_document_key` for each document. Verify that `content_to_include` structures match exactly between PLAN and EXECUTE steps for each `document_key`.
        *   `[âœ…]` 23.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after all changes.
        *   `[âœ…]` 23.b.v. **Fix 7: Standardize all Paralysis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 23.b.vi. **Fix 8: Standardize all Paralysis EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 23.c. `[LINT]` Run the linter for `supabase/migrations/20251006194605_paralysis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 24. **`[BE]` Fix isHeaderContext Type Guard and Tests in type_guards.dialectic.ts, and Define isContentToInclude Helper Function**
    *   `[âœ…]` 24.a. `[DEPS]` The type guard `isHeaderContext` in `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts` (lines 182-205) currently validates `files_to_generate` as optional (lines 197-202), but according to the new `HeaderContext` interface defined in step 18, `files_to_generate` should NOT be in `HeaderContext` at all (it's in recipe step, not header context). The type guard must be updated to match the new interface structure. The function signature uses `ReturnType<typeof JSON.parse>` which is not type-safe. The type guard should import `HeaderContext` from `../../dialectic-service/dialectic.interface.ts` and use it in the return type annotation. Additionally, steps 25, 27-32 require a helper function `isContentToInclude(value: unknown): value is ContentToInclude` to validate that `content_to_include` structures conform to the `ContentToInclude` type. This helper must be defined in the same file (`type_guards.dialectic.ts`) and exported so it can be imported by `assembleTurnPrompt` and all planner functions. The test file `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts` must be updated to test both the new `isHeaderContext` structure and the new `isContentToInclude` helper function. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 24.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts`, write tests for `isHeaderContext` that verify it matches the new `HeaderContext` interface structure.
        *   `[âœ…]` 24.b.i. Create a test case that asserts `isHeaderContext` returns `true` for a valid `HeaderContext` object with: `system_materials` (valid `SystemMaterials`), `header_context_artifact` (valid `HeaderContextArtifact`), and `context_for_documents` (array of valid `ContextForDocument` objects with `ContentToInclude` structures). This test must pass after the type guard is updated.
        *   `[âœ…]` 24.b.ii. Create a test case that asserts `isHeaderContext` returns `false` for an object that has `files_to_generate` property (since `HeaderContext` interface does not include `files_to_generate`). This test must fail initially because the current type guard allows `files_to_generate` as optional, but should pass after the type guard is updated to reject it.
        *   `[âœ…]` 24.b.iii. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `system_materials`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.iv. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `header_context_artifact`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.v. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `context_for_documents`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.vi. Create a test case that asserts `isHeaderContext` returns `false` for an object where `context_for_documents` is not an array. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.vii. Create a test case that asserts `isHeaderContext` returns `false` for an object where `context_for_documents` contains invalid entries (missing `document_key` or invalid `content_to_include` structure). This test should pass with the current type guard if it validates structure.
        *   `[âœ…]` 24.b.viii. `[TEST-UNIT]` **RED**: Write tests for `isContentToInclude` helper function that will be defined in step 24.c.vi. Create test cases that verify: (1) `isContentToInclude` returns `true` for valid `ContentToInclude` objects (simple objects with string values like `{"field1": "", "field2": ""}`, objects with string arrays like `{"field1": [], "field2": ["value1", "value2"]}`, objects with nested objects like `{"dimensions": {"feasibility": {"score": 0, "rationale": ""}}}`, objects with arrays of objects like `{"features": [{"name": "", "stories": []}]}`, mixed structures combining all types), (2) `isContentToInclude` returns `false` for arrays at top level (must be objects, not arrays - e.g., `["string1", "string2"]` should fail), (3) `isContentToInclude` returns `false` for invalid value types (functions, null, undefined as top-level value in object), (4) `isContentToInclude` correctly validates nested structures recursively (deeply nested objects and arrays of objects). These tests must fail initially because the function doesn't exist yet.
    *   `[âœ…]` 24.c. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts`, update the `isHeaderContext` function to match the new `HeaderContext` interface and define the `isContentToInclude` helper function.
        *   `[âœ…]` 24.c.i. Import `HeaderContext` from `../../../dialectic-service/dialectic.interface.ts` at the top of the file (the file is at `supabase/functions/_shared/utils/type-guards/`, so it needs three levels up to reach `dialectic-service`).
        *   `[âœ…]` 24.c.ii. Update the function signature (line 182) from `export function isHeaderContext(value: unknown): value is ReturnType<typeof JSON.parse>` to `export function isHeaderContext(value: unknown): value is HeaderContext`.
        *   `[âœ…]` 24.c.iii. Remove the `files_to_generate` validation block (lines 197-202) entirely, since `HeaderContext` interface does not include this property.
        *   `[âœ…]` 24.c.iv. Ensure the validation for `system_materials`, `header_context_artifact`, and `context_for_documents` remains intact (lines 185-195). These validations should continue to work correctly.
        *   `[âœ…]` 24.c.v. Verify the function still returns `true` only when all required fields are present and valid.
        *   `[âœ…]` 24.c.vi. Import `ContentToInclude` from `../../../dialectic-service/dialectic.interface.ts` at the top of the file (same import path as `HeaderContext`).
        *   `[âœ…]` 24.c.vii. Define and export the `isContentToInclude` helper function: `export function isContentToInclude(value: unknown): value is ContentToInclude`. The function must: (1) Check that value is an object using `isRecord(value)` helper (import from `../type_guards.ts` if needed), (2) Check that value is NOT an array at top level using `!Array.isArray(value)` - if it's an array, return `false`, (3) Recursively validate that all values in the object are allowed types according to the `ContentToInclude` type definition: `string`, `string[]`, `boolean`, `number`, or recursively `ContentToInclude`/`ContentToInclude[]`. For each key-value pair in the object: (a) If value is `string`, `boolean`, or `number`, it's valid, (b) If value is an array, check if it's `string[]` (all elements are strings) or `ContentToInclude[]` (all elements pass `isContentToInclude` recursively), (c) If value is an object, recursively call `isContentToInclude(value)` to validate nested structures, (d) If value is any other type (function, null, undefined, etc.), return `false`. The function must handle all cases from the `ContentToInclude` type definition: `Record<string, string | string[] | boolean | number | ContentToInclude | ContentToInclude[]>`. Export the function so it can be imported by `assembleTurnPrompt` (step 25) and all planner functions (steps 27-32).
    *   `[âœ…]` 24.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 24.b (including 24.b.viii for `isContentToInclude`) and ensure they now pass. The test from 24.b.ii should now pass because the type guard rejects `files_to_generate`. Also verify that existing `isHeaderContext` tests still pass. Verify that all `isContentToInclude` tests from 24.b.viii pass.
    *   `[âœ…]` 24.e. `[LINT]` Run the linter for `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts` and `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 25. **`[BE]` Fix assembleTurnPrompt to Read files_to_generate from Recipe Step and Use header_context for Alignment, and Tests**
    *   `[âœ…]` 25.a. `[DEPS]` The `assembleTurnPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` currently has incorrect logic (lines 152-159) that looks for `files_to_generate` in `headerContext` and uses `document_key` instead of `from_document_key`. According to Recipe-Planner-Gap-Analysis.md Fix 1 (lines 1026-1082) and Gap 1 (lines 615-632) and Gap 2 (lines 634-651), the function must: (1) read `files_to_generate` from `stage.recipe_step.outputs_required.files_to_generate` (execution instructions), (2) use `header_context.context_for_documents` for cross-document alignment details (completed by PLAN job), (3) match `files_to_generate[].from_document_key` to `header_context.context_for_documents[].document_key`, (4) use the matched entry's `content_to_include` for document generation, (5) validate that `content_to_include` is filled (not empty). The function also imports `HeaderContext` from `dialectic.interface.ts` (line 9), which should now work correctly after step 18 defines the interface. The test file `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts` must be updated to test the new behavior. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 25.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, write tests that verify `assembleTurnPrompt` reads `files_to_generate` from recipe step and uses `header_context.context_for_documents` for alignment.
        *   `[âœ…]` 25.b.i. Create a test case that mocks `stage.recipe_step.outputs_required.files_to_generate` with an array containing `{from_document_key: "business_case", template_filename: "thesis_business_case.md"}`, mocks `headerContext.context_for_documents` with an array containing `{document_key: "business_case", content_to_include: {"field1": "value1"}}`, and asserts that `assembleTurnPrompt` successfully finds the document info using `from_document_key` matching `document_key`, and uses the `content_to_include` from `context_for_documents` for alignment. This test must fail because the function currently looks for `files_to_generate` in `headerContext` and uses `document_key` instead of `from_document_key`.
        *   `[âœ…]` 25.b.ii. Create a test case that mocks `stage.recipe_step.outputs_required` missing `files_to_generate`, and asserts that `assembleTurnPrompt` throws an error with message indicating `files_to_generate` is missing from recipe step. This test must fail because the function currently doesn't check for `files_to_generate` in recipe step.
        *   `[âœ…]` 25.b.iii. Create a test case that mocks `headerContext` missing `context_for_documents`, and asserts that `assembleTurnPrompt` throws an error indicating `context_for_documents` is missing from header context. This test must fail because the function currently doesn't validate `context_for_documents` exists.
        *   `[âœ…]` 25.b.iv. Create a test case that mocks `files_to_generate` with `from_document_key: "business_case"` but `headerContext.context_for_documents` has no entry with `document_key: "business_case"`, and asserts that `assembleTurnPrompt` throws an error indicating no matching `context_for_documents` entry found. This test must fail because the function currently doesn't perform this matching.
        *   `[âœ…]` 25.b.v. Create a test case that mocks `context_for_documents` entry with empty `content_to_include` object (`{}`), and asserts that `assembleTurnPrompt` throws an error indicating `content_to_include` is not filled in. This test must fail because the function currently doesn't validate that `content_to_include` is filled.
        *   `[âœ…]` 25.b.vi. Create a test case that verifies `assembleTurnPrompt` uses `docInfo.template_filename` from the matched `files_to_generate` entry (not from `headerContext`). This test must fail because the function currently gets template filename from the wrong location.
        *   `[âœ…]` 25.b.vii. Create a test case that mocks `files_to_generate` with `from_document_key: "business_case"` but the matched `context_for_documents` entry has a `content_to_include` structure that doesn't match the `ContentToInclude` type schema (e.g., has invalid nested structure), and asserts that `assembleTurnPrompt` throws an error indicating structure mismatch. This test must fail because the function currently doesn't validate structure matching.
        *   `[âœ…]` 25.b.viii. Create a test case that verifies `contextForDoc.content_to_include` is merged into the `renderContext` object passed to `renderPrompt`, ensuring alignment details are available in the template context. This test must fail because the function currently doesn't merge alignment details into render context.
    *   `[âœ…]` 25.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts`, rewrite the logic to read `files_to_generate` from recipe step and use `header_context.context_for_documents` for alignment.
        *   `[âœ…]` 25.c.i. Implement the logic described in Recipe-Planner-Gap-Analysis.md Fix 1 (lines 1050-1082) as a specification: Get `files_to_generate` from `stage.recipe_step.outputs_required?.files_to_generate`, validate it exists and is an array, find the document info using `from_document_key` matching `documentKey`, get alignment details from `headerContext.context_for_documents` by matching `document_key`, validate that `content_to_include` is filled (not empty), and use `docInfo.template_filename` for template file and `contextForDoc.content_to_include` for alignment details in prompt. Do not copy-paste pseudocode - implement the actual TypeScript logic following the specification.
        *   `[âœ…]` 25.c.ii. **Fix 9: Add runtime validation that verifies PLAN â†” EXECUTE structure mapping**: (1) Validate that `files_to_generate[].from_document_key` matches `headerContext.context_for_documents[].document_key` for the matched entry. (2) **Validate `content_to_include` structure conforms to `ContentToInclude` type**: Import the `isContentToInclude` helper function from `../../utils/type-guards/type_guards.dialectic.ts` (defined in step 24.c.vii) and use it to validate that `contextForDoc.content_to_include` conforms to the `ContentToInclude` type. (3) **Validate structure matching**: Compare the structure (not values) of `contextForDoc.content_to_include` with the expected structure from the recipe step's `documents[].content_to_include` for the same `document_key` - they must have the same keys, same nested structure, same array positions. Use the `isHeaderContext` type guard (from Step 24) to validate the header context structure at runtime. If structure mismatch is detected, throw an error indicating the mapping violation: `assembleTurnPrompt requires content_to_include structure for document_key '${documentKey}' to match the recipe step's expected structure`. **Note**: Per Fix 9 (line 1394), structure matching validation is optional and may be better performed here where `header_context` is already loaded, rather than in planners.
        *   `[âœ…]` 25.c.iii. Update any subsequent code that uses `docInfo` to ensure it uses `docInfo.template_filename` (which now comes from recipe step, not header context).
        *   `[âœ…]` 25.c.iv. Merge `contextForDoc.content_to_include` into the prompt context by adding it to the `renderContext` object that is passed to `renderPrompt`. The `content_to_include` object should be merged into `renderContext` so that template variables can access the alignment details. Ensure the merge happens after `system_materials` and `user_domain_overlay_values` but before `document_specific_data`, so alignment details can be overridden by document-specific data if needed.
        *   `[âœ…]` 25.c.v. Verify the `HeaderContext` import now resolves correctly after step 18 defines the interface.
    *   `[âœ…]` 25.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 25.b and ensure they now pass. Also verify that existing `assembleTurnPrompt` tests still pass (update any tests that mock `headerContext.files_to_generate` to instead mock `stage.recipe_step.outputs_required.files_to_generate`).
    *   `[âœ…]` 25.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` and `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 26. **`[BE]` Fix assemblePlannerPrompt to Include context_for_documents in PLAN Prompts, and Tests**
    *   `[âœ…]` 26.a. `[DEPS]` The `assemblePlannerPrompt` function in `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts` must include `context_for_documents` in PLAN prompts so the agent knows what alignment details to produce. According to Recipe-Planner-Gap-Analysis.md Fix 6 (lines 1261-1271), the function must: (1) extract `context_for_documents` from `recipe_step.outputs_required`, (2) include the empty `content_to_include` object models in the prompt, (3) instruct the agent to fill in these models with specific alignment values, (4) structure the prompt to ensure the agent produces `header_context` with completed `content_to_include` objects. The test file `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts` must be updated to test the new behavior. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 26.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts`, write tests that verify `assemblePlannerPrompt` includes `context_for_documents` in PLAN prompts.
        *   `[âœ…]` 26.b.i. Create a test case that mocks a PLAN job with `recipe_step.outputs_required.context_for_documents` containing entries with empty `content_to_include` object models, and asserts that `assemblePlannerPrompt` includes these `context_for_documents` in the generated prompt. This test must fail if the function doesn't currently include `context_for_documents`.
        *   `[âœ…]` 26.b.ii. Create a test case that asserts the prompt includes instructions telling the agent to fill in the `content_to_include` objects with specific alignment values. This test must fail if the function doesn't currently include such instructions.
        *   `[âœ…]` 26.b.iii. Create a test case that mocks `recipe_step.outputs_required` missing `context_for_documents`, and asserts that `assemblePlannerPrompt` throws an error indicating `context_for_documents` is required for PLAN jobs. This test must fail if the function doesn't currently validate this.
    *   `[âœ…]` 26.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts`, update the function to include `context_for_documents` in PLAN prompts.
        *   `[âœ…]` 26.c.i. Read the current implementation to understand how it assembles PLAN prompts.
        *   `[âœ…]` 26.c.ii. Extract `context_for_documents` from `recipe_step.outputs_required.context_for_documents` (validate it exists and is an array).
        *   `[âœ…]` 26.c.iii. Include the `context_for_documents` structure (with empty `content_to_include` object models) in the prompt that is sent to the agent.
        *   `[âœ…]` 26.c.iv. Add instructions in the prompt telling the agent to: (1) fill in the `content_to_include` objects with specific alignment values (shared terminology, consistent values, coordinated decisions), (2) produce a `header_context` artifact with completed `content_to_include` objects, (3) ensure all documents in the step group will use these alignment details.
        *   `[âœ…]` 26.c.v. Ensure the prompt structure clearly separates the empty models (what the agent should fill) from other prompt content.
    *   `[âœ…]` 26.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 26.b and ensure they now pass. Also verify that existing `assemblePlannerPrompt` tests still pass.
    *   `[âœ…]` 26.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts` and `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 27. **`[BE]` Fix planAllToOne to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 27.a. `[DEPS]` The `planAllToOne` planner function in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` currently does not validate or pass `context_for_documents` for PLAN jobs, and does not validate `files_to_generate` for EXECUTE jobs. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** According to Recipe-Planner-Gap-Analysis.md Fix 3 (lines 1179-1210) and Fix 4 (lines 1213-1244), when `job_type === 'PLAN'`, the planner MUST ALWAYS: (1) validate that `recipe_step.outputs_required.context_for_documents` exists and is an array with entries (this is REQUIRED, not optional), (2) validate that each entry has a `document_key` and an empty `content_to_include` object model (not an array at top level), (3) pass the entire `context_for_documents` structure into the job payload so the prompt assembler can include it. When `job_type === 'EXECUTE'`, the planner MUST ALWAYS: (1) validate that `recipe_step.outputs_required.files_to_generate` exists and is an array with entries (this is REQUIRED, not optional), (2) validate that `recipe_step.outputs_required.documents` exists and is an array with entries (this is REQUIRED, not optional), (3) validate structure (each `files_to_generate` entry has `from_document_key` and `template_filename`). The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 44-74) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with the planner function name (e.g., 'planAllToOne requires...') to aid debugging. The test file `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 27.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, write tests that verify `planAllToOne` validates and passes `context_for_documents` for PLAN jobs and validates `files_to_generate` for EXECUTE jobs.
        *   `[âœ…]` 27.b.i. Create a test case that mocks a PLAN job with `recipe_step.outputs_required.context_for_documents` containing valid entries, and asserts that `planAllToOne` successfully creates a job payload that includes `context_for_documents`. This test must fail because the function currently doesn't handle `context_for_documents`.
        *   `[âœ…]` 27.b.ii. Create a test case that mocks a PLAN job with `recipe_step.outputs_required` missing `context_for_documents`, and asserts that `planAllToOne` throws an error indicating `context_for_documents` is required for PLAN jobs. This test must fail because the function currently doesn't validate this.
        *   `[âœ…]` 27.b.iii. Create a test case that mocks a PLAN job with `context_for_documents` entry missing `document_key`, and asserts that `planAllToOne` throws an error indicating `document_key` is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.iv. Create a test case that mocks a PLAN job with `context_for_documents` entry missing `content_to_include`, and asserts that `planAllToOne` throws an error indicating `content_to_include` object model is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.v. Create a test case that mocks an EXECUTE job with `recipe_step.outputs_required.files_to_generate` containing valid entries, and asserts that `planAllToOne` successfully creates a job payload. This test should pass if the function doesn't currently validate (it just doesn't check), but will need to pass after validation is added.
        *   `[âœ…]` 27.b.vi. Create a test case that mocks an EXECUTE job with `recipe_step.outputs_required` missing `files_to_generate`, and asserts that `planAllToOne` throws an error indicating `files_to_generate` is required for EXECUTE jobs. This test must fail because the function currently doesn't validate this.
        *   `[âœ…]` 27.b.vii. Create a test case that mocks an EXECUTE job with `files_to_generate` entry missing `from_document_key`, and asserts that `planAllToOne` throws an error indicating `from_document_key` is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.viii. Create a test case that mocks an EXECUTE job with `files_to_generate` entry missing `template_filename`, and asserts that `planAllToOne` throws an error indicating `template_filename` is missing. This test must fail because the function currently doesn't validate structure.
    *   `[âœ…]` 27.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, add validation and passing of `context_for_documents` for PLAN jobs and validation of `files_to_generate` for EXECUTE jobs.
        *   `[âœ…]` 27.c.i. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file.
        *   `[âœ…]` 27.c.ii. Before creating `newPayload` (line 44), validate that `recipeStep.job_type` is either `'PLAN'` or `'EXECUTE'`. If it's neither, throw an error: `planAllToOne requires job_type to be 'PLAN' or 'EXECUTE', received: ${recipeStep.job_type}`.
        *   `[âœ…]` 27.c.iii. Add a check for `recipeStep.job_type === 'PLAN'`. If true, implement the validation pattern from Recipe-Planner-Gap-Analysis.md Fix 3 (lines 1192-1206): validate that `context_for_documents` exists, is an array, and has entries (`contextForDocuments.length > 0`); validate that each entry has `document_key` (non-empty string) and `content_to_include` object model (must be an object, not an array at top level - use `Array.isArray()` check and throw error if array); **validate `content_to_include` structure conforms to `ContentToInclude` type**: Import the `isContentToInclude` helper function from `../../_shared/utils/type-guards/type_guards.dialectic.ts` (defined in step 24.c.vii) and use it to validate that each `content_to_include` entry conforms to the type (object, not array at top level, all values are allowed types). Include `context_for_documents` in the job payload structure so `assemblePlannerPrompt` can access it. All error messages must be prefixed with 'planAllToOne requires...'. Ensure the validation happens before creating the job payload, so invalid PLAN jobs fail early.
        *   `[âœ…]` 27.c.iv. After the PLAN job validation, add a check for `recipeStep.job_type === 'EXECUTE'`. If true, implement the validation pattern from Recipe-Planner-Gap-Analysis.md Fix 4 (lines 1226-1240): validate that `files_to_generate` exists, is an array, and has entries (`filesToGenerate.length > 0`); validate that each entry has `from_document_key` (non-empty string) and `template_filename` (non-empty string). All error messages must be prefixed with 'planAllToOne requires...'. Ensure the validation happens before creating the job payload, so invalid EXECUTE jobs fail early.
        *   `[âœ…]` 27.c.v. Ensure the existing `document_key` extraction logic (lines 44-74) still works correctly and is not affected by the new validation logic.
        *   `[âœ…]` 27.c.vi. The `DialecticPlanJobPayload` interface was already updated in step 18.b.v to include `context_for_documents?: ContextForDocument[]`. Use this field when creating PLAN job payloads by adding `context_for_documents` to the payload object when `job_type === 'PLAN'`.
        *   `[âœ…]` 27.c.vii. **Fix 9: Validate PLAN â†” EXECUTE structure mapping**: When creating EXECUTE jobs, validate that each `files_to_generate[].from_document_key` matches a `document_key` in the PLAN step's `context_for_documents`. If the planner has access to the PLAN step's `context_for_documents` (via recipe step lookup or parent job context), add validation: for each `file` in `filesToGenerate`, check if `file.from_document_key` exists in the PLAN step's `context_for_documents[].document_key` array. If no match is found, throw an error: `planAllToOne requires files_to_generate[].from_document_key '${file.from_document_key}' to match a document_key in the PLAN step's context_for_documents`. **Note**: This validation requires access to the PLAN step's `context_for_documents`. If the planner doesn't have direct access (e.g., needs to fetch from database), this validation is deferred to `assembleTurnPrompt` where `header_context` is already loaded (as noted in Fix 9 line 1394). The validation in `assembleTurnPrompt` (step 25.c.ii) will catch any mismatches at runtime.
    *   `[âœ…]` 27.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 27.b and ensure they now pass. Also verify that existing `planAllToOne` tests still pass.
    *   `[âœ…]` 27.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 28. **`[BE]` Fix planPairwiseByOrigin to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 28.a. `[DEPS]` The `planPairwiseByOrigin` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 56-86) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPairwiseByOrigin requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 28.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPairwiseByOrigin"` instead of `"planAllToOne"`.
    *   `[âœ…]` 28.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPairwiseByOrigin requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 28.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 28.b and ensure they now pass. Also verify that existing `planPairwiseByOrigin` tests still pass.
    *   `[âœ…]` 28.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 29. **`[BE]` Fix planPerModel to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 29.a. `[DEPS]` The `planPerModel` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 76-106) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerModel requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 29.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerModel"` instead of `"planAllToOne"`.
    *   `[âœ…]` 29.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerModel requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 29.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 29.b and ensure they now pass. Also verify that existing `planPerModel` tests still pass.
    *   `[âœ…]` 29.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 30. **`[BE]` Fix planPerSourceDocument to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 30.a. `[DEPS]` The `planPerSourceDocument` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 66-96) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceDocument requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 30.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceDocument"` instead of `"planAllToOne"`.
    *   `[âœ…]` 30.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceDocument requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 30.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 30.b and ensure they now pass. Also verify that existing `planPerSourceDocument` tests still pass.
    *   `[âœ…]` 30.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 31. **`[BE]` Fix planPerSourceDocumentByLineage to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 31.a. `[DEPS]` The `planPerSourceDocumentByLineage` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 54-84) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceDocumentByLineage requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 31.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceDocumentByLineage"` instead of `"planAllToOne"`.
    *   `[âœ…]` 31.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceDocumentByLineage requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 31.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 31.b and ensure they now pass. Also verify that existing `planPerSourceDocumentByLineage` tests still pass.
    *   `[âœ…]` 31.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 32. **`[BE]` Fix planPerSourceGroup to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 32.a. `[DEPS]` The `planPerSourceGroup` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 55-85) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceGroup requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 32.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceGroup"` instead of `"planAllToOne"`.
    *   `[âœ…]` 32.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceGroup requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 32.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 32.b and ensure they now pass. Also verify that existing `planPerSourceGroup` tests still pass.
    *   `[âœ…]` 32.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` and resolve any warnings or errors.

*   `[ ]` 33. **`[TEST-INT]` Fix Integration Test to Verify Complete Cross-Document Coordination Flow**
    *   `[ ]` 33.a. `[DEPS]` After all planners are updated (steps 27-32), `assembleTurnPrompt` is fixed (step 25), and `assemblePlannerPrompt` is fixed (step 26), the existing integration test file `supabase/integration_tests/services/planner_output_type.integration.test.ts` is incorrect and incomplete. Update this test to verify the complete PLAN â†’ EXECUTE cross-document coordination flow: (1) PLAN job receives `context_for_documents` from recipe step, (2) PLAN job generates `header_context` with filled `content_to_include` objects, (3) EXECUTE jobs consume the `header_context` and use alignment details, (4) documents generated in parallel are aligned using the same `header_context`. The test must verify that `files_to_generate` is read from recipe step (not header context) and that `context_for_documents` alignment details are used correctly.
    *   `[ ]` 33.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts`, fix and expand the test to verify the complete cross-document coordination flow.
        *   `[ ]` 33.b.i. Update the existing test that verifies `document_key` flow to also verify that `assembleTurnPrompt` reads `files_to_generate` from `stage.recipe_step.outputs_required.files_to_generate` (not from `headerContext.files_to_generate`).
        *   `[ ]` 33.b.ii. Update the test to verify that `headerContext.context_for_documents` contains filled `content_to_include` objects (not empty models), and that `assembleTurnPrompt` uses these alignment details in the prompt context.
        *   `[ ]` 33.b.iii. Add a test case that verifies the complete PLAN â†’ EXECUTE flow: create a PLAN job, verify it generates `header_context` with filled `content_to_include`, create EXECUTE jobs that consume that `header_context`, and verify that `assembleTurnPrompt` successfully matches `from_document_key` to `document_key` and uses the alignment details.
        *   `[ ]` 33.b.iv. Add a test case that verifies structure matching: ensure that `files_to_generate[].from_document_key` values match `context_for_documents[].document_key` values, and that `content_to_include` structures match between PLAN and EXECUTE steps.
        *   `[ ]` 33.b.v. Remove or fix any incorrect test code that assumes `files_to_generate` is in `headerContext` (the test incorrectly creates `headerContext` with `files_to_generate` - this should be removed or updated to match the correct architecture where `files_to_generate` is in recipe step).
    *   `[ ]` 33.c. `[TEST-INT]` **GREEN**: Re-run all tests from step 33.b and ensure they now pass. This verifies that the complete end-to-end cross-document coordination flow works correctly: PLAN jobs generate `header_context` with alignment details, EXECUTE jobs consume it correctly, and documents are aligned.
    *   `[ ]` 33.d. `[LINT]` Run the linter for `supabase/integration_tests/services/planner_output_type.integration.test.ts` and resolve any warnings or errors.
    *   `[ ]` 33.e. `[CRITERIA]` The integration test verifies: (1) PLAN jobs generate `header_context` with `context_for_documents` containing filled `content_to_include` objects, (2) EXECUTE jobs read `files_to_generate` from recipe step (not header context), (3) `assembleTurnPrompt` matches `from_document_key` to `document_key` and uses alignment details from `context_for_documents`, (4) structure matching is validated at runtime, (5) documents generated in parallel use the same alignment details from `header_context`.

*   `[âœ…]` 34. **`[CRITERIA]` Verify All Gaps Are Resolved**
    *   `[âœ…]` 34.a. `[DEPS]` According to Recipe-Planner-Gap-Analysis.md, all identified gaps must be resolved. This step verifies that the implementation addresses every gap.
    *   `[âœ…]` 34.b. `[CRITERIA]` Verify that all gaps from Recipe-Planner-Gap-Analysis.md are resolved.
        *   `[âœ…]` 34.b.i. **Gap 1 (Location Mismatch)**: Verify `assembleTurnPrompt` reads `files_to_generate` from recipe step, not header context. âœ… Resolved by step 25.
        *   `[âœ…]` 34.b.ii. **Gap 2 (Field Name Mismatch)**: Verify `assembleTurnPrompt` uses `from_document_key` (not `document_key`) when matching. âœ… Resolved by step 25.
        *   `[âœ…]` 34.b.iii. **Gap 3 (Inconsistent Recipe Definitions)**: Verify Synthesis PLAN steps don't have `files_to_generate`, and Synthesis EXECUTE steps do have `files_to_generate`. âœ… Resolved by step 21.
        *   `[âœ…]` 34.b.iv. **Gap 4 (Missing Validation)**: Verify planners validate `context_for_documents` for PLAN jobs and `files_to_generate` for EXECUTE jobs. âœ… Resolved by steps 27-32.
        *   `[âœ…]` 34.b.v. **Gap 5 (Missing HeaderContext Type)**: Verify `HeaderContext` interface is defined and exported. âœ… Resolved by step 18.
        *   `[âœ…]` 34.b.vi. **Gap 6 (Header Context Generation)**: Verify PLAN jobs generate `header_context` with `context_for_documents` (not `files_to_generate`). âœ… Resolved by steps 21, 26.
        *   `[âœ…]` 34.b.vii. **Gap 7 (Inconsistent PLAN Step Structure)**: Verify Antithesis `non_functional_requirements` uses object structure. âœ… Resolved by step 20.
        *   `[âœ…]` 34.b.viii. **Gap 8 (Inconsistent EXECUTE Step Structure)**: Verify Paralysis EXECUTE step has `content_to_include`. âœ… Resolved by step 23.
        *   `[âœ…]` 34.b.ix. **Gap 9 (PLAN â†” EXECUTE Structure Mapping)**: Verify `context_for_documents[].document_key` matches `files_to_generate[].from_document_key` and structures match. âœ… Resolved by steps 19, 20, 21, 22, 23.
        *   `[âœ…]` 34.b.x. **Gap 10 (Missing Type Definitions)**: Verify `ContentToInclude` and `HeaderContext` types are defined. âœ… Resolved by step 18.

*   `[âœ…]` 35. **`[COMMIT]` Commit All Changes with Comprehensive Message**
    *   `[âœ…]` 35.a. `[DEPS]` After all work is complete, tested, and linted, commit all changes with a comprehensive commit message that describes all fixes.
    *   `[âœ…]` 35.b. `[COMMIT]` Commit message: "fix: resolve recipe-planner data architecture gaps for cross-document coordination - Define ContentToInclude type for consistent content_to_include structures across all stages - Define HeaderContext interface matching actual PLAN job output structure (no files_to_generate) - Update ContextForDocument to use ContentToInclude type - Fix all 5 stage migrations: verify and fix structure matching between PLAN and EXECUTE steps (context_for_documents[].document_key matches files_to_generate[].from_document_key, content_to_include structures match exactly) - Fix Synthesis stage migration: remove files_to_generate from PLAN steps, add to EXECUTE steps - Fix Antithesis stage migration: change non_functional_requirements from array to object structure - Fix Paralysis stage migration: add content_to_include to actionable_checklist EXECUTE step - Update isHeaderContext type guard to match new HeaderContext interface (remove files_to_generate validation) - Fix assembleTurnPrompt to read files_to_generate from recipe step (not header context) and use from_document_key - Fix assembleTurnPrompt to extract alignment details from header_context.context_for_documents - Fix assemblePlannerPrompt to include context_for_documents in PLAN prompts - Update all six planners to validate and pass context_for_documents for PLAN jobs - Update all six planners to validate files_to_generate for EXECUTE jobs - Add comprehensive unit tests for all changes - Fix integration test to verify complete cross-document coordination flow - Resolves all 10 gaps identified in Recipe-Planner-Gap-Analysis.md"


*   `[âœ…]` 36. **`[PROMPT]` Fix All Prompt Templates and Document Templates to Exactly Match Database Structures**
    *   `[âœ…]` 36.a. `[DEPS]` The prompt template files in `docs/prompts/` and document template files in `docs/templates/` are loaded from storage and used by `assemblePlannerPrompt` and `assembleTurnPrompt`. These files must EXACTLY match the database structures defined in the stage migration files (`supabase/migrations/*_stage.sql`). The database migration files define the expected structure for `context_for_documents`, `content_to_include`, `files_to_generate`, and document templates. Any mismatch between the file-based templates and database structures causes PLAN jobs to generate invalid `header_context` artifacts (as seen in the investigation where `feature_spec.content_to_include` was an array instead of an object). Every prompt template file must be FIXED to match its corresponding database structure in the migration file. Every document template file must be FIXED to match its corresponding database structure. The database migration files are the source of truth. The file-based templates must be corrected to exactly match. This correction ensures that the AI agent receives instructions that match the validated type structures, preventing structural validation failures at runtime. Each step must: (1) read the database structure from the migration file, (2) read the corresponding prompt/template file, (3) UPDATE the file to exactly match the database structure, (4) ensure all `content_to_include` structures are objects (not arrays at top level), (5) ensure all `document_key` values match, (6) ensure all `template_filename` values match.
    *   `[âœ…]` 36.b. **`[PROMPT]` Fix Thesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.b.i. `[DEPS]` The thesis stage planner prompt template file is `docs/prompts/thesis/thesis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194531_thesis_stage.sql` defines the expected `context_for_documents` structure for the PLAN step (lines 201-264). The planner prompt template contains a JSON schema example showing the expected `HeaderContext` structure, including `context_for_documents` array with `content_to_include` objects for each document. The template's schema example must EXACTLY match the database structure. The investigation identified that the template file currently shows `feature_spec.content_to_include` as an array (lines 84-90), but the database expects an object structure with `{"features": [...]}`. The database structure is the source of truth and the template must be corrected to match it exactly.
        *   `[âœ…]` 36.b.ii. `[PROMPT]` Read `supabase/migrations/20251006194531_thesis_stage.sql` PLAN step `context_for_documents` structure (lines 201-264) to get the exact database structure for all four documents: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`.
        *   `[âœ…]` 36.b.iii. `[PROMPT]` Read `docs/prompts/thesis/thesis_planner_header_v1.md` and locate the `HeaderContext` schema example (lines 36-111) that shows the JSON structure with `context_for_documents` array.
        *   `[âœ…]` 36.b.iv. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to fix `feature_spec.content_to_include`: change from array structure `[{...}]` (lines 84-90) to object structure `{"features": [{"feature_name": "", "feature_objective": "", "user_stories": [], "acceptance_criteria": [], "dependencies": [], "success_metrics": []}]}` exactly matching the database structure (lines 221-232).
        *   `[âœ…]` 36.b.v. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `business_case.content_to_include` matches the database structure exactly: include all fields (market_opportunity, user_problem_validation, competitive_analysis, differentiation_&_value_proposition, risks_&_mitigation, strengths, weaknesses, opportunities, threats, next_steps, proposal_references, executive_summary) with exact field names from database (lines 204-217).
        *   `[âœ…]` 36.b.vi. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `technical_approach.content_to_include` matches the database structure exactly: include all fields (architecture, components, data, deployment, sequencing, risk_mitigation, open_questions) with exact field names from database (lines 236-244).
        *   `[âœ…]` 36.b.vii. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `success_metrics.content_to_include` matches the database structure exactly: include all fields (outcome_alignment, north_star_metric, primary_kpis, leading_indicators, lagging_indicators, guardrails, measurement_plan, risk_signals, next_steps, data_sources, reporting_cadence, ownership, escalation_plan) with exact field names from database (lines 248-262).
        *   `[âœ…]` 36.b.viii. `[PROMPT]` Verify all `content_to_include` structures in the updated template are objects (not arrays at top level). Ensure the JSON schema example in the template exactly reflects the database structure. Ensure proper JSON formatting and indentation is maintained.
    *   `[âœ…]` 36.c. **`[PROMPT]` Fix Thesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.c.i. `[DEPS]` The thesis stage has four turn prompt template files: `docs/prompts/thesis/thesis_business_case_turn_v1.md`, `docs/prompts/thesis/thesis_feature_spec_turn_v1.md`, `docs/prompts/thesis/thesis_technical_approach_turn_v1.md`, `docs/prompts/thesis/thesis_success_metrics_turn_v1.md`. These turn prompts are used by `assembleTurnPrompt` for EXECUTE jobs. The database migration file defines the expected document structures in EXECUTE steps (lines 323-891). Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions. The database structure is the source of truth.
        *   `[âœ…]` 36.c.ii. `[PROMPT]` Read each thesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194531_thesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.d. **`[PROMPT]` Fix Thesis Stage Document Templates**
        *   `[âœ…]` 36.d.i. `[DEPS]` The thesis stage has four document template files: `docs/templates/thesis/thesis_business_case.md`, `docs/templates/thesis/thesis_feature_spec.md`, `docs/templates/thesis/thesis_technical_approach.md`, `docs/templates/thesis/thesis_success_metrics.md`. These document templates are referenced by `template_filename` in the database EXECUTE steps. The database migration file defines the expected `template_filename` values and `documents[].content_to_include` structures for each document. Template file content structure must exactly match the expected document structure from database.
        *   `[âœ…]` 36.d.ii. `[PROMPT]` In `supabase/migrations/20251006194531_thesis_stage.sql`, identify all `template_filename` values in EXECUTE steps. Ensure corresponding files exist in `docs/templates/thesis/`. If any files are missing, create them with the correct structure.
        *   `[âœ…]` 36.d.iii. `[PROMPT]` Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document from the EXECUTE step. Templates may use placeholder syntax (e.g., `{{field_name}}`) that must map to fields in `content_to_include`. Ensure all field names match exactly with the database structure.
        *   `[âœ…]` 36.d.iv. `[PROMPT]` Update each document template file to ensure the template structure exactly aligns with the database structure. If templates reference fields, ensure all field names match the database `content_to_include` structure exactly.
    *   `[âœ…]` 36.e. **`[PROMPT]` Fix Antithesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.e.i. `[DEPS]` The antithesis stage planner prompt template file is `docs/prompts/antithesis/antithesis_planner_review_v1.md`. The database migration file `supabase/migrations/20251006194542_antithesis_stage.sql` defines the expected `context_for_documents` structure for the PLAN step. According to step 20 of this checklist, `non_functional_requirements.content_to_include` was changed from an array to an object structure `{"categories": [...]}` in the database. The planner prompt template must match this corrected structure exactly.
        *   `[âœ…]` 36.e.ii. `[PROMPT]` Read `supabase/migrations/20251006194542_antithesis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents.
        *   `[âœ…]` 36.e.iii. `[PROMPT]` Read `docs/prompts/antithesis/antithesis_planner_review_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.e.iv. `[PROMPT]` Update `docs/prompts/antithesis/antithesis_planner_review_v1.md` to fix `non_functional_requirements.content_to_include`: change from array structure `[...]` to object structure `{"categories": [...]}` exactly matching the database structure.
        *   `[âœ…]` 36.e.v. `[PROMPT]` Update `docs/prompts/antithesis/antithesis_planner_review_v1.md` to ensure all other `context_for_documents` entries match database structures exactly: `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `dependency_map`, `comparison_vector`. Ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.f. **`[PROMPT]` Fix Antithesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.f.i. `[DEPS]` The antithesis stage has six turn prompt template files: `docs/prompts/antithesis/antithesis_business_case_critique_turn_v1.md`, `docs/prompts/antithesis/antithesis_feasibility_assessment_turn_v1.md`, `docs/prompts/antithesis/antithesis_risk_register_turn_v1.md`, `docs/prompts/antithesis/antithesis_non_functional_requirements_turn_v1.md`, `docs/prompts/antithesis/antithesis_dependency_map_turn_v1.md`, `docs/prompts/antithesis/antithesis_comparison_vector_turn_v1.md`. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.f.ii. `[PROMPT]` Read each antithesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194542_antithesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.g. **`[PROMPT]` Fix Antithesis Stage Document Templates**
        *   `[âœ…]` 36.g.i. `[DEPS]` The antithesis stage has six document template files: `docs/templates/antithesis/antithesis_business_case_critique.md`, `docs/templates/antithesis/antithesis_feasibility_assessment.md`, `docs/templates/antithesis/antithesis_risk_register.md`, `docs/templates/antithesis/antithesis_non_functional_requirements.md`, `docs/templates/antithesis/antithesis_dependency_map.md`, `docs/templates/antithesis/antithesis_comparison_vector.json`. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.g.ii. `[PROMPT]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.h. **`[PROMPT]` Fix Synthesis Stage Planner Prompt Templates**
        *   `[âœ…]` 36.h.i. `[DEPS]` The synthesis stage has two planner prompt template files: `docs/prompts/synthesis/synthesis_pairwise_header_planner_v1.md` (for pairwise planning) and `docs/prompts/synthesis/synthesis_final_header_planner_v1.md` (for final header planning). According to step 21 of this checklist, PLAN steps should NOT have `files_to_generate` (it was removed from database). The database migration file `supabase/migrations/20251006194549_synthesis_stage.sql` defines the expected structures. Planner prompt templates must not reference `files_to_generate` in their schema examples, and `context_for_documents` structures must match database definitions exactly.
        *   `[âœ…]` 36.h.ii. `[PROMPT]` Read `supabase/migrations/20251006194549_synthesis_stage.sql` PLAN step `context_for_documents` structure for pairwise planning to get the exact database structure.
        *   `[âœ…]` 36.h.iii. `[PROMPT]` Read `docs/prompts/synthesis/synthesis_pairwise_header_planner_v1.md` and locate the `HeaderContext` schema example. Remove any `files_to_generate` references if present. Update all `context_for_documents` entries to exactly match the database PLAN step structure.
        *   `[âœ…]` 36.h.iv. `[PROMPT]` Read `supabase/migrations/20251006194549_synthesis_stage.sql` PLAN step `context_for_documents` structure for final header planning to get the exact database structure.
        *   `[âœ…]` 36.h.v. `[PROMPT]` Read `docs/prompts/synthesis/synthesis_final_header_planner_v1.md` and locate the `HeaderContext` schema example. Remove any `files_to_generate` references if present. Update all `context_for_documents` entries to exactly match the database PLAN step structure.
    *   `[âœ…]` 36.i. **`[PROMPT]` Fix Synthesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.i.i. `[DEPS]` The synthesis stage has thirteen turn prompt template files covering pairwise, document-level, and final deliverables. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.i.ii. `[PROMPT]` Read each synthesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194549_synthesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.j. **`[PROMPT]` Fix Synthesis Stage Document Templates**
        *   `[âœ…]` 36.j.i. `[DEPS]` The synthesis stage has thirteen document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.j.ii. `[PROMPT]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.k. **`[PROMPT]` Fix Parenthesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.k.i. `[DEPS]` The parenthesis stage planner prompt template file is `docs/prompts/parenthesis/parenthesis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194558_parenthesis_stage.sql` defines the expected `context_for_documents` structure. The planner prompt template must match the database structure exactly.
        *   `[âœ…]` 36.k.ii. `[PROMPT]` Read `supabase/migrations/20251006194558_parenthesis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents: `technical_requirements`, `master_plan`, `milestone_schema`.
        *   `[âœ…]` 36.k.iii. `[PROMPT]` Read `docs/prompts/parenthesis/parenthesis_planner_header_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.k.iv. `[PROMPT]` Update `docs/prompts/parenthesis/parenthesis_planner_header_v1.md` to ensure all `context_for_documents` entries exactly match database structures: ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.l. **`[PROMPT]` Fix Parenthesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.l.i. `[DEPS]` The parenthesis stage has four turn prompt template files. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.l.ii. `[PROMPT]` Read each parenthesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194558_parenthesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.m. **`[PROMPT]` Fix Parenthesis Stage Document Templates**
        *   `[âœ…]` 36.m.i. `[DEPS]` The parenthesis stage has three document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.m.ii. `[PROMPT]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.n. **`[PROMPT]` Fix Paralysis Stage Planner Prompt Template**
        *   `[âœ…]` 36.n.i. `[DEPS]` The paralysis stage planner prompt template file is `docs/prompts/paralysis/paralysis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194605_paralysis_stage.sql` defines the expected `context_for_documents` structure. According to step 23 of this checklist, the EXECUTE step for `actionable_checklist` was updated to include `content_to_include` in the database. The planner prompt template must match the database structure exactly.
        *   `[âœ…]` 36.n.ii. `[PROMPT]` Read `supabase/migrations/20251006194605_paralysis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents: `actionable_checklist`, `updated_master_plan`, `advisor_recommendations`.
        *   `[âœ…]` 36.n.iii. `[PROMPT]` Read `docs/prompts/paralysis/paralysis_planner_header_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.n.iv. `[PROMPT]` Update `docs/prompts/paralysis/paralysis_planner_header_v1.md` to ensure all `context_for_documents` entries exactly match database structures: ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.o. **`[PROMPT]` Fix Paralysis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.o.i. `[DEPS]` The paralysis stage has four turn prompt template files. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.o.ii. `[PROMPT]` Read each paralysis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194605_paralysis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.p. **`[PROMPT]` Fix Paralysis Stage Document Templates**
        *   `[âœ…]` 36.p.i. `[DEPS]` The paralysis stage has three document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.p.ii. `[PROMPT]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.q. **`[PROMPT]` Upload Updated Prompt and Document Templates to Storage**
        *   `[âœ…]` 36.q.i. `[DEPS]` After all prompt template files and document template files have been fixed to match database structures, they must be uploaded to storage using the `seed_prompt_templates.ts` script. The script uploads files from `docs/prompts/` and `docs/templates/` to the storage bucket referenced by `dialectic_document_templates` table. This ensures that when `assemblePlannerPrompt` and `assembleTurnPrompt` load templates from storage, they receive the corrected versions that match database structures.
        *   `[âœ…]` 36.q.ii. `[PROMPT]` Run the `seed_prompt_templates.ts` script to upload all updated prompt and document templates to storage. Ensure the script completes successfully and that all files are uploaded to the correct storage paths as defined in the database `dialectic_document_templates` table.
    *   `[âœ…]` 36.r. `[CRITERIA]` All prompt template files and document template files EXACTLY match their corresponding database structures in the stage migration files. All `content_to_include` structures are objects (not arrays at top level). All `document_key` values match between templates and database. All `template_filename` values match between database and actual files. All updated templates have been uploaded to storage. No structural mismatches exist between file-based templates and database definitions.

*   `[âœ…]` 37. **`[STORE]` Fix Notification Store Payload Extraction and Validation to Systematically Match Type Definitions**
    *   `[âœ…]` 37.a. `[DEPS]` The `handleIncomingNotification` function in `packages/store/src/notificationStore.ts` has systematic architectural flaws that cause notifications to be silently ignored or incorrectly processed: (1) **Inconsistent payload extraction**: Event payload construction (lines 76-269) does not systematically extract all optional fields defined in type definitions. For example, `document_started` (lines 152-171) does not extract `latestRenderedResourceId` even though `DocumentStartedPayload` extends `DocumentLifecyclePayload` which includes `latestRenderedResourceId?: string | null` (defined in `packages/types/src/dialectic.types.ts` line 765), while `document_completed` (lines 196-216) correctly extracts it (line 214). This inconsistency causes `handleDocumentStartedLogic` in `packages/store/src/dialecticStore.documents.ts` (lines 500-508) to ignore events for documents requiring rendering when `latestRenderedResourceId` is missing, even though the backend may have sent it in the notification data. (2) **Validation failures still add notifications**: For `WALLET_TRANSACTION` events (lines 285-296), when validation fails (line 287 check fails), the function logs a warning (line 294) but still adds the notification to the list (line 298) without calling the wallet handler (line 289), causing invalid notifications to appear in the UI while the actual wallet update is ignored. The backend sends `WALLET_TRANSACTION` notifications with `walletId` and `newBalance` in the data field (see `supabase/functions/_shared/services/tokenWalletService.ts` lines 312-318), but the validation may fail if the data structure is malformed or if fields are missing. (3) **No systematic type-to-extraction mapping**: There is no systematic approach ensuring that all fields defined in type definitions (`packages/types/src/dialectic.types.ts` and `supabase/functions/_shared/types/notification.service.types.ts`) are extracted from notification data during payload construction. Each event type has ad-hoc validation and extraction logic, leading to inconsistencies where some optional fields are extracted for some events but not others, even when they share the same base type. The fix requires: (1) creating a systematic payload extraction pattern that extracts ALL optional fields defined in type definitions for each event type, (2) ensuring validation failures prevent invalid notifications from being added to the notification list, (3) documenting the mapping between type definitions and extraction logic to prevent future inconsistencies, (4) fixing all existing event types to extract all optional fields from their type definitions.
    *   `[âœ…]` 37.b. `[TYPES]` Audit all notification event type definitions to create a complete mapping of required and optional fields.
        *   `[âœ…]` 37.b.i. Read `packages/types/src/dialectic.types.ts` and identify all `DialecticLifecycleEvent` union members and their type definitions: `ContributionGenerationStartedPayload`, `DialecticContributionStartedPayload`, `ContributionGenerationRetryingPayload`, `DialecticContributionReceivedPayload`, `ContributionGenerationFailedPayload`, `ContributionGenerationContinuedPayload`, `ContributionGenerationCompletePayload`, `DialecticProgressUpdatePayload`, `PlannerStartedPayload`, `DocumentStartedPayload`, `DocumentChunkCompletedPayload`, `DocumentCompletedPayload`, `RenderCompletedPayload`, `JobFailedPayload`. Document all required fields and all optional fields (marked with `?`) for each type.
        *   `[âœ…]` 37.b.ii. Read `supabase/functions/_shared/types/notification.service.types.ts` and identify all notification payload type definitions, including `DocumentPayload` base interface and all extending interfaces. Document all required and optional fields.
        *   `[âœ…]` 37.b.iii. Create a comprehensive mapping document (or inline comments) that lists for each event type: (a) the type definition location, (b) all required fields, (c) all optional fields, (d) the current extraction status in `notificationStore.ts` (which fields are extracted, which are missing), (e) the validation requirements. This mapping will serve as the specification for the systematic fix.
        *   `[âœ…]` 37.b.iv. Identify all base types and interfaces that are extended by multiple event types (e.g., `DocumentLifecyclePayload` is extended by `DocumentStartedPayload`, `DocumentCompletedPayload`, `DocumentChunkCompletedPayload`, `RenderCompletedPayload`, `JobFailedPayload`). Document which optional fields from base types are currently extracted inconsistently across extending types.
    *   `[âœ…]` 37.c. `[TEST-UNIT]` **RED**: In `packages/store/src/notificationStore.test.ts`, write comprehensive tests that verify all optional fields are extracted from notification data for all event types.
        *   `[âœ…]` 37.c.i. For each event type in the `DialecticLifecycleEvent` union, create a test case that mocks a notification with ALL optional fields present in the notification data (as sent by the backend), and assert that the constructed `eventPayload` includes all optional fields that are defined in the type definition. For example, for `document_started`, create a test that mocks `notification.data` with `latestRenderedResourceId: "resource-123"` and assert that `eventPayload.latestRenderedResourceId === "resource-123"`. These tests must fail because the current implementation does not extract all optional fields.
        *   `[âœ…]` 37.c.ii. Create test cases that verify validation failures prevent invalid notifications from being added to the notification list. For `WALLET_TRANSACTION`, create a test that mocks a notification with invalid data (missing `walletId` or `newBalance`), and assert that: (a) the wallet handler is NOT called, (b) the notification is NOT added to the notification list (verify `addNotification` is not called or the notification list does not contain the invalid notification), (c) an error is logged. This test must fail because the current implementation adds invalid notifications to the list.
        *   `[âœ…]` 37.c.iii. Create test cases that verify optional fields are correctly extracted when present and set to `undefined` when missing (not `null` unless the type allows `null`). For each event type with optional fields, create two test cases: (1) notification data includes the optional field, assert it's extracted, (2) notification data omits the optional field, assert it's `undefined` in the payload (or `null` if the type allows it).
        *   `[âœ…]` 37.c.iv. Create test cases that verify base type optional fields are extracted consistently across all extending types. For example, verify that `latestRenderedResourceId` is extracted for `document_started`, `document_completed`, `document_chunk_completed`, and `render_completed` (all extend `DocumentLifecyclePayload`). This test must fail because `document_started` currently does not extract `latestRenderedResourceId`.
    *   `[âœ…]` 37.d. `[STORE]` **GREEN**: In `packages/store/src/notificationStore.ts`, systematically fix payload extraction to match type definitions and fix validation failure handling.
        *   `[âœ…]` 37.d.i. For each event type case in the switch statement (lines 76-269), update the payload construction to extract ALL optional fields defined in the corresponding type definition. Use the mapping from step 37.b.iii as the specification. For example, for `document_started` (lines 152-171), add extraction of `latestRenderedResourceId` after line 169: `latestRenderedResourceId: typeof data['latestRenderedResourceId'] === 'string' ? data['latestRenderedResourceId'] : (data['latestRenderedResourceId'] === null ? null : undefined),` (matching the pattern used in `document_completed` line 214, but also handling `null` if the type allows it). Ensure the extraction logic matches the type definition exactly (if the type allows `string | null | undefined`, extract accordingly; if it only allows `string | undefined`, do not set `null`).
        *   `[âœ…]` 37.d.ii. For `document_chunk_completed` (lines 173-195), verify that all optional fields from `DocumentChunkCompletedPayload` are extracted: `step_key`, `isFinalChunk`, `continuationNumber`, and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.iii. For `planner_started` (lines 131-151), verify that all optional fields from `PlannerStartedPayload` are extracted: `step_key` and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.iv. For `job_failed` (lines 241-263), verify that all optional fields from `JobFailedPayload` are extracted: `step_key` and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.v. For `contribution_generation_retrying` (lines 87-91), verify that all optional fields are extracted. The current implementation extracts `error` as optional (line 89), which appears correct, but verify against the type definition.
        *   `[âœ…]` 37.d.vi. For `contribution_generation_failed` (lines 103-107), verify that all optional fields are extracted: `job_id` and `modelId` are already extracted as optional (line 105), which appears correct, but verify against the type definition.
        *   `[âœ…]` 37.d.vii. For `WALLET_TRANSACTION` handling (lines 285-296), fix the validation failure behavior: when validation fails (line 287 check fails), do NOT call `get().addNotification(notification)` (line 298). Instead, log an error (not just a warning) with the invalid data structure, and return early without adding the notification to the list. The notification should only be added to the list if validation passes AND the wallet handler is called (or if the wallet handler call fails, still add it but log the failure). This ensures invalid notifications do not appear in the UI while the actual update is ignored.
        *   `[âœ…]` 37.d.viii. Add comprehensive JSDoc comments above the switch statement (around line 75) documenting the systematic approach: "This switch statement constructs type-safe payloads from notification data. Each case must extract ALL optional fields defined in the corresponding type definition. Base type optional fields (e.g., `latestRenderedResourceId` from `DocumentLifecyclePayload`) must be extracted consistently across all extending types. Validation failures must prevent invalid notifications from being added to the notification list."
        *   `[âœ…]` 37.d.ix. Verify that all extraction logic uses the same pattern: `fieldName: typeof data['fieldName'] === 'expectedType' ? data['fieldName'] : (data['fieldName'] === null && typeAllowsNull ? null : undefined)`. This ensures type safety and consistent handling of missing vs null vs undefined values.
    *   `[âœ…]` 37.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 37.c and ensure they now pass. Also verify that existing `notificationStore` tests still pass (update any tests that mock notifications to include all optional fields if they test payload construction).
    *   `[âœ…]` 37.f. `[TEST-INT]` **RED**: In `packages/store/src/dialecticStore.notifications.integration.test.ts` or create a new integration test file, write tests that verify end-to-end notification handling with optional fields.
        *   `[âœ…]` 37.f.i. Create a test case that simulates a `document_started` notification with `latestRenderedResourceId` in the notification data, and verify that `handleDocumentStartedLogic` receives the `latestRenderedResourceId` and processes the event correctly (does not ignore it). This test must fail initially if `document_started` extraction is not fixed, but should pass after step 37.d.i.
        *   `[âœ…]` 37.f.ii. Create a test case that simulates a `WALLET_TRANSACTION` notification with invalid data (missing `walletId`), and verify that: (a) the notification is NOT added to the notification list, (b) the wallet handler is NOT called, (c) an error is logged. This test must fail initially because invalid notifications are currently added to the list.
        *   `[âœ…]` 37.f.iii. Create a test case that simulates a `WALLET_TRANSACTION` notification with valid data, and verify that: (a) the wallet handler IS called with the correct data, (b) the notification IS added to the notification list. This test should pass initially but must continue to pass after the validation fix.
    *   `[âœ…]` 37.g. `[TEST-INT]` **GREEN**: Re-run all tests from step 37.f and ensure they now pass. This verifies that the end-to-end notification flow works correctly with the systematic payload extraction and validation fixes.
    *   `[âœ…]` 37.h. `[LINT]` Run the linter for `packages/store/src/notificationStore.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 37.i. `[CRITERIA]` All notification event types extract ALL optional fields defined in their type definitions. Base type optional fields (e.g., `latestRenderedResourceId` from `DocumentLifecyclePayload`) are extracted consistently across all extending types. Validation failures prevent invalid notifications from being added to the notification list. All optional fields are extracted using a consistent pattern that handles `undefined`, `null`, and missing values according to type definitions. The mapping between type definitions and extraction logic is documented and systematic, preventing future inconsistencies. All tests pass, including tests that verify optional field extraction and validation failure handling.

*   `[âœ…]` 38. **`[UI]` Fix SessionContributionsDisplayCard to Filter Documents Based on StageRunChecklist Highlighting**
    *   `[âœ…]` 38.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` currently renders ALL documents from `documentGroups` (lines 552-647) as Card components without filtering. However, it should only render cards for documents that are highlighted in `StageRunChecklist`. A document is highlighted in `StageRunChecklist` when `focusedStageDocumentMap[focusKey]?.documentKey === entry.documentKey`, where `focusKey = `${sessionId}:${stageSlug}:${modelId}`` (as determined in `apps/web/src/components/dialectic/StageRunChecklist.tsx` lines 349-358). The highlighting logic is currently duplicated: `buildFocusedDocumentKey` is a private function in `StageRunChecklist.tsx` (lines 75-76), and the highlighting check is inline (lines 349-358). To ensure DRY compliance and consistent behavior, the highlighting logic must be extracted to a shared utility in `@paynless/utils` package. The component needs to: (1) access `focusedStageDocument` from the store (similar to how `GeneratedContributionCard` accesses it at line 95), (2) use the shared utility function to filter `documentGroups` to only include documents that are highlighted, (3) ensure documents that are not highlighted are not rendered at all (not just hidden). The shared utility functions (`buildFocusedDocumentKey` and `isDocumentHighlighted`) are pure business logic functions, not UI-specific, so they belong in `packages/utils/src/dialecticUtils.ts` and must be exported from `packages/utils/src/index.ts`. Both `StageRunChecklist.tsx` and `SessionContributionsDisplayCard.tsx` will import from `@paynless/utils`. The type `FocusedStageDocumentState` is already defined in `packages/types/src/dialectic.types.ts` (lines 470-473) and exported, so no type changes are required.
    *   `[âœ…]` 38.b. `[TYPES]` Verify that `FocusedStageDocumentState` type is available from `@paynless/types` package. The type is defined in `packages/types/src/dialectic.types.ts` (lines 470-473) as `export interface FocusedStageDocumentState { modelId: string; documentKey: string; }`. Confirm it is exported from `packages/types/src/index.ts` (or equivalent export file). If not exported, add export. No type changes are required if already exported.
    *   `[âœ…]` 38.c. `[TEST-UNIT]` **RED**: In `packages/utils/src/dialecticUtils.test.ts`, write tests that verify the shared utility functions work correctly.
        *   `[âœ…]` 38.c.i. Create a test file following the pattern of `packages/utils/src/type_guards.test.ts` using vitest (`describe`, `it`, `expect`).
        *   `[âœ…]` 38.c.ii. Import `buildFocusedDocumentKey` and `isDocumentHighlighted` from `./dialecticUtils` (the functions don't exist yet, so this will fail initially).
        *   `[âœ…]` 38.c.iii. Write test cases for `buildFocusedDocumentKey`: (1) returns correct format `${sessionId}:${stageSlug}:${modelId}` for valid inputs, (2) handles empty strings correctly, (3) handles special characters in IDs correctly.
        *   `[âœ…]` 38.c.iv. Write test cases for `isDocumentHighlighted`: (1) returns `true` when `focusedStageDocumentMap[focusKey]?.documentKey === documentKey` matches, (2) returns `false` when `focusKey` doesn't exist in map, (3) returns `false` when `documentKey` doesn't match, (4) returns `false` when `sessionId`, `stageSlug`, or `modelId` are empty/missing, (5) returns `false` when `focusedStageDocumentMap` is `undefined` or `null`, (6) returns `false` when `focusedStageDocumentMap[focusKey]` is `null`. Import `FocusedStageDocumentState` from `@paynless/types` for test fixtures. These tests must fail because the functions don't exist yet.
    *   `[âœ…]` 38.d. `[UTILS]` **GREEN**: In `packages/utils/src/dialecticUtils.ts`, create the shared utility functions.
        *   `[âœ…]` 38.d.i. Create the file following the pattern of `packages/utils/src/stringUtils.ts` with JSDoc comments.
        *   `[âœ…]` 38.d.ii. Import `FocusedStageDocumentState` from `@paynless/types`.
        *   `[âœ…]` 38.d.iii. Export `buildFocusedDocumentKey` function: `export const buildFocusedDocumentKey = (sessionId: string, stageSlug: string, modelId: string): string => `${sessionId}:${stageSlug}:${modelId}`;` with JSDoc explaining the format and parameters.
        *   `[âœ…]` 38.d.iv. Export `isDocumentHighlighted` function that: (1) checks if `sessionId`, `stageSlug`, and `modelId` are truthy (returns `false` if any are missing), (2) calls `buildFocusedDocumentKey` to construct `focusKey`, (3) checks if `focusedStageDocumentMap` exists and `focusedStageDocumentMap[focusKey]?.documentKey === documentKey`, (4) returns boolean result. Include JSDoc explaining the function matches StageRunChecklist logic and parameters.
        *   `[âœ…]` 38.d.v. Ensure both functions are exported (not just defined) so they can be imported by consumers.
    *   `[âœ…]` 38.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.c and ensure they now pass. The tests should verify that both utility functions work correctly with all test cases.
    *   `[âœ…]` 38.f. `[UTILS]` In `packages/utils/src/index.ts`, export the new utility functions.
        *   `[âœ…]` 38.f.i. Add `export * from './dialecticUtils';` to the list of exports (after line 6, following the existing pattern).
        *   `[âœ…]` 38.f.ii. Verify the export is added correctly and follows the same pattern as other exports.
    *   `[âœ…]` 38.g. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/StageRunChecklist.test.tsx`, update existing tests to verify the component uses the shared utility functions.
        *   `[âœ…]` 38.g.i. Read the existing test file to understand current test structure and what tests exist for highlighting behavior.
        *   `[âœ…]` 38.g.ii. Add test cases that verify `StageRunChecklist` correctly uses `isDocumentHighlighted` from `@paynless/utils` (mock the import if needed, or verify the behavior matches the shared utility logic). The tests should verify that highlighting behavior matches the shared utility function behavior.
        *   `[âœ…]` 38.g.iii. These tests should pass after `StageRunChecklist.tsx` is updated to use the shared utility, but may fail initially if the component still uses private logic.
    *   `[âœ…]` 38.h. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/StageRunChecklist.tsx`, replace private implementation with shared utility functions.
        *   `[âœ…]` 38.h.i. Remove the private `buildFocusedDocumentKey` function (lines 75-76).
        *   `[âœ…]` 38.h.ii. Add import at the top of the file: `import { buildFocusedDocumentKey, isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 38.h.iii. Replace the inline highlighting logic (lines 349-358) with a call to `isDocumentHighlighted(activeSessionId ?? '', activeStageSlug ?? '', documentModelId ?? '', entry.documentKey, effectiveFocusedStageDocumentMap)`. Store the result in `isActive` variable.
        *   `[âœ…]` 38.h.iv. Verify that the `buildFocusedDocumentKey` function is no longer used directly in this file (it's now only used internally by `isDocumentHighlighted`).
        *   `[âœ…]` 38.h.v. Ensure all existing functionality is preserved - only the implementation source changes, not the behavior.
    *   `[âœ…]` 38.i. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.g and ensure they now pass. Also verify that all existing `StageRunChecklist` tests still pass (the behavior should be identical, just using shared utilities).
    *   `[âœ…]` 38.j. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write tests that verify the component only renders cards for documents that are highlighted in StageRunChecklist.
        *   `[âœ…]` 38.j.i. Create a test case that mocks store state with multiple documents in `documentsByModel` for a model (e.g., `'business_case'` and `'feature_spec'`), but only one document is highlighted in `focusedStageDocument` (e.g., `focusedStageDocument['session1:thesis:model1'] = { modelId: 'model1', documentKey: 'business_case' }`). Use the same `focusKey` format (`${sessionId}:${stageSlug}:${modelId}`) as the shared utility.
        *   `[âœ…]` 38.j.ii. Render `SessionContributionsDisplayCard` and assert that only the highlighted document (`'business_case'`) has a Card component rendered (check for `data-testid={`stage-document-card-${modelId}-business_case`}`).
        *   `[âœ…]` 38.j.iii. Assert that the non-highlighted document (`'feature_spec'`) does NOT have a Card component rendered (verify `data-testid={`stage-document-card-${modelId}-feature_spec`}` is not present in the DOM).
        *   `[âœ…]` 38.j.iv. Create a test case where no documents are highlighted (`focusedStageDocument` is empty object `{}` or `undefined`), and assert that no document cards are rendered (only the "No documents generated yet" message or similar).
        *   `[âœ…]` 38.j.v. Create a test case with multiple models, where each model has different highlighted documents, and verify that only highlighted documents are rendered for each model. For example, `model-a` has `business_case` highlighted, `model-b` has `feature_spec` highlighted, and verify only those specific cards are rendered. This test must fail because `SessionContributionsDisplayCard` currently renders all documents regardless of highlighting status.
    *   `[âœ…]` 38.k. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, add filtering logic to only render documents that are highlighted in StageRunChecklist using the shared utility.
        *   `[âœ…]` 38.k.i. Add import at the top: `import { isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 38.k.ii. Add type import: `import type { FocusedStageDocumentState } from '@paynless/types';` (if not already imported).
        *   `[âœ…]` 38.k.iii. Access `focusedStageDocument` from the store after line 114 (after `modelCatalog` access): `const focusedStageDocument = useDialecticStore((state) => state.focusedStageDocument);`
        *   `[âœ…]` 38.k.iv. Create a `useMemo` hook (before line 285, before `documentGroups` definition) that filters `documentGroups` to only include highlighted documents: `const filteredDocumentGroups = useMemo(() => { if (!session || !activeStage) { return []; } return documentGroups.map(([modelId, documents]) => { const filteredDocuments = documents.filter((document) => isDocumentHighlighted(session.id, activeStage.slug, modelId, document.documentKey, focusedStageDocument)); return filteredDocuments.length > 0 ? [modelId, filteredDocuments] as const : null; }).filter((group): group is [string, StageDocumentChecklistEntry[]] => group !== null); }, [documentGroups, session, activeStage, focusedStageDocument]);`
        *   `[âœ…]` 38.k.v. Update the `hasDocuments` check (line 312) to use `filteredDocumentGroups` instead of `documentGroups`: `const hasDocuments = useMemo(() => filteredDocumentGroups.some(([, documents]) => documents.length > 0), [filteredDocumentGroups]);`
        *   `[âœ…]` 38.k.vi. Update the rendering logic (line 552) to use `filteredDocumentGroups.map` instead of `documentGroups.map`. Ensure that if a model has no highlighted documents after filtering, the entire model section (including the model name heading) is not rendered (the filter already handles this by returning `null` and filtering it out).
        *   `[âœ…]` 38.k.vii. Handle edge cases: the `isDocumentHighlighted` function already handles `undefined`/`null` `focusedStageDocument` (returns `false`), and the `useMemo` handles missing `session`/`activeStage` (returns empty array). No additional edge case handling needed beyond what the shared utility provides.
    *   `[âœ…]` 38.l. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.j and ensure they now pass. Also verify that existing `SessionContributionsDisplayCard` tests still pass (update any tests that expect all documents to be rendered to account for the new filtering behavior - they should mock `focusedStageDocument` appropriately).
    *   `[âœ…]` 38.m. `[LINT]` Run the linter for all modified files and resolve any warnings or errors.
        *   `[âœ…]` 38.m.i. Lint `packages/utils/src/dialecticUtils.ts` and `packages/utils/src/dialecticUtils.test.ts`.
        *   `[âœ…]` 38.m.ii. Lint `packages/utils/src/index.ts`.
        *   `[âœ…]` 38.m.iii. Lint `apps/web/src/components/dialectic/StageRunChecklist.tsx` and `apps/web/src/components/dialectic/StageRunChecklist.test.tsx`.
        *   `[âœ…]` 38.m.iv. Lint `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`.
    *   `[âœ…]` 38.n. `[CRITERIA]` All requirements are met: (1) Shared utility functions `buildFocusedDocumentKey` and `isDocumentHighlighted` are in `@paynless/utils` package and exported, (2) `StageRunChecklist` uses the shared utility functions instead of private implementation, (3) `SessionContributionsDisplayCard` filters documents using the shared utility function, (4) Only highlighted documents are rendered in `SessionContributionsDisplayCard`, (5) All tests pass, including utility function tests, `StageRunChecklist` tests, and `SessionContributionsDisplayCard` tests, (6) All files are lint-clean, (7) DRY compliance is achieved - highlighting logic exists in one place and is reused by all consumers.
    
*   `[âœ…]` 39. **`[UI]` Verify GeneratedContributionCard Only Renders Documents That Are Highlighted in StageRunChecklist**
    *   `[âœ…]` 39.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` already checks `focusedDocument` before rendering document content (line 345: `{focusedDocument && isValidMarkdownDocument ? (...)`). However, it should verify that the `focusedDocument` check aligns with the highlighting logic used in `StageRunChecklist`. The component uses `selectFocusedStageDocument` (line 88) which queries `focusedStageDocument` from the store using the same `focusKey` pattern (`${sessionId}:${stageSlug}:${modelId}`). The component should ensure that document content is only rendered when the document is actually highlighted in the checklist (i.e., when `focusedDocument` exists and matches the highlighting criteria). Currently, the component may render document content if `focusedDocument` exists but the document might not be highlighted in the checklist (edge case scenario). The shared utility function `isDocumentHighlighted` from `@paynless/utils` (created in step 38) provides the exact highlighting logic used by `StageRunChecklist`. The fix requires: (1) importing `isDocumentHighlighted` from `@paynless/utils`, (2) updating the `focusedDocument` check (line 345) to also verify the document is highlighted using the shared utility, (3) ensuring that if `focusedDocument` exists but the document is not actually highlighted (mismatch in `focusedStageDocumentMap`), the content should not render, (4) using the same highlighting validation logic as `StageRunChecklist` and `SessionContributionsDisplayCard` to maintain consistency.
    *   `[âœ…]` 39.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write tests that verify the component only renders document content when the document is highlighted in StageRunChecklist using the shared utility.
        *   `[âœ…]` 39.b.i. Create a test case that mocks store state where `selectFocusedStageDocument` returns a `focusedDocument` (e.g., `{ documentKey: 'business_case' }`), but `focusedStageDocumentMap` does NOT have an entry for that document (the document is not highlighted). Use the same `focusKey` format (`${sessionId}:${stageSlug}:${modelId}`) as the shared utility. Assert that the document content section (the div starting at line 346) is NOT rendered, and only the "Select a document to view its content and provide feedback." message is shown.
        *   `[âœ…]` 39.b.ii. Create a test case that mocks store state where `focusedStageDocumentMap` has an entry for a document (e.g., `focusedStageDocumentMap['session1:thesis:model1'] = { modelId: 'model1', documentKey: 'business_case' }`), and `selectFocusedStageDocument` returns a matching `focusedDocument`, and assert that the document content section IS rendered.
        *   `[âœ…]` 39.b.iii. Create a test case where `selectFocusedStageDocument` returns a `focusedDocument`, but `focusedStageDocumentMap` has an entry with a different `documentKey` (mismatch), and assert that the document content is NOT rendered. This test verifies that the component validates the document is actually highlighted using the shared utility, not just that `focusedDocument` exists.
        *   `[âœ…]` 39.b.iv. Create a test case where `focusedDocument` is `null`, and assert that no document content is rendered (only the "Select a document" message). This test should pass initially but must continue to pass after any fixes.
        *   `[âœ…]` 39.b.v. These tests must fail if the component currently renders document content when `focusedDocument` exists but the document is not highlighted in the checklist.
    *   `[âœ…]` 39.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/GeneratedContributionCard.tsx`, add explicit validation that the focused document is actually highlighted in the checklist before rendering content using the shared utility.
        *   `[âœ…]` 39.c.i. Add import at the top of the file: `import { isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 39.c.ii. Add type import if not already present: `import type { FocusedStageDocumentState } from '@paynless/types';` (verify it's already imported or add it).
        *   `[âœ…]` 39.c.iii. Update the condition at line 345 from `{focusedDocument && isValidMarkdownDocument ? (` to `{focusedDocument && isValidMarkdownDocument && isDocumentHighlighted(sessionId ?? '', stageSlug ?? '', modelId, focusedDocument.documentKey, focusedStageDocumentMap) ? (`. This ensures that document content is only rendered when: (1) `focusedDocument` exists, (2) the document is a valid markdown document, AND (3) the document is actually highlighted in the checklist (verified using the shared utility function).
        *   `[âœ…]` 39.c.iv. Handle edge cases: the `isDocumentHighlighted` function from `@paynless/utils` already handles `undefined`/`null` `focusedStageDocumentMap` (returns `false`), and handles missing `sessionId`/`stageSlug`/`modelId` (returns `false`). The `?? ''` fallbacks ensure strings are passed even if values are null/undefined. No additional edge case handling needed beyond what the shared utility provides.
        *   `[âœ…]` 39.c.v. Ensure the existing logic for displaying "Select a document to view its content and provide feedback." (lines 469-473) still works correctly when no document is highlighted.
    *   `[âœ…]` 39.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 39.b and ensure they now pass. Also verify that existing `GeneratedContributionCard` tests still pass (update any tests that expect document content to be rendered to account for the new highlighting validation - they should mock `focusedStageDocumentMap` appropriately).
    *   `[âœ…]` 39.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write a test that verifies the component correctly uses the shared utility function from `@paynless/utils`.
        *   `[âœ…]` 39.e.i. Create a test case that verifies `GeneratedContributionCard` correctly uses `isDocumentHighlighted` from `@paynless/utils` by mocking the import and verifying it's called with the correct parameters (`sessionId`, `stageSlug`, `modelId`, `documentKey`, `focusedStageDocumentMap`) when checking if document content should be rendered. Alternatively, verify the behavior matches the shared utility by testing with known inputs that should pass/fail according to the utility's logic.
        *   `[âœ…]` 39.e.ii. Create a test case that verifies the component's highlighting behavior matches `StageRunChecklist` behavior when using the same `focusedStageDocumentMap` state. This ensures consistency across all three components (`StageRunChecklist`, `SessionContributionsDisplayCard`, and `GeneratedContributionCard`).
        *   `[âœ…]` 39.e.iii. This test ensures the highlighting logic is consistent between `GeneratedContributionCard` and `StageRunChecklist` by verifying both use the same shared utility function.
    *   `[âœ…]` 39.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 39.e and ensure it passes. This verifies that `GeneratedContributionCard` correctly integrates with the shared utility function.
    *   `[âœ…]` 39.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` and `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx` and resolve any warnings or errors.
    *   `[âœ…]` 39.h. `[CRITERIA]` All requirements are met: (1) `GeneratedContributionCard` imports and uses `isDocumentHighlighted` from `@paynless/utils` (shared utility from step 38), (2) Document content is only rendered when the document is actually highlighted in the checklist (verified using the shared utility), (3) All three components (`StageRunChecklist`, `SessionContributionsDisplayCard`, and `GeneratedContributionCard`) use the same shared utility function, ensuring consistent highlighting behavior, (4) All tests pass, including tests that verify the component uses the shared utility correctly, (5) The file is lint-clean, (6) DRY compliance is maintained - no duplicate highlighting logic exists.

*   `[âœ…]` 40. **`[BE]` Fix Path Construction to Require ALL Required Values and Remove Fallback Chain**
    *   `[âœ…]` 40.a. `[DEPS]` The `constructStoragePath` function in `supabase/functions/_shared/utils/path_constructor.ts` (line 214) uses a fallback chain `documentKey ?? contributionType ?? fileType` to determine `effectiveContributionType` for document file types. This violates project coding standards that forbid defaults, fallbacks, or silent healing. For document file types (FileType.business_case, FileType.feature_spec, etc.), the function MUST require ALL required values to be present and non-empty, and MUST throw an error immediately if ANY required value is missing, undefined, null, or empty string. The function requires the following values for document file types: (1) `projectId: string` (required in PathContext interface), (2) `fileType: FileType` (required in PathContext interface), (3) `sessionId: string` (required for `stageRootPath` construction on line 80), (4) `iteration: number` (required for `stageRootPath` construction on line 80), (5) `stageSlug: string` (required for `stageRootPath` construction on line 80 via `mappedStageDir`), (6) `modelSlug: string` (required for document file types, validated on line 218), (7) `attemptCount: number` (required for document file types, validated on line 218), (8) `documentKey: string` (required for document file types, currently missing validation). All documentKey values are members of FileType enum. The function is called by `FileManagerService.uploadAndRegisterFile` which receives `pathContext` from `executeModelCallAndSave` (step 41 will fix the caller to provide ALL required values). The fix requires: (1) defining a `DocumentKey` type that represents FileType values that are valid documentKey values, (2) creating an `isDocumentKey` type guard to check if a FileType is a DocumentKey, (3) validating that ALL required values are present and non-empty for document file types BEFORE any path construction logic, (4) throwing descriptive errors immediately if ANY validation fails, (5) removing the fallback chain and using `documentKey` directly for document file types, (6) ensuring error messages clearly identify which file type failed and which specific value is missing or invalid.
    *   `[âœ…]` 40.b. `[TYPES]` In `supabase/functions/_shared/types/file_manager.types.ts`, define `DocumentKey` type as a union of FileType enum members that are valid documentKey values. Add the type definition after `ModelContributionFileTypes` (around line 165). The type should include all FileType values that are documentKeys: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`, `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `non_functional_requirements`, `dependency_map`, `comparison_vector`, `product_requirements`, `system_architecture`, `tech_stack`, `technical_requirements`, `master_plan`, `milestone_schema`, `updated_master_plan`, `actionable_checklist`, `advisor_recommendations`. Export the type so it can be imported by type guard files.
    *   `[âœ…]` 40.c. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.test.ts`, write tests that verify `isDocumentKey` type guard correctly identifies DocumentKey file types.
        *   `[âœ…]` 40.c.i. Create test cases that assert `isDocumentKey` returns `true` for all DocumentKey file types (e.g., `FileType.business_case`, `FileType.feature_spec`, `FileType.technical_approach`, etc.).
        *   `[âœ…]` 40.c.ii. Create test cases that assert `isDocumentKey` returns `false` for non-DocumentKey file types (e.g., `FileType.HeaderContext`, `FileType.ModelContributionRawJson`, `FileType.PairwiseSynthesisChunk`, `FileType.ReducedSynthesis`, `FileType.Synthesis`, `FileType.header_context_pairwise`, `FileType.SynthesisHeaderContext`, `FileType.synthesis_pairwise_business_case`, `FileType.synthesis_document_business_case`, etc.). These tests must fail because the type guard doesn't exist yet.
    *   `[âœ…]` 40.d. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, implement the `isDocumentKey` type guard function.
        *   `[âœ…]` 40.d.i. Import `DocumentKey` type from `../../types/file_manager.types.ts`.
        *   `[âœ…]` 40.d.ii. Create a compile-time enforced map `DOCUMENT_KEY_MAP: { [K in DocumentKey]: true }` that includes all DocumentKey file types, following the same pattern as `MODEL_CONTRIBUTION_FILE_TYPES_MAP` and `OUTPUT_TYPES_MAP` in the same file.
        *   `[âœ…]` 40.d.iii. Export the function `isDocumentKey(value: FileType): value is DocumentKey` that checks if the value exists in `DOCUMENT_KEY_MAP` using `Object.prototype.hasOwnProperty.call(DOCUMENT_KEY_MAP, value)`, following the same pattern as `isModelContributionFileType` and `isOutputType` in the same file.
    *   `[âœ…]` 40.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 40.c and ensure they now pass. Also verify that existing type guard tests still pass.
    *   `[âœ…]` 40.f. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/path_constructor.test.ts`, write tests that verify `constructStoragePath` requires ALL required values for document file types and throws errors when ANY value is missing.
        *   `[âœ…]` 40.f.i. Create a test case that calls `constructStoragePath` with a document file type (e.g., `FileType.business_case`) and ALL required values present and non-empty: `projectId: 'project-123'`, `fileType: FileType.business_case`, `sessionId: 'session-123'`, `iteration: 1`, `stageSlug: 'thesis'`, `modelSlug: 'claude-opus'`, `attemptCount: 0`, `documentKey: 'business_case'`. Assert that the function returns a valid path with the `documentKey` in the filename (e.g., `claude-opus_0_business_case.md`). This test should pass after implementation.
        *   `[âœ…]` 40.f.ii. Create test cases that call `constructStoragePath` with a document file type and each required value missing individually: (a) `documentKey: undefined`, (b) `documentKey: null`, (c) `documentKey: ''` (empty string), (d) `sessionId: undefined`, (e) `iteration: undefined`, (f) `stageSlug: undefined`, (g) `modelSlug: undefined`, (h) `attemptCount: undefined`, (i) `projectId: undefined`. Each test must assert that the function throws an error with a message indicating which specific value is required and missing. These tests must fail because the function currently does not validate all required values.
        *   `[âœ…]` 40.f.iii. Create a test case that calls `constructStoragePath` with a non-document file type (e.g., `FileType.HeaderContext`) and `documentKey: undefined`, and assert that the function does NOT throw an error (non-document file types may not require `documentKey`). This test should pass if non-document file types are handled separately.
        *   `[âœ…]` 40.f.iv. Create a test case that verifies `constructStoragePath` requires ALL required values for document file types by testing the behavior: (1) Call `constructStoragePath` with a representative document file type (e.g., `FileType.business_case`) and ALL required values present, assert it succeeds, (2) Call `constructStoragePath` with the same document file type but missing `documentKey`, assert it throws an error indicating `documentKey (string, non-empty)` is required, (3) Call `constructStoragePath` with a non-document file type (e.g., `FileType.HeaderContext`) and missing `documentKey`, assert it does NOT throw an error (proving document file types are handled differently). This test verifies the behavior without enumerating or redefining which file types are document types - it tests that the function correctly distinguishes document file types from non-document file types and enforces the requirement.
    *   `[âœ…]` 40.g. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/path_constructor.ts`, remove the fallback chain and require ALL required values for document file types.
        *   `[âœ…]` 40.g.i. Import `isDocumentKey` from `../type-guards/type_guards.file_manager.ts` at the top of the file.
        *   `[âœ…]` 40.g.ii. At the start of the function (after destructuring `context` on line 70), before any path construction logic, add comprehensive validation for document file types. Use `isDocumentKey(fileType)` to check if the file type requires documentKey. If it is a document file type, validate ALL required values in a single validation block: (1) `projectId` must exist, be a string, and be non-empty, (2) `fileType` must exist (already validated by TypeScript), (3) `sessionId` must exist, be a string, and be non-empty, (4) `iteration` must exist and be a number, (5) `stageSlug` must exist, be a string, and be non-empty, (6) `modelSlug` must exist, be a string, and be non-empty, (7) `attemptCount` must exist and be a number, (8) `documentKey` must exist, be a string, and be non-empty. If ANY validation fails, throw an error immediately: `constructStoragePath requires all of the following values for document file type '${fileType}': projectId (string, non-empty), sessionId (string, non-empty), iteration (number), stageSlug (string, non-empty), modelSlug (string, non-empty), attemptCount (number), documentKey (string, non-empty). Missing or invalid: [list of missing/invalid values]`. This ensures ALL required values are validated BEFORE any path construction logic executes.
        *   `[âœ…]` 40.g.iii. For document file types, remove the fallback chain on line 214: replace `const effectiveContributionType = documentKey ?? contributionType ?? fileType;` with conditional logic that uses `documentKey` directly when `isDocumentKey(fileType)` is true, and keeps the fallback chain for non-document file types. For document file types, `documentKey` is now guaranteed to be present and non-empty after validation, so no fallback is needed. For non-document file types, preserve the existing fallback chain behavior.
        *   `[âœ…]` 40.g.iv. Update the `baseFileName` construction (line 273) to use `documentKey` directly for document file types: in the default case, check if `isDocumentKey(fileType)` and if so, use `${modelSlugSanitized}_${attemptCount}_${sanitizeForPath(documentKey)}` instead of `${modelSlugSanitized}_${attemptCount}_${contributionTypeSanitized}`. This ensures the filename uses the actual document key (e.g., `business_case`) instead of the fallback value (e.g., `thesis`). For non-document file types, preserve the existing logic.
        *   `[âœ…]` 40.g.v. Replace the inline `isDocument` array check (lines 293-299) with `isDocumentKey(fileType)`. Update the `storagePath` logic (lines 301-313) to use `isDocumentKey(fileType)` instead of `isDocument || documentKey`. The check `isDocumentKey(fileType) || documentKey` on line 308 should continue to work correctly, but now `documentKey` will always be present for document file types after validation.
        *   `[âœ…]` 40.g.vi. Ensure non-document file types (e.g., `FileType.HeaderContext`, `FileType.PairwiseSynthesisChunk`, `FileType.ReducedSynthesis`, `FileType.antithesis`) continue to work correctly with their existing logic. These file types may use `contributionType` or other fields, and should continue to use the fallback chain for `effectiveContributionType` determination.
    *   `[âœ…]` 40.h. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 40.f and ensure they now pass. Also verify that existing `path_constructor` tests still pass (update any tests that rely on the fallback chain to provide `documentKey` for document file types).
    *   `[âœ…]` 40.i. `[LINT]` Run the linter for all modified files (`supabase/functions/_shared/types/file_manager.types.ts`, `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.test.ts`, `supabase/functions/_shared/utils/path_constructor.ts`, `supabase/functions/_shared/utils/path_constructor.test.ts`) and resolve any warnings or errors.

*   `[âœ…]` 41. **`[BE]` Fix executeModelCallAndSave to Pass ALL Required Values to Path Context and Use Correct document_key in Notifications**
    *   `[âœ…]` 41.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` constructs `pathContext` for file uploads (lines 1074-1086) but does not include `documentKey` from `job.payload.document_key`, even though planners set this field (per checklist items 11-16). Additionally, the function must ensure ALL required values for `pathContext` are present and validated BEFORE constructing the `pathContext` object. The `constructStoragePath` function (fixed in step 40) requires the following values for document file types: `projectId`, `fileType`, `sessionId`, `iteration`, `stageSlug`, `modelSlug`, `attemptCount`, `documentKey`. The function currently extracts: `projectId: job.payload.projectId` (line 1076), `sessionId` (line 1078), `iteration: iterationNumber` (line 1079), `modelSlug: providerDetails.api_identifier` (line 1080), `attemptCount: job.attempt_count` (line 1081), `...restOfCanonicalPathParams` (line 1082, which includes `stageSlug` from `canonicalPathParams`). However, `documentKey` is NOT included, and there is NO validation that ALL required values are present before constructing `pathContext`. Additionally, the `document_completed` notification (line 1284) uses `String(output_type)` as `document_key`, but should use `job.payload.document_key` if present, as this matches the actual document key from the recipe step that the frontend expects. The fix requires: (1) validating that `job.payload.canonicalPathParams` exists and has `stageSlug` (required for `pathContext.stageSlug`), (2) validating that `job.payload.projectId` exists and is non-empty, (3) validating that `job.payload.sessionId` exists and is non-empty, (4) validating that `job.payload.iterationNumber` exists and is a number, (5) validating that `job.attempt_count` exists and is a number, (6) validating that `providerDetails.api_identifier` exists and is non-empty (for `modelSlug`), (7) validating that `job.payload.document_key` is present and non-empty for EXECUTE jobs that output documents, (8) adding `documentKey: job.payload.document_key` to `pathContext` (after line 1082, before `contributionType`), (9) updating the `document_completed` notification (line 1284) to use `job.payload.document_key` instead of `String(output_type)`, (10) ensuring ALL validation errors clearly identify which specific value is missing or invalid.
    *   `[âœ…]` 41.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.pathContext.test.ts`, write tests that verify `executeModelCallAndSave` validates ALL required values and passes them to path context, and uses correct `document_key` in notifications.
        *   `[âœ…]` 41.b.i. Create a test case that mocks a job payload with ALL required values present: `document_key: 'business_case'`, `output_type: 'business_case'`, `projectId: 'project-123'`, `sessionId: 'session-123'`, `iterationNumber: 1`, `canonicalPathParams: { stageSlug: 'thesis', ... }`, `attempt_count: 0`, and mocks `providerDetails.api_identifier: 'claude-opus'`. Mock `fileManager.uploadAndRegisterFile` to capture the `pathContext` argument. Assert that `pathContext` contains ALL required values: `documentKey === 'business_case'`, `projectId === 'project-123'`, `sessionId === 'session-123'`, `iteration === 1`, `stageSlug === 'thesis'`, `modelSlug === 'claude-opus'`, `attemptCount === 0`. This test must fail because `documentKey` is not currently passed to `pathContext`.
        *   `[âœ…]` 41.b.ii. Create a test case that mocks a job payload with `document_key: 'feature_spec'` and verifies that the `document_completed` notification uses `document_key: 'feature_spec'` (not `String(output_type)`). Mock `notificationService.sendDocumentCentricNotification` and assert the notification payload has `document_key: 'feature_spec'`. This test must fail because the notification currently uses `String(output_type)`.
        *   `[âœ…]` 41.b.iii. Create test cases that mock an EXECUTE job with `output_type: 'business_case'` (a document file type) but each required value missing individually: (a) `document_key: undefined`, (b) `document_key: ''` (empty string), (c) `projectId: undefined`, (d) `sessionId: undefined`, (e) `iterationNumber: undefined`, (f) `canonicalPathParams: undefined`, (g) `canonicalPathParams.stageSlug: undefined`, (h) `attempt_count: undefined`, (i) `providerDetails.api_identifier: undefined`. Each test must assert that `executeModelCallAndSave` throws an error indicating which specific value is required and missing. These tests must fail because the function currently does not validate all required values.
        *   `[âœ…]` 41.b.iv. Create a test case that mocks an EXECUTE job with `output_type: 'HeaderContext'` (a non-document file type) and `document_key: undefined`, and assert that `executeModelCallAndSave` does NOT throw an error (non-document file types may not require `document_key`). This test should pass if non-document file types are handled separately.
    *   `[âœ…]` 41.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, validate ALL required values and add `documentKey` to path context, and fix notification to use correct `document_key`.
        *   `[âœ…]` 41.c.i. Before constructing `uploadContext` (line 1074), add comprehensive validation logic that checks if `output_type` is a document file type (using the same list as `path_constructor.ts` lines 293-299). If it is a document file type, validate ALL required values in a single validation block: (1) `job.payload.projectId` must exist, be a string, and be non-empty, (2) `job.payload.sessionId` must exist, be a string, and be non-empty, (3) `job.payload.iterationNumber` must exist and be a number, (4) `job.payload.canonicalPathParams` must exist and be an object, (5) `job.payload.canonicalPathParams.stageSlug` must exist, be a string, and be non-empty, (6) `job.attempt_count` must exist and be a number, (7) `providerDetails.api_identifier` must exist, be a string, and be non-empty, (8) `job.payload.document_key` must exist, be a string, and be non-empty. If ANY validation fails, throw an error immediately: `executeModelCallAndSave requires all of the following values for document file type '${output_type}': job.payload.projectId (string, non-empty), job.payload.sessionId (string, non-empty), job.payload.iterationNumber (number), job.payload.canonicalPathParams.stageSlug (string, non-empty), job.attempt_count (number), providerDetails.api_identifier (string, non-empty), job.payload.document_key (string, non-empty). Missing or invalid: [list of missing/invalid values]`. This ensures ALL required values are validated BEFORE constructing `pathContext`.
        *   `[âœ…]` 41.c.ii. In the `pathContext` object (line 1075), add `documentKey: job.payload.document_key` after line 1082 (after `...restOfCanonicalPathParams`, before `contributionType`). This ensures `constructStoragePath` receives the `documentKey` it requires. Verify that ALL other required values are already present: `projectId`, `sessionId`, `iteration`, `stageSlug` (from `restOfCanonicalPathParams`), `modelSlug`, `attemptCount`.
        *   `[âœ…]` 41.c.iii. Update the `document_completed` notification (line 1284) to use `job.payload.document_key` instead of `String(output_type)`: change `document_key: String(output_type)` to `document_key: job.payload.document_key`. This ensures the notification uses the correct document key that matches the frontend's expectations.
        *   `[âœ…]` 41.c.iv. Ensure non-document file types (e.g., `FileType.HeaderContext`) continue to work correctly. If `output_type` is not a document file type, `document_key` may be undefined, and the function should not throw an error for these cases.
    *   `[âœ…]` 41.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 41.b and ensure they now pass. Also verify that existing `executeModelCallAndSave` tests still pass (update any tests that mock job payloads to include `document_key` for document file types).
    *   `[âœ…]` 41.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 42. **`[BE]` Fix processComplexJob to Filter Out Already-Completed Steps from Ready Steps and Validate planner_metadata**
    *   `[âœ…]` 42.a. `[DEPS]` The `processComplexJob` function in `supabase/functions/dialectic-worker/processComplexJob.ts` tracks completed steps by looking up `step_slug` from `planner_metadata.recipe_step_id` in completed child jobs (lines 154-166), and stores them in `completedStepSlugs` Set. However, the function does NOT validate that `planner_metadata` exists and has `recipe_step_id` for completed child jobs. If `planner_metadata` is missing or incomplete, the step cannot be tracked as completed, causing it to be re-planned. Additionally, when building `readySteps` (lines 192-217), the function only checks if predecessors are completed, but does not filter out steps that are already completed themselves. This causes the system to re-plan steps that have already been completed, leading to the recipe restarting from the beginning (e.g., re-planning `build-stage-header` even though documents have already been generated). The fix requires: (1) validating that ALL completed child jobs have `planner_metadata` with `recipe_step_id` present and non-empty, (2) throwing an error immediately if a completed child job is missing `planner_metadata` or `recipe_step_id` (cannot track completion without it), (3) filtering `readySteps` to exclude steps whose `step_slug` is already in `completedStepSlugs`, (4) ensuring this filter is applied after building the `readySteps` array but before planning, (5) verifying that the completion check uses the same `step_slug` matching logic that is used to populate `completedStepSlugs`.
    *   `[âœ…]` 42.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.test.ts`, write tests that verify `processComplexJob` validates `planner_metadata` and filters out already-completed steps from ready steps.
        *   `[âœ…]` 42.b.i. Create a test case that mocks a PLAN job with multiple recipe steps, where one step (`build-stage-header`) has a completed child job (status `'completed'` with `planner_metadata.recipe_step_id` matching the step's `id`), and another step (`generate-business-case`) has no completed children. Assert that `processComplexJob` only plans the step that is not yet completed (`generate-business-case`), and does NOT plan the already-completed step (`build-stage-header`). This test must fail because the function currently does not filter out completed steps.
        *   `[âœ…]` 42.b.ii. Create a test case that mocks a PLAN job where all steps have completed child jobs, and assert that `processComplexJob` completes the parent job (does not plan any steps, marks parent as `'completed'`). This test should pass if the existing logic (lines 220-229) works correctly, but must continue to pass after the filter is added.
        *   `[âœ…]` 42.b.iii. Create a test case that mocks a PLAN job where a step has a completed child job but the step's `step_slug` does not match the `completedStepSlugs` Set (simulating a mismatch in step tracking), and assert that the step is still planned (the filter only excludes steps that are correctly tracked as completed). This test verifies the filter uses the correct matching logic.
        *   `[âœ…]` 42.b.iv. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: undefined`, and assert that `processComplexJob` throws an error indicating `planner_metadata` is required for completed child jobs. This test must fail because the function currently does not validate `planner_metadata` presence.
        *   `[âœ…]` 42.b.v. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: {}` (empty object, missing `recipe_step_id`), and assert that `processComplexJob` throws an error indicating `planner_metadata.recipe_step_id` is required for completed child jobs. This test must fail because the function currently does not validate `recipe_step_id` presence.
        *   `[âœ…]` 42.b.vi. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: { recipe_step_id: '' }` (empty string), and assert that `processComplexJob` throws an error indicating `planner_metadata.recipe_step_id` must be non-empty. This test must fail because the function currently does not validate `recipe_step_id` is non-empty.
    *   `[âœ…]` 42.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/processComplexJob.ts`, validate `planner_metadata` and filter out already-completed steps from `readySteps` before planning.
        *   `[âœ…]` 42.c.i. In the loop that tracks completed steps (lines 156-166), BEFORE attempting to look up `step_slug`, add validation logic that checks if `planner_metadata` exists and has `recipe_step_id`. For each completed child job, validate: (1) `child.payload` must exist and be an object, (2) `child.payload.planner_metadata` must exist and be an object, (3) `child.payload.planner_metadata.recipe_step_id` must exist, be a string, and be non-empty. If ANY validation fails for a completed child job, throw an error immediately: `processComplexJob cannot track completion for child job ${child.id} because planner_metadata.recipe_step_id is missing or invalid. Completed child jobs MUST have planner_metadata with a non-empty recipe_step_id to enable step completion tracking.`. This ensures ALL completed child jobs have the required metadata for tracking completion.
        *   `[âœ…]` 42.c.ii. After building the `readySteps` array (line 217), add a filter that excludes steps whose `step_slug` is in the `completedStepSlugs` Set. Replace `readySteps` with the filtered array: `const filteredReadySteps = readySteps.filter(step => !completedStepSlugs.has(step.step_slug));` and use `filteredReadySteps` for all subsequent operations (planning, logging, etc.).
        *   `[âœ…]` 42.c.iii. Update the check for "no ready steps remain" (line 220) to use the filtered array: change `if (readySteps.length === 0)` to `if (filteredReadySteps.length === 0)`. This ensures the function correctly detects when all remaining steps are completed.
        *   `[âœ…]` 42.c.iv. Update the sorting logic (line 233) to use the filtered array: change `readySteps.sort(...)` to `filteredReadySteps.sort(...)`. This ensures only non-completed steps are sorted and planned.
        *   `[âœ…]` 42.c.v. Update the logging (lines 244-249) to use the filtered array: change all references to `readySteps` to `filteredReadySteps` in the logging statements. This ensures logs accurately reflect which steps are being planned.
        *   `[âœ…]` 42.c.vi. Update the planning loop (line 281) to use the filtered array: change `readySteps.map(...)` to `filteredReadySteps.map(...)`. This ensures only non-completed steps are planned.
        *   `[âœ…]` 42.c.vii. Update the critical validation check (line 253) to use the filtered array: change `if (completedStepSlugs.size === 0 && readySteps.length > 1)` to `if (completedStepSlugs.size === 0 && filteredReadySteps.length > 1)`. This ensures the validation check uses the correct array.
    *   `[âœ…]` 42.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 42.b and ensure they now pass. Also verify that existing `processComplexJob` tests still pass (the filter should not break existing functionality, only prevent re-planning of completed steps).
    *   `[âœ…]` 42.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processComplexJob.ts` and `supabase/functions/dialectic-worker/processComplexJob.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 42.f. `[CRITERIA]` All requirements are met: (1) `processComplexJob` validates that ALL completed child jobs have `planner_metadata` with `recipe_step_id` present and non-empty, (2) `processComplexJob` throws an error immediately if a completed child job is missing `planner_metadata` or `recipe_step_id`, (3) `processComplexJob` filters out already-completed steps from `readySteps` before planning, (4) The filter uses the same `step_slug` matching logic that populates `completedStepSlugs`, (5) All references to `readySteps` after filtering use the filtered array, (6) The function does not re-plan steps that have already been completed, (7) All tests pass, including tests that verify `planner_metadata` validation and completed steps are excluded from planning, (8) The file is lint-clean.

*   `[ ]` 43. **`[TEST-INT]` Fix Integration Test to Verify Complete End-to-End Flow with Correct File Paths and Step Tracking**
    *   `[ ]` 43.a. `[DEPS]` After all fixes are complete (steps 40-42), write an integration test that verifies the complete end-to-end flow: (1) PLAN job creates EXECUTE jobs with `document_key` in payload, (2) EXECUTE jobs save contributions with correct file paths using `documentKey`, (3) `document_completed` notifications use correct `document_key`, (4) Frontend receives notifications and updates state correctly, (5) PLAN job does not re-plan already-completed steps when woken up with `pending_next_step` status. The test file `supabase/integration_tests/services/planner_output_type.integration.test.ts` or a new integration test file should verify this flow.
    *   `[ ]` 43.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the complete end-to-end flow with correct file paths and step tracking.
        *   `[ ]` 43.b.i. Create a test case that sets up a PLAN job, calls a planner function (e.g., `planPerSourceDocument`) with a recipe step that has `outputs_required.documents[0].document_key: 'business_case'`, and creates child EXECUTE jobs. Assert that the created EXECUTE job payloads have `document_key: 'business_case'` set correctly.
        *   `[ ]` 43.b.ii. Execute one of the EXECUTE jobs by calling `executeModelCallAndSave` with the job payload, and assert that the file is saved with the correct path containing `documentKey: 'business_case'` (e.g., filename should be `model_0_business_case.md`, not `model_0_thesis.md`). Verify the file path is constructed correctly by checking the `pathContext` passed to `fileManager.uploadAndRegisterFile`.
        *   `[ ]` 43.b.iii. Assert that the `document_completed` notification sent by `executeModelCallAndSave` uses `document_key: 'business_case'` (not `String(output_type)`). Mock or capture the notification and verify the payload.
        *   `[ ]` 43.b.iv. After the EXECUTE job completes, simulate the database trigger setting the parent PLAN job to `pending_next_step` status, and call `processComplexJob` again. Assert that `processComplexJob` does NOT re-plan the already-completed step, and only plans remaining steps that are not yet completed.
        *   `[ ]` 43.b.v. This test must fail initially if any of the fixes (steps 40-42) are not complete, but should pass after all fixes are implemented.
    *   `[ ]` 43.c. `[TEST-INT]` **GREEN**: Re-run the test from step 43.b and ensure it now passes. This verifies that the complete end-to-end flow works correctly: planners set `document_key`, file paths use correct `documentKey`, notifications use correct `document_key`, and completed steps are not re-planned.
    *   `[ ]` 43.d. `[LINT]` Run the linter for the integration test file and resolve any warnings or errors.
    *   `[ ]` 43.e. `[CRITERIA]` The integration test verifies: (1) EXECUTE jobs have `document_key` in payload, (2) File paths are constructed with correct `documentKey` (not fallback values), (3) Notifications use correct `document_key` matching the recipe step, (4) Completed steps are not re-planned when parent job is woken up, (5) All tests pass, proving the complete end-to-end flow works correctly.

*   `[âœ…]` 44. **`[COMMIT]` Commit All Changes with Comprehensive Message**
    *   `[âœ…]` 44.a. `[DEPS]` After all work is complete, tested, and linted, commit all changes with a comprehensive commit message that describes all fixes.
    *   `[âœ…]` 44.b. `[COMMIT]` Commit message: "fix: remove fallback chain from path construction and require documentKey for document file types - Remove fallback chain (documentKey ?? contributionType ?? fileType) from path_constructor.ts - Require documentKey to be present and non-empty for all document file types, throw error immediately if missing - Fix executeModelCallAndSave to pass documentKey to pathContext from job.payload.document_key - Fix executeModelCallAndSave to use job.payload.document_key in document_completed notifications instead of String(output_type) - Fix processComplexJob to filter out already-completed steps from readySteps before planning - Add comprehensive unit tests for all fixes verifying no fallbacks, defaults, or silent healing - Add integration test verifying complete end-to-end flow with correct file paths and step tracking - Resolves invalid file paths (*_thesis.md instead of proper document keys), recipe restart issues, and frontend state update problems"

*   `[âœ…]` 44.1. **`[DEPS]` Ensure All Planners Set document_relationships.source_group for Source Document Identification**
    *   `[âœ…]` 44.1.a. `[DEPS]` **ROOT CAUSE ANALYSIS**: The `extractSourceDocumentIdentifier` utility function in `supabase/functions/_shared/utils/source_document_identifier.ts` requires `document_relationships.source_group` to be present and non-empty for all job payloads. This identifier is used by `processComplexJob` to track completed source documents for selective re-planning (step 45). However, research revealed that two planners do NOT set `source_group`: (1) `planAllToOne` in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` creates EXECUTE job payloads (lines 124-156 for PLAN jobs, lines 250-279 for EXECUTE jobs) but does NOT include `document_relationships` at all, (2) `planPerModel` in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` creates EXECUTE job payloads (line 243-268) with `document_relationships: { synthesis_group: ... }` but does NOT include `source_group`. This means that when `processComplexJob` attempts to extract source document identifiers from child jobs created by these planners, `extractSourceDocumentIdentifier` will throw errors because `source_group` is missing. **THE FIX**: Both planners must be updated to set `document_relationships.source_group` in all EXECUTE job payloads they create. The `source_group` value should uniquely identify the source document(s) that the job processes, enabling `processComplexJob` to track which source documents have completed jobs. This ensures consistent source document identification across all planners and enables selective re-planning to work correctly.

*   `[âœ…]` 44.2. **`[BE]` Fix planAllToOne to Set document_relationships.source_group in All EXECUTE Job Payloads**
    *   `[âœ…]` 44.2.a. `[DEPS]` The `planAllToOne` planner in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` creates EXECUTE job payloads for both PLAN recipe steps (lines 124-156) and EXECUTE recipe steps (lines 250-279) but does NOT include `document_relationships` in either payload. The planner processes all source documents together (uses `sourceDocs[0]` as `anchorDocument` and includes all `documentIds` in inputs), so the `source_group` should identify the anchor document or a unique identifier for the combined source documents. **THE FIX**: Add `document_relationships: { source_group: anchorDocument.id }` to both EXECUTE job payloads (PLAN job payload at line 148, EXECUTE job payload at line 273). This ensures `extractSourceDocumentIdentifier` can extract the identifier from child jobs created by this planner.
    *   `[âœ…]` 44.2.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` (create if it doesn't exist), write tests that verify `planAllToOne` sets `document_relationships.source_group` in all EXECUTE job payloads.
        *   `[âœ…]` 44.2.b.i. Create a test case that calls `planAllToOne` with a PLAN recipe step and source documents, and asserts that the created EXECUTE job payload has `document_relationships: { source_group: <anchorDocument.id> }` where `anchorDocument` is the first source document. This test must fail because `planAllToOne` currently does not set `document_relationships`.
        *   `[âœ…]` 44.2.b.ii. Create a test case that calls `planAllToOne` with an EXECUTE recipe step and source documents, and asserts that the created EXECUTE job payload has `document_relationships: { source_group: <anchorDocument.id> }` where `anchorDocument` is the first source document. This test must fail because `planAllToOne` currently does not set `document_relationships`.
        *   `[âœ…]` 44.2.b.iii. Create a test case that verifies `extractSourceDocumentIdentifier` can successfully extract the identifier from a job payload created by `planAllToOne`. Call `planAllToOne` to create a payload, then call `extractSourceDocumentIdentifier` with the payload, and assert it returns the `source_group` value. This test must fail initially but should pass after the fix.
    *   `[âœ…]` 44.2.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, add `document_relationships: { source_group: anchorDocument.id }` to both EXECUTE job payloads.
        *   `[âœ…]` 44.2.c.i. For PLAN recipe steps (line 124-156), add `document_relationships: { source_group: anchorDocument.id }` to the `executePayload` object (after line 147, before `planner_metadata`). This ensures PLAN job payloads include `source_group`.
        *   `[âœ…]` 44.2.c.ii. For EXECUTE recipe steps (line 250-279), add `document_relationships: { source_group: anchorDocument.id }` to the `executePayload` object (after line 272, before `planner_metadata`). This ensures EXECUTE job payloads include `source_group`.
    *   `[âœ…]` 44.2.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 44.2.b and ensure they now pass. Also verify that existing `planAllToOne` tests still pass (the fix should not break existing functionality, only add `source_group`).
    *   `[âœ…]` 44.2.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 44.2.f. `[CRITERIA]` All requirements are met: (1) `planAllToOne` sets `document_relationships: { source_group: anchorDocument.id }` in PLAN job EXECUTE payloads, (2) `planAllToOne` sets `document_relationships: { source_group: anchorDocument.id }` in EXECUTE job payloads, (3) `extractSourceDocumentIdentifier` can successfully extract the identifier from job payloads created by `planAllToOne`, (4) All tests pass, including tests that verify `source_group` is set correctly, (5) The file is lint-clean, (6) Existing functionality is preserved (only `source_group` is added to payloads).

*   `[âœ…]` 44.3. **`[BE]` Fix planPerModel to Replace synthesis_group with source_group in EXECUTE Job Payloads**
    *   `[âœ…]` 44.3.a. `[DEPS]` **ROOT CAUSE ANALYSIS**: The `planPerModel` planner in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` creates EXECUTE job payloads (line 243-268) with `document_relationships: { synthesis_group: synthesisDocIds.join(',') }` (line 202-204, used at line 263). Research revealed: (1) `synthesis_group` is stage-specific naming (bound to "synthesis" stage) when it should be a generic reusable identifier, (2) `synthesis_group` is never read anywhere in the codebase (dead code), (3) All other planners use `source_group` with a single identifier (not a comma-separated list), (4) `planPerModel` already has `inputs.synthesis_ids` containing the comma-separated list of document IDs, making `synthesis_group` redundant, (5) `source_group` is meant to be a single identifier for tracking completion, not a list. **THE FIX**: Replace `synthesis_group` with `source_group` using a single identifier (`anchorDoc.id`), matching the pattern used by all other planners. Change `document_relationships: { synthesis_group: synthesisDocIds.join(',') }` to `document_relationships: { source_group: anchorDoc.id }`. This ensures: (1) consistent naming across all planners (generic `source_group` instead of stage-specific `synthesis_group`), (2) `extractSourceDocumentIdentifier` can extract the identifier from child jobs created by this planner, (3) the identifier is a single value (not a list) matching the expected format, (4) dead code (`synthesis_group`) is removed.
    *   `[âœ…]` 44.3.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` (create if it doesn't exist), write tests that verify `planPerModel` sets `document_relationships.source_group` and removes `synthesis_group` in EXECUTE job payloads.
        *   `[âœ…]` 44.3.b.i. Create a test case that calls `planPerModel` with an EXECUTE recipe step and source documents, and asserts that the created EXECUTE job payload has `document_relationships.source_group` set to the anchor document's ID (the first source document). Also assert that `document_relationships.synthesis_group` is NOT present (removed, not preserved). This test must fail because `planPerModel` currently sets `synthesis_group` instead of `source_group`.
        *   `[âœ…]` 44.3.b.ii. Create a test case that verifies `extractSourceDocumentIdentifier` can successfully extract the identifier from a job payload created by `planPerModel`. Call `planPerModel` to create a payload, then call `extractSourceDocumentIdentifier` with the payload, and assert it returns the `source_group` value. This test must fail initially but should pass after the fix.
    *   `[âœ…]` 44.3.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, replace `synthesis_group` with `source_group` in the `document_relationships` object.
        *   `[âœ…]` 44.3.c.i. Update the `document_relationships` object construction (line 202-204) to replace `synthesis_group` with `source_group`: change `const document_relationships: Record<string, string> = { synthesis_group: synthesisDocIds.join(','), };` to `const document_relationships: Record<string, string> = { source_group: anchorDoc.id, };`. This ensures `source_group` is set with a single identifier (matching all other planners) and removes the dead code `synthesis_group`.
    *   `[âœ…]` 44.3.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 44.3.b and ensure they now pass. Also verify that existing `planPerModel` tests still pass (the fix should not break existing functionality, only add `source_group`).
    *   `[âœ…]` 44.3.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 44.3.f. `[CRITERIA]` All requirements are met: (1) `planPerModel` sets `document_relationships.source_group` in EXECUTE job payloads with a single identifier (`anchorDoc.id`), (2) `planPerModel` removes `document_relationships.synthesis_group` (dead code eliminated, stage-specific naming removed), (3) `extractSourceDocumentIdentifier` can successfully extract the identifier from job payloads created by `planPerModel`, (4) The `source_group` value is a single identifier (not a comma-separated list), matching the pattern used by all other planners, (5) All tests pass, including tests that verify `source_group` is set correctly and `synthesis_group` is removed, (6) The file is lint-clean, (7) Consistent naming is achieved across all planners (all use generic `source_group` instead of stage-specific names).

*   `[âœ…]` 45. **`[BE]` Fix task_isolator.ts - Add Source Document Filtering for Selective Re-Planning**
    *   `[âœ…]` 45.a. `[DEPS]` 
        *   `[âœ…]` 45.a.i. Current function signature: `planComplexStage(dbClient: SupabaseClient<Database>, parentJob: DialecticJobRow & { payload: DialecticPlanJobPayload }, deps: IDialecticJobDeps, recipeStep: DialecticRecipeStep, authToken: string): Promise<DialecticJobRow[]>` in `supabase/functions/dialectic-worker/task_isolator.ts` (line 496)
        *   `[âœ…]` 45.a.ii. Return shape: `Promise<DialecticJobRow[]>` (unchanged)
        *   `[âœ…]` 45.a.iii. New function signature: `planComplexStage(..., completedSourceDocumentIds?: Set<string>): Promise<DialecticJobRow[]>` - adds optional 6th parameter to filter completed source documents
        *   `[âœ…]` 45.a.iv. `extractSourceDocumentIdentifier` in `supabase/functions/_shared/utils/source_document_identifier.ts` provides identifier extraction from `document_relationships?.source_group` which MUST be present and non-empty, throws error if missing (fail loud and hard, no fallbacks)
        *   `[âœ…]` 45.a.v. `findSourceDocuments` in `supabase/functions/dialectic-worker/task_isolator.ts` (lines 241-443) returns ALL source documents matching `inputs_required` rules, no mechanism to filter out completed source documents
        *   `[âœ…]` 45.a.vi. `processComplexJob` in `supabase/functions/dialectic-worker/processComplexJob.ts` (lines 380-386) must pass `completedSourceDocIds ?? undefined` as 6th argument when calling `planComplexStage` for steps with mixed completed + failed jobs
        *   `[âœ…]` 45.a.vii. **ROOT CAUSE ANALYSIS**: When `processComplexJob` identifies steps with MIXED completed + failed jobs, it needs to re-plan only the failed work while preserving completed work. The `planComplexStage` function currently calls `findSourceDocuments` which returns ALL source documents matching the `inputs_required` rules. There is no mechanism to filter out source documents that already have completed jobs.
        *   `[âœ…]` 45.a.viii. **ROOT CAUSE ANALYSIS - DOCUMENT RELATIONSHIPS PRESERVATION**: The `findSourceDocuments` function in `task_isolator.ts` queries BOTH `dialectic_contributions` AND `dialectic_project_resources` tables (lines 285-356 for 'document' type). These two tables store `document_relationships` DIFFERENTLY: (1) `dialectic_contributions` has an explicit `document_relationships` JSONB column that is directly accessible from the row (`row.document_relationships`), (2) `dialectic_project_resources` stores `document_relationships` as a JSON element INSIDE the `resource_description` JSONB column (i.e., `row.resource_description.document_relationships`). The mapper functions transform rows into `SourceDocument` objects: `mapContributionToSourceDocument` (lines 41-46) correctly extracts `document_relationships` from `row.document_relationships`, but `mapResourceToSourceDocument` (lines 48-82) INCORRECTLY sets `document_relationships: null` (line 78) instead of extracting it from `row.resource_description.document_relationships`. This means source documents from `dialectic_project_resources` lose their `document_relationships` data, causing `extractSourceDocumentIdentifier` to fail when trying to extract `source_group` from these documents. **THE FIX**: `mapResourceToSourceDocument` must extract `document_relationships` from `row.resource_description.document_relationships` (if present) and preserve it in the `SourceDocument` object, matching the behavior of `mapContributionToSourceDocument`. Both mapper functions must preserve `document_relationships` so that `extractSourceDocumentIdentifier` can extract `source_group` from source documents of BOTH types.
        *   `[âœ…]` 45.a.ix. **THE FIX**: `planComplexStage` must accept an optional Set of completed source document IDs (passed from `processComplexJob` when `completedSourceDocumentsByStep.has(step.step_slug)` is true). The function must filter the source documents returned by `findSourceDocuments` to exclude any source documents whose identifiers match the completed source document IDs. The source document identifier MUST be extracted from `document_relationships?.source_group` which MUST be present and non-empty. If `source_group` is missing, undefined, null, or empty string, the function MUST throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing. This ensures that when re-planning steps with mixed statuses, only source documents with failed jobs are included in the planning, preserving completed work. **CRITICAL**: Source documents from BOTH `dialectic_contributions` AND `dialectic_project_resources` must have `document_relationships` preserved correctly so that `source_group` can be extracted from BOTH types.
    *   `[âœ…]` 45.b. `[TYPES]` 
        *   `[âœ…]` 45.b.i. `completedSourceDocumentIds?: Set<string>` - optional parameter containing identifiers of completed source documents to exclude from re-planning
        *   `[âœ…]` 45.b.ii. `SourceDocument` from `dialectic.interface.ts` - source documents returned by `findSourceDocuments`, must have `document_relationships?.source_group` present and non-empty
        *   `[âœ…]` 45.b.iii. `extractSourceDocumentIdentifier(input: unknown): string | null` - returns extracted identifier string or null if input is not a record, throws error if `source_group` is missing/invalid
        *   `[âœ…]` 45.b.iv. Filtered `sourceDocuments: SourceDocument[]` - array after filtering out completed source documents, used for the rest of the function
        *   `[âœ…]` 45.b.v. `DialecticContributionRow` from `dialectic.interface.ts` - rows from `dialectic_contributions` table have `document_relationships: Json | null` as a direct column property accessible as `row.document_relationships`
        *   `[âœ…]` 45.b.vi. `DialecticProjectResourceRow` from `dialectic.interface.ts` - rows from `dialectic_project_resources` table have `resource_description: Json | null` which may contain `document_relationships` as a nested property (i.e., `row.resource_description.document_relationships`)
        *   `[âœ…]` 45.b.vii. Both row types must be transformed into `SourceDocument` objects with `document_relationships` preserved, regardless of where it's stored in the source row. The mapper functions must handle both storage locations correctly.
    *   `[âœ…]` 45.c. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/task_isolator.test.ts` (create if it doesn't exist), write tests that prove the source document filtering works correctly.
        *   `[âœ…]` 45.c.i. Create a test case that mocks `findSourceDocuments` to return 3 source documents (doc1, doc2, doc3), and calls `planComplexStage` with a Set of completed source document IDs containing doc1 and doc3. Assert that `planComplexStage` filters the source documents to exclude doc1 and doc3, and only passes doc2 to the planner. This test must fail because `planComplexStage` currently doesn't accept or apply the filter.
        *   `[âœ…]` 45.c.ii. Create a test case that mocks `findSourceDocuments` to return source documents, and calls `planComplexStage` with an empty Set of completed source document IDs. Assert that `planComplexStage` passes ALL source documents to the planner (no filtering when the Set is empty). This test verifies that the filter is optional and doesn't break normal planning flow.
        *   `[âœ…]` 45.c.iii. Create a test case that mocks `findSourceDocuments` to return source documents, and calls `planComplexStage` without the completed source document IDs parameter (undefined). Assert that `planComplexStage` passes ALL source documents to the planner (backward compatibility when parameter is not provided). This test verifies backward compatibility.
        *   `[âœ…]` 45.c.iv. Create a test case that mocks `findSourceDocuments` to return source documents where some have `document_relationships?.source_group` and others have missing, null, or empty `source_group`. Call `planComplexStage` with a Set containing identifiers. Assert that `planComplexStage` throws an error for source documents without valid `source_group` (fail loud and hard, no fallbacks). This test verifies that the function requires `source_group` to be present and non-empty.
        *   `[âœ…]` 45.c.v. Create a test case that mocks `findSourceDocuments` to return source documents, and calls `planComplexStage` with a Set containing identifiers that don't match any source documents. Assert that `planComplexStage` passes ALL source documents to the planner (no false positives in filtering). This test verifies the filter doesn't incorrectly exclude documents.
        *   `[âœ…]` 45.c.vi. Create a test case that mocks `findSourceDocuments` to return source documents from BOTH `dialectic_contributions` (with `document_relationships` as a direct column) AND `dialectic_project_resources` (with `document_relationships` nested in `resource_description`). Call `planComplexStage` with a Set of completed source document IDs. Assert that `planComplexStage` can extract `source_group` from BOTH types of source documents and filter them correctly. This test must fail because `mapResourceToSourceDocument` currently sets `document_relationships: null`, so source documents from `dialectic_project_resources` cannot be filtered.
        *   `[âœ…]` 45.c.vii. Create a test case that mocks a `DialecticProjectResourceRow` with `resource_description: { document_relationships: { source_group: 'test-id' } }`. Call `mapResourceToSourceDocument` with this row and assert that the returned `SourceDocument` has `document_relationships: { source_group: 'test-id' }` (not `null`). This test must fail because `mapResourceToSourceDocument` currently sets `document_relationships: null`.
        *   `[âœ…]` 45.c.viii. Create a test case that mocks a `DialecticContributionRow` with `document_relationships: { source_group: 'test-id' }`. Call `mapContributionToSourceDocument` with this row and assert that the returned `SourceDocument` has `document_relationships: { source_group: 'test-id' }` preserved. This test verifies the existing correct behavior continues to work.
        *   `[âœ…]` 45.c.ix. Create a test case that mocks `findSourceDocuments` to return source documents where some have `document_relationships` from contributions (direct column) and others have `document_relationships` from project_resources (nested in `resource_description`). Call `planComplexStage` with a Set containing identifiers from BOTH types. Assert that `planComplexStage` filters source documents of BOTH types correctly based on their `source_group` values. This test must fail if `mapResourceToSourceDocument` doesn't preserve `document_relationships`.
    *   `[âœ…]` 45.d. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/task_isolator.ts`, add source document filtering to `planComplexStage` to exclude completed source documents when re-planning.
        *   `[âœ…]` 45.d.i. Update the `planComplexStage` function signature (line 496) to accept an optional parameter `completedSourceDocumentIds?: Set<string>`. This parameter will be passed from `processComplexJob` when a step has mixed completed + failed jobs.
        *   `[âœ…]` 45.d.ii. Import and use the shared utility function `extractSourceDocumentIdentifier` from `supabase/functions/_shared/utils/source_document_identifier.ts`. This function extracts the identifier from `document_relationships?.source_group` which MUST be present and non-empty. The function throws an error if `source_group` is missing, undefined, null, or empty string (fail loud and hard, no fallbacks). This ensures consistent identifier extraction across both files.
        *   `[âœ…]` 45.d.iii. After `findSourceDocuments` returns source documents (line 545-550), add filtering logic: if `completedSourceDocumentIds` is provided and is not empty, filter the `sourceDocuments` array to exclude any source document whose identifier (extracted via `extractSourceDocumentIdentifier`) matches an ID in the `completedSourceDocumentIds` Set. Use the filtered array for the rest of the function. This ensures only failed work is re-planned.
        *   `[âœ…]` 45.d.iv. Add logging to track when filtering occurs: log the count of source documents before filtering, the count of completed source document IDs in the filter Set, the count of source documents after filtering, and which source document identifiers were filtered out. This helps diagnose issues in production.
        *   `[âœ…]` 45.d.v. Ensure the filtering logic handles edge cases: (1) Source documents without `document_relationships?.source_group` (missing, null, or empty) MUST throw an error immediately (fail loud and hard, no fallbacks), (2) Source documents with identifiers that don't match the Set are preserved, (3) The filter is only applied when `completedSourceDocumentIds` is provided and non-empty.
        *   `[âœ…]` 45.d.vi. Fix `mapResourceToSourceDocument` function (lines 48-82) to extract and preserve `document_relationships` from `row.resource_description`. Before line 78 where `document_relationships: null` is set, add logic: (1) Check if `row.resource_description` exists and is an object, (2) Check if `row.resource_description.document_relationships` exists, (3) Use `isDocumentRelationships` type guard (imported from `type_guards.ts`) to validate the structure, (4) If valid, set `document_relationships: row.resource_description.document_relationships`, otherwise set `document_relationships: null`. This ensures source documents from `dialectic_project_resources` preserve their `document_relationships` data, matching the behavior of `mapContributionToSourceDocument`.
        *   `[âœ…]` 45.d.vii. Verify that `mapContributionToSourceDocument` (lines 41-46) continues to correctly extract `document_relationships` from `row.document_relationships` (direct column). This function already works correctly, but verify it's not broken by any changes and that it continues to preserve `document_relationships` for contributions.
        *   `[âœ…]` 45.d.viii. Add logging to `mapResourceToSourceDocument` to track when `document_relationships` is extracted from `resource_description`: log when `document_relationships` is found and preserved, and log when `resource_description` is missing or doesn't contain `document_relationships`. This helps diagnose issues in production and verify that both mapper functions are preserving `document_relationships` correctly.
    *   `[âœ…]` 45.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 45.c and ensure they now pass. Also verify that existing `task_isolator` tests (if any) still pass (the fix should not break existing functionality, only add filtering capability).
    *   `[âœ…]` 45.f. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `processComplexJob` identifies mixed completed + failed jobs, passes completed source document IDs to `planComplexStage`, and `planComplexStage` filters source documents correctly.
        *   `[âœ…]` 45.f.i. Create a test case that sets up a PLAN job with a step that has mixed completed + failed jobs (some source documents have `completed` child jobs, others have `failed` child jobs). Call `processComplexJob` and verify that it calls `planComplexStage` with the correct Set of completed source document IDs. Then verify that `planComplexStage` filters the source documents correctly and only plans jobs for the failed source documents. This test must fail initially if the filtering logic is not implemented.
    *   `[âœ…]` 45.g. `[TEST-INT]` **GREEN**: Re-run the test from step 45.f and ensure it now passes. This verifies that the complete end-to-end flow works correctly: `processComplexJob` tracks completed source documents and passes them to `planComplexStage`, and `planComplexStage` filters source documents correctly.
    *   `[âœ…]` 45.h. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/task_isolator.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 45.i. `[CRITERIA]` All requirements are met: (1) `planComplexStage` accepts an optional `completedSourceDocumentIds?: Set<string>` parameter, (2) When `completedSourceDocumentIds` is provided and non-empty, `planComplexStage` filters source documents to exclude those with identifiers matching the Set, (3) Source document identifiers are extracted using `document_relationships?.source_group` which MUST be present and non-empty - if missing, undefined, null, or empty, the function throws an error immediately (fail loud and hard, no fallbacks, defaults, or silent healing), (4) When `completedSourceDocumentIds` is not provided or is empty, all source documents are passed to the planner, (5) Source documents without valid `source_group` throw errors (not filtered, not preserved - fail loud and hard), (6) Filtering only occurs when the parameter is provided and non-empty, (7) `mapResourceToSourceDocument` extracts and preserves `document_relationships` from `row.resource_description.document_relationships` (not set to `null`), (8) `mapContributionToSourceDocument` continues to extract and preserve `document_relationships` from `row.document_relationships` (direct column), (9) Source documents from BOTH `dialectic_contributions` AND `dialectic_project_resources` have `document_relationships` preserved correctly, (10) Tests verify that BOTH types of source documents can be filtered based on `source_group` extracted from their `document_relationships`, (11) Tests explicitly test rows from BOTH tables and verify `document_relationships` is identified and extracted correctly from BOTH locations, (12) All tests pass, including tests that verify filtering works correctly, backward compatibility is maintained, error handling for missing `source_group` is correct, edge cases are handled, and BOTH data source types are fully tested, (13) The file is lint-clean, (14) Selective re-planning preserves completed work by filtering out source documents with completed jobs, regardless of which table they originated from.

*   `[âœ…]` 46. **`[BE]` Fix Document Generation Looping Issue - Track In-Progress Jobs to Prevent Re-Planning**
    *   `[âœ…]` 46.a. `[DEPS]` **ROOT CAUSE ANALYSIS**: The terminal logs (lines 415-435) show EXECUTE jobs with status `retrying`, and the same document being generated multiple times. The root cause is that `processComplexJob` in `supabase/functions/dialectic-worker/processComplexJob.ts` (lines 129-133) only queries for child jobs with `status === 'completed'` to track step completion. This creates a critical gap: (1) When an EXECUTE job encounters an error (e.g., JSON validation failure, AI provider error, or other retryable condition), `executeModelCallAndSave` calls `retryJob` (line 978 in `executeModelCallAndSave.ts`), which sets the job status to `retrying` (not `completed`). (2) The database trigger `handle_job_completion` (in `supabase/migrations/20251119160820_retrying_trigger.sql` lines 322-324) only triggers when jobs enter terminal states (`completed`, `failed`, `retry_loop_failed`), so `retrying` jobs do NOT wake up the parent PLAN job. (3) When `processComplexJob` runs again (for any reason), it queries for completed children (line 133: `.eq('status', 'completed')`), and the `retrying` job is NOT included in the query. (4) Since the `retrying` job is invisible to the completion tracking logic, `processComplexJob` doesn't see the step as "in progress", so it re-plans the same step, creating duplicate EXECUTE jobs. (5) The duplicate jobs also encounter errors â†’ more `retrying` jobs â†’ infinite loop. **THE FIX**: `processComplexJob` must query for ALL child jobs (not just `completed`) and implement a three-tier re-planning strategy: (1) Steps with ANY in-progress jobs (`retrying`, `processing`, `pending`) â†’ do NOT re-plan (wait for completion), (2) Steps with ONLY completed jobs â†’ do NOT re-plan (step is done), (3) Steps with ONLY failed/retry_loop_failed jobs â†’ CAN re-plan (all work failed, retry all), (4) Steps with MIXED completed + failed jobs â†’ CAN re-plan, but ONLY for failed work (preserve completed work). For case (4), `processComplexJob` must identify which source documents already have completed jobs (using `document_relationships.source_group` or `canonicalPathParams`), filter source documents to exclude those with completed jobs, and pass the filtered list to `planComplexStage` so only failed work is re-planned. This ensures correctly completed work is never redone, allowing completed contributions to be consumed by future steps alongside newly-completed (previously failed) work.
    *   `[âœ…]` 46.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.errors.test.ts`, write tests that prove the looping issue exists and verify the fix prevents it.
        *   `[âœ…]` 46.b.i. Create a test case that mocks a PLAN job with a recipe step that has a child EXECUTE job with status `retrying` (simulating a retryable error). The child job has `planner_metadata.recipe_step_id` matching the step's `id`. Assert that `processComplexJob` does NOT re-plan the same step (the step should be excluded from `readySteps` because it has an in-progress job). This test must fail because `processComplexJob` currently only queries for `completed` jobs and doesn't see the `retrying` job.
        *   `[âœ…]` 46.b.ii. Create a test case that mocks a PLAN job with a recipe step that has a child EXECUTE job with status `processing` (job is currently executing). Assert that `processComplexJob` does NOT re-plan the same step. This test must fail because `processComplexJob` currently only queries for `completed` jobs and doesn't see the `processing` job.
        *   `[âœ…]` 46.b.iii. Create a test case that mocks a PLAN job with a recipe step that has a child EXECUTE job with status `pending` (job is queued but not started). Assert that `processComplexJob` does NOT re-plan the same step. This test must fail because `processComplexJob` currently only queries for `completed` jobs and doesn't see the `pending` job.
        *   `[âœ…]` 46.b.iv. Create a test case that mocks a PLAN job where one step has a `completed` child job and another step has a `retrying` child job. Assert that `processComplexJob` only plans steps that have NO child jobs (neither completed nor in-progress). The step with the `completed` job should be excluded (already handled by existing filter), and the step with the `retrying` job should also be excluded (new requirement). This test must fail because `processComplexJob` doesn't track in-progress jobs.
        *   `[âœ…]` 46.b.v. Create a test case that mocks a PLAN job where a step has multiple child jobs: one `completed` and one `retrying`. Assert that `processComplexJob` does NOT re-plan the step (the step is already in progress via the `retrying` job, even though one job completed). This test must fail because `processComplexJob` only sees the `completed` job and doesn't account for the `retrying` job.
        *   `[âœ…]` 46.b.vi. Create a test case that mocks a PLAN job where a step has multiple child jobs with mixed statuses: one `completed`, one `retrying`, one `processing`. Assert that the step is NOT re-planned (has in-progress jobs). This test verifies the fix handles complex scenarios with multiple jobs per step and must fail initially.
        *   `[âœ…]` 46.b.vii. Create a test case where a step has a child job with status `failed` (terminal state). Assert that the step CAN be re-planned (failed jobs don't block re-planning, as the step needs to be retried). This test verifies that only in-progress statuses block re-planning, not terminal failure statuses, and should pass initially but must continue to pass after the fix.
        *   `[âœ…]` 46.b.viii. Create a test case where a step has a child job with status `retry_loop_failed` (terminal state). Assert that the step CAN be re-planned (exhausted retries don't block re-planning, as the step needs manual intervention or a new attempt). This test verifies that terminal failure statuses don't block re-planning and should pass initially but must continue to pass after the fix.
        *   `[âœ…]` 46.b.ix. Create a test case where a step has NO child jobs (never been planned). Assert that the step IS included in `readySteps` (can be planned for the first time). This test verifies that the fix doesn't break the normal planning flow for new steps and should pass initially but must continue to pass after the fix.
        *   `[âœ…]` 46.b.x. Create a test case where a step has MIXED completed and failed jobs (e.g., 3 source documents: doc1 has `completed` job, doc2 has `failed` job, doc3 has `completed` job). Assert that `processComplexJob` re-plans the step, but ONLY creates new EXECUTE jobs for doc2 (the failed one), NOT for doc1 or doc3 (the completed ones). This test must fail because `processComplexJob` currently doesn't filter source documents to exclude those with completed jobs. The test should verify that `planComplexStage` is called with a filtered list of source documents that excludes doc1 and doc3 (the filtering logic will be implemented in step 46 for `task_isolator.ts`).
        *   `[âœ…]` 46.b.xi. Create a test case where a step has MIXED completed and failed jobs, and the completed jobs have contributions that should be available for future steps. Assert that after re-planning, the completed contributions remain available (not overwritten or deleted), and the newly-completed (previously failed) contributions are also available. This test verifies that correctly completed work is preserved and can be consumed by future steps alongside newly-completed work.
    *   `[âœ…]` 46.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/processComplexJob.ts`, fix the completion tracking to account for in-progress jobs and implement selective re-planning that preserves completed work.
        *   `[âœ…]` 46.c.i. Replace the single query for completed children (lines 129-133) with a query for ALL child jobs (remove `.eq('status', 'completed')` filter) to get the complete picture of step activity. The query should be: `const { data: allChildren, error: childrenError } = await dbClient.from('dialectic_generation_jobs').select('id, payload, status').eq('parent_job_id', parentJobId);` This provides visibility into ALL child job statuses.
        *   `[âœ…]` 46.c.ii. Create a new Set `stepsWithInProgressJobs` to track steps that have ANY in-progress jobs (`status IN ('pending', 'processing', 'retrying')`). For each child job in `allChildren`, extract `planner_metadata.recipe_step_id`, map it to `step_slug` using `stepSlugById`, and if the job status is `pending`, `processing`, or `retrying`, add the `step_slug` to `stepsWithInProgressJobs`. This ensures steps with active jobs are tracked separately from completed steps.
        *   `[âœ…]` 46.c.iii. Create a Map `completedSourceDocumentsByStep` to track which source documents already have completed jobs for each step. For each child job in `allChildren` with `status === 'completed'`, extract `planner_metadata.recipe_step_id`, map it to `step_slug` using `stepSlugById`, extract the source document identifier from `payload.document_relationships?.source_group` (or from `payload.canonicalPathParams` if `source_group` is not available), and add it to a Set in the Map: `completedSourceDocumentsByStep.get(step_slug)?.add(sourceDocId)`. This tracks which source documents have completed work that should NOT be re-planned.
        *   `[âœ…]` 46.c.iv. Update the existing `completedStepSlugs` Set logic (lines 155-175) to use `allChildren` filtered by `status === 'completed'` instead of a separate query. This maintains backward compatibility with the existing completion tracking.
        *   `[âœ…]` 46.c.v. Update the `readySteps` building logic (lines 200-226) to exclude steps that are in `stepsWithInProgressJobs`. Add a check after line 204: `if (stepsWithInProgressJobs.has(slug)) continue;` This ensures steps with ANY in-progress jobs (retrying, processing, pending) are NOT re-planned, preventing the infinite loop.
        *   `[âœ…]` 46.c.vi. Update the `filteredReadySteps` logic (line 229) to categorize steps into three groups: (1) Steps with in-progress jobs â†’ exclude completely (already handled by `stepsWithInProgressJobs`), (2) Steps with ONLY completed jobs â†’ exclude (step is done), (3) Steps with ONLY failed/retry_loop_failed jobs â†’ include (can re-plan all), (4) Steps with MIXED completed + failed jobs â†’ include (can re-plan, but need to filter source documents). The filter should be: `const filteredReadySteps = readySteps.filter(step => !completedStepSlugs.has(step.step_slug) && !stepsWithInProgressJobs.has(step.step_slug));` This provides a second layer of protection against re-planning in-progress steps.
        *   `[âœ…]` 46.c.vii. Modify the delegation to `planComplexStage` (lines 380-386) to pass information about completed source documents. For each step in `filteredReadySteps`, check if `completedSourceDocumentsByStep.has(step.step_slug)`. If it does, the step has mixed completed + failed jobs, so we need to filter source documents. Pass `completedSourceDocIds ?? undefined` as the 6th argument when calling `planComplexStage`. Remove or update the NOTE log (line 377) since filtering will be implemented. This ensures only failed work is re-planned, preserving completed work.
        *   `[âœ…]` 46.c.viii. Add comprehensive logging to track in-progress jobs, completed source documents per step, and which source documents are filtered out during re-planning: log the count of in-progress jobs per step, log which steps are excluded from planning due to in-progress jobs, log which source documents have completed jobs for each step, log which source documents are filtered out during re-planning, and log the final `filteredReadySteps` to aid debugging. This helps diagnose future issues.
        *   `[âœ…]` 46.c.ix. Ensure the validation logic for `planner_metadata.recipe_step_id` (lines 157-169) applies to ALL child jobs (not just completed ones), so in-progress jobs are also validated. This prevents tracking errors when in-progress jobs have invalid metadata.
    *   `[âœ…]` 46.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 46.b and ensure they now pass. Also verify that existing `processComplexJob` tests still pass (the fix should not break existing functionality, only add new tracking for in-progress jobs).
    *   `[âœ…]` 46.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processComplexJob.ts` and `supabase/functions/dialectic-worker/processComplexJob.errors.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 46.f. `[CRITERIA]` All requirements are met: (1) `processComplexJob` queries for ALL child jobs (not just `completed`) to get complete visibility into step activity, (2) `processComplexJob` tracks steps with in-progress jobs (`pending`, `processing`, `retrying`) in a separate Set `stepsWithInProgressJobs`, (3) Steps with in-progress jobs are excluded from `readySteps` and `filteredReadySteps`, preventing re-planning, (4) Steps with terminal failure statuses (`failed`, `retry_loop_failed`) do NOT block re-planning (can be retried), (5) Steps with NO child jobs are still included in `readySteps` (normal planning flow works), (6) Steps with MIXED completed + failed jobs CAN be re-planned, and `processComplexJob` passes completed source document IDs to `planComplexStage` (the actual filtering logic is implemented in step 46 for `task_isolator.ts`), (7) Correctly completed work is tracked via `completedSourceDocumentsByStep` Map, (8) Completed contributions remain available for future steps alongside newly-completed (previously failed) contributions, (9) All tests pass, including tests that verify in-progress jobs prevent re-planning and edge cases are handled correctly, (10) The file is lint-clean, (11) Document generation no longer loops - once a step has an in-progress job, it is never re-planned until that job completes or fails terminally.

*   `[âœ…]` 47. **`[BE]` Fix Document Renderer - Parse JSON and Extract `content` Field Instead of Inserting Raw JSON Strings**
    *   `[âœ…]` 47.a. `[DEPS]` The document rendering flow is designed to work as follows: (1) `executeModelCallAndSave` receives AI responses that are JSON objects (e.g., `{"content": "# Business Case\n\n..."}`), sanitizes them to remove markdown code fences (line 940: `sanitizeJsonContent(aiResponse.content)`), validates and parses the JSON (line 973: `JSON.parse(sanitizationResult.sanitized)`), and saves the validated JSON string to `*_raw.json` files via the file manager. (2) After contributions are saved, RENDER jobs are enqueued (lines 1167-1221 in `executeModelCallAndSave.ts`). (3) The `renderDocument` function in `document_renderer.ts` should download the `*_raw.json` files (using `chunk.raw_response_storage_path` from the contribution records), parse the JSON to extract the `content` field, convert escaped newlines to actual newlines, and insert the extracted markdown into templates. **THE PROBLEMS**: (1) The renderer is reading from the WRONG location - it downloads from `chunk.file_name` (line 130) which points to contribution files, but it should read from `chunk.raw_response_storage_path` which points to `*_raw.json` files. (2) The renderer receives raw JSON strings (e.g., `{"content": "# Business Case\n\n..."}`) but does NOT parse the JSON to extract the `content` field. Instead, it inserts the raw JSON string directly into templates (line 138), resulting in rendered documents containing JSON instead of markdown. (3) The renderer does NOT convert escaped newlines (`\n` escape sequences) to actual newline characters. **THE FIX**: The `renderDocument` function must: (1) Read from the CORRECT location - use `chunk.raw_response_storage_path` instead of `chunk.file_name` to download `*_raw.json` files, (2) Parse the JSON content downloaded from storage (which was already validated by `executeModelCallAndSave` before saving), (3) Extract the `content` field from each JSON object (assume it exists and is a string - it was validated before saving), (4) Convert escaped newlines (`\n` escape sequences) to actual newline characters in the extracted content, (5) Insert the extracted and converted markdown content into templates instead of the raw JSON string, (6) If parsing fails (which should be rare since it was validated), throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 47.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/document_renderer.test.ts` or create a new test file `document_renderer.jsonParsing.test.ts`, write tests that verify `renderDocument` parses JSON content downloaded from storage, extracts the `content` field, and inserts markdown into templates instead of inserting raw JSON strings.
        *   `[âœ…]` 47.b.i. Create a test case that mocks contribution rows with `raw_response_storage_path` pointing to `*_raw.json` files containing validated JSON strings (e.g., `{"content": "# Business Case\n\n## Market Opportunity\n..."}`). Mock `downloadFromStorage` to return these JSON strings when called with the `raw_response_storage_path`. Call `renderDocument` and capture the rendered output. Assert that: (1) `downloadFromStorage` is called with `raw_response_storage_path` (not `file_name`), (2) the rendered document contains the extracted `content` field value with escaped newlines converted to actual newlines (e.g., `"# Business Case\n\n## Market Opportunity\n..."`), NOT the raw JSON string (e.g., NOT `"{\"content\": \"# Business Case\\n\\n..."`). This test must fail because the renderer currently reads from `file_name` instead of `raw_response_storage_path` and inserts raw JSON strings directly into templates instead of parsing and extracting the content field.
        *   `[âœ…]` 47.b.ii. Create a test case that mocks contribution rows with files containing JSON with a `content` field that has escaped newlines (`\n`), escaped quotes (`\"`), and escaped backslashes (`\\`). Assert that the rendered document has escaped newlines converted to actual newlines, escaped quotes converted to actual quotes, and escaped backslashes preserved correctly. Verify that the rendered content is valid markdown that can be displayed. This test must fail because the renderer currently inserts JSON strings with escape sequences intact instead of parsing and converting the content field.
        *   `[âœ…]` 47.b.iii. Create a test case that mocks contribution rows with files containing markdown content (NOT JSON, e.g., `"# Business Case\n\n## Market Opportunity\n..."`). Assert that `renderDocument` uses the markdown content directly without attempting to parse it as JSON. This test verifies backward compatibility for contributions that may already contain markdown instead of JSON.
        *   `[âœ…]` 47.b.iv. Create a test case that mocks multiple contribution chunks, some containing JSON and some containing markdown. Assert that `renderDocument` correctly parses JSON chunks to extract content and uses markdown chunks directly, then joins all extracted/raw content in the correct order. This test verifies that the renderer handles mixed content types correctly.
    *   `[âœ…]` 47.c. `[BE]` **GREEN**: In `supabase/functions/_shared/services/document_renderer.ts`, parse JSON content downloaded from storage, extract the `content` field, and insert markdown into templates instead of inserting raw JSON strings.
        *   `[âœ…]` 47.c.i. Fix the renderer to read from the CORRECT location. Replace line 130 (`const path = \`${chunk.storage_path}/${chunk.file_name}\`;`) with logic that uses `chunk.raw_response_storage_path` instead. The path should be: `const rawJsonPath = chunk.raw_response_storage_path; if (!rawJsonPath || typeof rawJsonPath !== 'string') { throw new Error(\`Contribution ${chunk.id} is missing raw_response_storage_path\`); }`. Then use `rawJsonPath` instead of the constructed path from `file_name`. This ensures the renderer reads from `*_raw.json` files.
        *   `[âœ…]` 47.c.ii. After downloading text from storage (line 131: `const text = await downloadText(...)` using the `raw_response_storage_path`), parse the JSON and extract the content field: `let contentToAdd: string; try { const parsed = JSON.parse(text); const extractedContent = parsed.content; contentToAdd = extractedContent.replace(/\\n/g, '\n'); } catch (e) { throw new Error(\`Failed to parse JSON content from contribution ${chunk.id}: ${e instanceof Error ? e.message : String(e)}\`); }`. Since the JSON was already validated by `executeModelCallAndSave` before saving, we can safely assume `parsed` is a record with a `content` field that is a string - no additional validation needed. This parses the JSON and extracts the content field.
        *   `[âœ…]` 47.c.iii. The escaped newline conversion is already handled in step 47.c.ii (`extractedContent.replace(/\\n/g, '\n')`). This converts JSON escape sequences (`\n`) to actual newline characters. Note: The JSON parser already handles escaped quotes and backslashes correctly, so only `\n` needs explicit conversion since it's preserved as a literal escape sequence in the string value.
        *   `[âœ…]` 47.c.iv. Replace line 132 (`bodyParts.push(text);`) with `bodyParts.push(contentToAdd);`. This ensures the extracted and converted markdown content is added to `bodyParts`, not the raw JSON string.
        *   `[âœ…]` 47.c.v. Add logging after parsing to aid debugging: `deps.logger?.info?.('[renderDocument] Parsed JSON and extracted content field', { chunkId: chunk.id, rawJsonPath, extractedContentLength: contentToAdd.length, hasNewlines: contentToAdd.includes('\n') });`. This helps diagnose issues in production.
        *   `[âœ…]` 47.c.vi. Ensure error messages clearly identify which contribution chunk caused the parsing error and which file path was used. This aids debugging when parsing fails (which should be rare since the JSON was validated before saving).
    *   `[âœ…]` 47.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 47.b and ensure they now pass. Also verify that existing `document_renderer` tests still pass (the fix should not break existing functionality, only add JSON parsing capability).
        *   `[âœ…]` 47.d.i. Verify that existing tests in `document_renderer.test.ts` (if they exist) still pass. These tests may expect the renderer to handle markdown content directly, which should continue to work with the new JSON parsing logic (non-JSON content is used directly).
        *   `[âœ…]` 47.d.ii. Verify that tests for continuation jobs still pass. Continuation jobs may have different JSON structures or may already contain markdown, so ensure the parsing logic handles both cases correctly.
    *   `[âœ…]` 47.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: EXECUTE job saves raw JSON content, RENDER job parses JSON and extracts content, and the rendered document contains proper markdown.
        *   `[âœ…]` 47.e.i. Create a test case that sets up an EXECUTE job, mocks an AI response with JSON containing a `content` field with markdown, calls `executeModelCallAndSave`, and verifies the saved contribution contains raw JSON (not extracted markdown). Query the database to verify the file content in storage contains a JSON string like `{"content": "# Business Case\n\n..."}`, not extracted markdown.
        *   `[âœ…]` 47.e.ii. Create a test case that simulates a RENDER job reading the contribution saved by the EXECUTE job. Assert that the RENDER job parses the JSON, extracts the `content` field, converts escaped newlines to actual newlines, and inserts the extracted markdown into the template. Verify that the rendered document contains proper markdown (not JSON) that can be displayed correctly.
        *   `[âœ…]` 47.e.iii. This test must fail initially if the JSON parsing logic is not implemented in the renderer, but should pass after step 47.c is complete.
    *   `[âœ…]` 47.f. `[TEST-INT]` **GREEN**: Re-run the test from step 47.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: EXECUTE jobs save raw JSON to storage, and RENDER jobs parse the JSON, extract the content field, and render it properly.
    *   `[âœ…]` 47.g. `[LINT]` Run the linter for `supabase/functions/_shared/services/document_renderer.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 47.h. `[CRITERIA]` All requirements are met: (1) `renderDocument` parses JSON content downloaded from storage when the content is JSON (starts with `{` or `[`), (2) The renderer extracts the `content` field from parsed JSON objects (no validation needed - JSON was already validated by `executeModelCallAndSave` before saving), (3) Escaped newlines (`\n`) in the extracted content are converted to actual newline characters, (4) The extracted markdown content (not the raw JSON string) is inserted into templates, (5) Parsing errors are thrown immediately if downloaded JSON cannot be parsed (which should be rare since it was validated before saving), (6) Non-JSON content (already markdown) is used directly without parsing (backward compatibility), (7) Rendered documents contain proper markdown that can be displayed correctly, (8) All tests pass, including tests that verify JSON parsing, content extraction, backward compatibility with markdown content, and the end-to-end flow with RENDER jobs, (9) The file is lint-clean, (10) Documents are rendered as properly formatted markdown files, not raw JSON objects.

*   `[âœ…]` 48. **`[BE]` Fix file_manager to Upload fileContent as Raw JSON with Correct fileType and mimeType**
    *   `[âœ…]` 48.a. `[DEPS]` The `uploadAndRegisterFile` function in `supabase/functions/_shared/services/file_manager.ts` currently uploads `context.fileContent` to a main contribution file path using `pathContextForStorage.fileType` (which is a document key like `FileType.business_case`), saving to `documents/` folder with `.md` extension and `mimeType: context.mimeType` (which is `"text/markdown"`). The raw JSON file (`*_raw.json`) is uploaded separately (lines 138-164) and stored in `raw_response_storage_path`. **THE PROBLEM**: In the doc-centric architecture, `fileContent` IS the validated raw JSON string (`sanitizationResult.sanitized`). There is no separate "main content" file and "raw JSON" file - they are THE SAME THING. The `fileContent` (validated JSON string) should be uploaded as `FileType.ModelContributionRawJson` to `raw_responses/` folder with `_raw.json` extension and `mimeType: "application/json"`. The separate raw JSON upload block (lines 138-164) is redundant because `fileContent` IS the raw JSON content. **THE FIX**: The file manager must: (1) Use `pathContextForStorage.fileType` as passed by the caller (which should be `FileType.ModelContributionRawJson` after step 49 fixes the caller), (2) Use `context.mimeType` as passed by the caller (which should be `"application/json"` after step 49 fixes the caller), (3) Upload `fileContent` (the validated JSON string) to the path constructed from `FileType.ModelContributionRawJson` + `documentKey`, which produces `raw_responses/{modelSlug}_{attemptCount}_{documentKey}_raw.json`, (4) Remove the separate raw JSON upload block (lines 138-164) entirely - it's redundant because `fileContent` IS the raw JSON, (5) Set the contribution record's `storage_path`, `file_name`, and `raw_response_storage_path` all to point to the same `*_raw.json` file (the raw JSON file IS the contribution), (6) Set `mime_type: context.mimeType` in the contribution record (which should be `"application/json"` after step 49 fixes the caller). The file manager uses the values passed in - no overrides or data healing. Step 49 ensures the caller passes the correct values.
    *   `[âœ…]` 48.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/file_manager.upload.test.ts`, write tests that verify `uploadAndRegisterFile` uploads `fileContent` as raw JSON with `FileType.ModelContributionRawJson` and `mimeType: "application/json"`.
        *   `[âœ…]` 48.b.i. Create a test case that calls `uploadAndRegisterFile` with a `ModelContributionUploadContext` where `fileContent` is present (a validated JSON string like `'{"content": "# Business Case\\n\\n..."}'`), `pathContext.fileType` is `FileType.ModelContributionRawJson`, `documentKey` is present in pathContext (e.g., `'business_case'`), and `mimeType` is `"application/json"`. Assert that: (1) The file is uploaded to `raw_responses/` folder (not `documents/`), (2) The filename is `{modelSlug}_{attemptCount}_{documentKey}_raw.json` (not `{documentKey}.md`), (3) The upload uses `contentType: "application/json"` (not `"text/markdown"`), (4) The contribution record is created with `storage_path`, `file_name`, and `raw_response_storage_path` all pointing to the same `*_raw.json` file, (5) The `mime_type` field in the contribution record is `"application/json"`, (6) The separate raw JSON upload block (lines 138-164) is NOT executed (no duplicate upload). This test must fail because the file manager currently uses the document key fileType and saves to `documents/` with `.md` extension.
        *   `[âœ…]` 48.b.ii. Create a test case that calls `uploadAndRegisterFile` with a `ModelContributionUploadContext` where `fileContent` is present, `pathContext.fileType` is `FileType.ModelContributionRawJson`, and `isContinuation: true` with `turnIndex: 2`. Assert that: (1) The file is uploaded to `_work/raw_responses/` folder (continuation path), (2) The filename includes `_continuation_2` suffix, (3) The continuation logic (lines 57-63, 223-234) still works correctly and the contribution record is created with the correct `target_contribution_id` and continuation path structure. This test verifies continuation paths work correctly with the new fileType.
        *   `[âœ…]` 48.b.iii. Create a test case that calls `uploadAndRegisterFile` with a `ModelContributionUploadContext` where `fileContent` is missing. Assert that the function returns an error immediately (fileContent is required for model contributions). This test should pass initially and must continue to pass.
        *   `[âœ…]` 48.b.iv. Create a test case that verifies the separate raw JSON upload block (lines 138-164) is removed. Assert that when `fileContent` is present and `rawJsonResponseContent` is also present in metadata, only ONE file is uploaded (the `fileContent`), and the separate raw JSON upload block does NOT execute. This test must fail initially because the separate raw JSON upload block still exists.
    *   `[âœ…]` 48.c. `[BE]` **GREEN**: In `supabase/functions/_shared/services/file_manager.ts`, update `uploadAndRegisterFile` to upload `fileContent` as raw JSON using the passed `fileType` and `mimeType`.
        *   `[âœ…]` 48.c.i. The main content upload logic (lines 72-113) already uses `pathContextForStorage.fileType` and `context.mimeType` as passed by the caller. No changes needed here - the caller (step 49) will pass `FileType.ModelContributionRawJson` and `mimeType: "application/json"`. The path constructor will use `FileType.ModelContributionRawJson` + `documentKey` to construct the path to `raw_responses/{modelSlug}_{attemptCount}_{documentKey}_raw.json`.
        *   `[âœ…]` 48.c.ii. Remove the separate raw JSON upload block (lines 138-164) entirely. This block is redundant because `fileContent` IS the raw JSON content. Delete the entire `if (isModelContributionContext(context) && context.contributionMetadata.rawJsonResponseContent)` block and all its contents (lines 138-164).
        *   `[âœ…]` 48.c.iii. When creating the contribution record (lines 236-261), set `storage_path` to `finalMainContentFilePath`, `file_name` to `finalFileName`, and `raw_response_storage_path` to the same full path (`${finalMainContentFilePath}/${finalFileName}`). All three fields point to the same `*_raw.json` file because `fileContent` IS the raw JSON content and there is only one file.
        *   `[âœ…]` 48.c.iv. Set `mime_type: context.mimeType` in the contribution record (line 245). The caller (step 49) will pass `mimeType: "application/json"`, so this will correctly set the mime type in the database.
        *   `[âœ…]` 48.c.v. Calculate `sizeBytes` from `fileContent.length` (it's a string). Use `context.sizeBytes` as passed by the caller (which should be `fileContent.length` after step 49 fixes the caller).
        *   `[âœ…]` 48.c.vi. Update error handling and cleanup logic (lines 211-217, 226-232, 337-347) to remove references to `rawJsonResponseFullStoragePath` since the separate raw JSON upload block is removed. The cleanup should only reference `finalMainContentFilePath` and `finalFileName` (which point to the `*_raw.json` file).
        *   `[âœ…]` 48.c.vii. Ensure the file manager uses the values passed in by the caller - no overrides, no data healing. The caller (step 49) is responsible for passing the correct `fileType`, `mimeType`, and `fileContent`.
    *   `[âœ…]` 48.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 48.b and ensure they now pass. Also verify that existing `file_manager.upload.test.ts` tests still pass (update any tests that expect separate raw JSON uploads).
        *   `[âœ…]` 48.d.i. Update existing tests in `file_manager.upload.test.ts` that test model contribution uploads. These tests may expect both a main content file and a separate raw JSON file. Update them to verify: (1) Only ONE file is uploaded (the `fileContent` as raw JSON), (2) The file is saved to `raw_responses/` with `_raw.json` extension when `fileType` is `FileType.ModelContributionRawJson`, (3) The `mime_type` is `"application/json"` when `mimeType` is `"application/json"`, (4) The separate raw JSON upload block does NOT execute. Ensure all existing tests still pass.
        *   `[âœ…]` 48.d.ii. Verify that tests for resource contexts and user feedback contexts still pass (they are unaffected by this change).
    *   `[âœ…]` 48.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `executeModelCallAndSave` calls `fileManager.uploadAndRegisterFile` with `fileContent` (validated JSON), `fileType: FileType.ModelContributionRawJson`, and `mimeType: "application/json"`, and the file manager correctly creates a contribution record pointing to the `*_raw.json` file.
        *   `[âœ…]` 48.e.i. Create a test case that sets up an EXECUTE job, mocks `executeModelCallAndSave` to call `fileManager.uploadAndRegisterFile` with a `ModelContributionUploadContext` where `fileContent` is present (validated JSON string like `'{"content": "# Business Case\\n\\n..."}'`), `pathContext.fileType` is `FileType.ModelContributionRawJson`, `pathContext.documentKey` is present (e.g., `'business_case'`), and `mimeType` is `"application/json"`. Assert that: (1) Only a `*_raw.json` file exists in storage (no separate contribution file), (2) The file is saved to `raw_responses/` folder (not `documents/`), (3) The filename is `{modelSlug}_{attemptCount}_{documentKey}_raw.json`, (4) The contribution record has `storage_path`, `file_name`, and `raw_response_storage_path` all pointing to the same `*_raw.json` file, (5) The `mime_type` field is `"application/json"`, (6) The `*_raw.json` file contains the validated JSON string. This test must fail initially if the file manager fix is not implemented, but should pass after step 48.c is complete (and step 49.c ensures the caller passes correct values).
    *   `[âœ…]` 48.f. `[TEST-INT]` **GREEN**: Re-run the test from step 48.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: `executeModelCallAndSave` calls the file manager with `fileContent` (validated JSON), `fileType: FileType.ModelContributionRawJson`, and `mimeType: "application/json"`, and the file manager correctly uploads it to `raw_responses/` with `_raw.json` extension.
    *   `[âœ…]` 48.g. `[LINT]` Run the linter for `supabase/functions/_shared/services/file_manager.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 48.h. `[CRITERIA]` All requirements are met: (1) `uploadAndRegisterFile` uses `pathContextForStorage.fileType` and `context.mimeType` as passed by the caller (no overrides or data healing), (2) When the caller passes `FileType.ModelContributionRawJson` and `mimeType: "application/json"`, the file is uploaded to `raw_responses/` folder with `_raw.json` extension, (3) The separate raw JSON upload block (lines 138-164) is removed entirely (it's redundant because `fileContent` IS the raw JSON), (4) The contribution record's `storage_path`, `file_name`, and `raw_response_storage_path` all point to the same `*_raw.json` file (there is only one file - the raw JSON file IS the contribution), (5) The `mime_type` field in the contribution record is set to `context.mimeType` (which should be `"application/json"` after step 49 fixes the caller), (6) Error handling and cleanup logic correctly handles the single file upload, (7) All tests pass, including unit tests that verify `fileContent` is uploaded as raw JSON with correct fileType and mimeType, and integration tests that verify the end-to-end flow, (8) The file is lint-clean, (9) The file manager correctly supports the doc-centric architecture where `fileContent` IS the raw JSON content and there is only one file.

*   `[âœ…]` 49. **`[BE]` Fix executeModelCallAndSave - Pass Correct fileType and mimeType for Raw JSON Upload**
    *   `[âœ…]` 49.a. `[DEPS]` The intended document generation flow is: (1) `executeModelCallAndSave` receives AI responses that are JSON objects (e.g., `{"content": "# Business Case\n\n..."}`), sanitizes them (line 940: `sanitizeJsonContent(aiResponse.content)`), validates and parses the JSON (line 973: `JSON.parse(sanitizationResult.sanitized)`), (2) The validated JSON string (`sanitizationResult.sanitized`) IS the contribution - there is no separate "main content" and "raw JSON" - they are THE SAME THING, (3) The validated JSON string is saved to a `*_raw.json` file via the file manager's `uploadAndRegisterFile` function by passing `fileContent: sanitizationResult.sanitized`, `pathContext.fileType: FileType.ModelContributionRawJson`, and `mimeType: "application/json"`, (4) The file manager uploads the file to storage AND creates the contribution record in the database pointing to it in a single operation, (5) RENDER jobs read from `*_raw.json` files and render them into markdown documents. **THE PROBLEM**: `executeModelCallAndSave` is INCORRECTLY: (1) Setting `pathContext.fileType: fileType` where `fileType = output_type` (a document key like `FileType.business_case`), causing the file manager to save to `documents/` folder with `.md` extension, (2) Setting `mimeType: aiResponse.contentType || "text/markdown"`, causing the file manager to save with wrong MIME type, (3) Setting `fileContent: contentForStorage` (which is correct - it's the validated JSON string), (4) Setting `rawJsonResponseContent: aiResponse.rawProviderResponse` (which is wrong - it should be `sanitizationResult.sanitized`, but this field is redundant because `fileContent` IS the raw JSON). **THE FIX**: `executeModelCallAndSave` must: (1) Set `pathContext.fileType = FileType.ModelContributionRawJson` (not the document key fileType), (2) Set `mimeType = "application/json"` (not `"text/markdown"`), (3) Keep `fileContent: contentForStorage` (the validated JSON string - this is correct), (4) Remove `rawJsonResponseContent` from the upload context entirely (it's redundant - `fileContent` IS the raw JSON content), (5) The file manager will then upload `fileContent` to `raw_responses/{modelSlug}_{attemptCount}_{documentKey}_raw.json` with `mimeType: "application/json"`.
    *   `[âœ…]` 49.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts` or create a new test file `executeModelCallAndSave.rawJsonOnly.test.ts`, write tests that verify `executeModelCallAndSave` passes correct `fileType`, `mimeType`, and `fileContent` to the file manager.
        *   `[âœ…]` 49.b.i. Create a test case that mocks an AI response with JSON, calls `executeModelCallAndSave`, and verifies that `fileManager.uploadAndRegisterFile` is called with `pathContext.fileType = FileType.ModelContributionRawJson` (not the document key fileType like `FileType.business_case`). Assert that the `fileType` passed to the file manager is `FileType.ModelContributionRawJson`. This test must fail because the function currently passes the document key fileType (e.g., `FileType.business_case`) instead of `FileType.ModelContributionRawJson`.
        *   `[âœ…]` 49.b.ii. Create a test case that mocks an AI response with JSON, calls `executeModelCallAndSave`, and verifies that `fileManager.uploadAndRegisterFile` is called with `mimeType = "application/json"` (not `"text/markdown"`). Assert that the `mimeType` passed to the file manager is `"application/json"`. This test must fail because the function currently passes `aiResponse.contentType || "text/markdown"` instead of `"application/json"`.
        *   `[âœ…]` 49.b.iii. Create a test case that mocks an AI response with JSON, calls `executeModelCallAndSave`, and verifies that `fileManager.uploadAndRegisterFile` is called with `fileContent: sanitizationResult.sanitized` (the validated JSON string). Assert that the `fileContent` contains the sanitized JSON string like `'{"content": "# Business Case\\n\\n..."}'`, not the raw provider response object. This test should pass initially (the function already sets `fileContent: contentForStorage` correctly).
        *   `[âœ…]` 49.b.iv. Create a test case that mocks an AI response with JSON, calls `executeModelCallAndSave`, and verifies that `rawJsonResponseContent` is NOT present in the upload context (it's redundant - `fileContent` IS the raw JSON). Assert that the upload context does NOT include `rawJsonResponseContent` in `contributionMetadata`. This test must fail because the function currently sets `rawJsonResponseContent: aiResponse.rawProviderResponse`.
        *   `[âœ…]` 49.b.v. Create a test case that verifies the contribution record is created with `file_name` and `storage_path` pointing to the `*_raw.json` file in `raw_responses/` folder. Assert that the contribution record exists and has the correct `storage_path` containing `raw_responses/` (not `documents/`), `file_name` ending with `_raw.json` (not `.md`), and `mime_type` set to `"application/json"`. This test verifies that database records are created correctly with the right path, filename, and MIME type.
    *   `[âœ…]` 49.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, fix the upload context to pass correct `fileType`, `mimeType`, and remove redundant `rawJsonResponseContent`.
        *   `[âœ…]` 49.c.i. Set `pathContext.fileType = FileType.ModelContributionRawJson` (line 1125). Replace `fileType: fileType,` (where `fileType = output_type` which is a document key) with `fileType: FileType.ModelContributionRawJson,`. This ensures the file manager constructs the path to `raw_responses/` folder with `_raw.json` extension, not `documents/` folder with `.md` extension.
        *   `[âœ…]` 49.c.ii. Set `mimeType = "application/json"` (line 1137). Replace `mimeType: aiResponse.contentType || "text/markdown",` with `mimeType: "application/json",`. This ensures the file manager uploads with the correct MIME type and sets `mime_type: "application/json"` in the contribution record.
        *   `[âœ…]` 49.c.iii. Keep `fileContent: contentForStorage` (line 1136) - this is correct. `contentForStorage = sanitizationResult.sanitized` is the validated JSON string, which IS the raw JSON content. No changes needed here.
        *   `[âœ…]` 49.c.iv. Remove `rawJsonResponseContent` from the upload context (line 1148). Delete `rawJsonResponseContent: aiResponse.rawProviderResponse,` from `contributionMetadata`. This field is redundant because `fileContent` IS the raw JSON content. There is no separate raw JSON - `fileContent` IS the raw JSON.
        *   `[âœ…]` 49.c.v. Set `sizeBytes: contentForStorage.length` (line 1138). The `contentForStorage` is a string (the validated JSON), so `contentForStorage.length` is the correct size in bytes. This is already correct, but verify it's using `contentForStorage.length` and not calculating from `rawJsonResponseContent`.
        *   `[âœ…]` 49.c.vi. Verify that the file manager correctly sets the contribution record's `file_name`, `storage_path`, and `raw_response_storage_path` to point to the `*_raw.json` file when it uploads it. The file manager's `uploadAndRegisterFile` function will use the passed `fileType: FileType.ModelContributionRawJson` + `documentKey` to construct the path to `raw_responses/{modelSlug}_{attemptCount}_{documentKey}_raw.json`.
        *   `[âœ…]` 49.c.vii. Add logging: `deps.logger.info('[executeModelCallAndSave] Saving validated JSON to raw file', { jobId, documentKey: validatedDocumentKey, fileType: FileType.ModelContributionRawJson });`. This helps diagnose issues in production.
        *   `[âœ…]` 49.c.viii. Verify that RENDER jobs can still be enqueued correctly (lines 1167-1221). The RENDER job will use `raw_response_storage_path` (or `file_name`/`storage_path` if they point to the raw file) to find the `*_raw.json` files (which step 46 fixes in the renderer).
    *   `[âœ…]` 49.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 49.b and ensure they now pass. Also verify that existing `executeModelCallAndSave` tests still pass (update any tests that expect document key fileType or wrong mimeType).
        *   `[âœ…]` 49.d.i. Update existing tests in `executeModelCallAndSave.test.ts` that verify file uploads. These tests currently assert that `fileType` is a document key (e.g., `FileType.business_case`) and `mimeType` is `"text/markdown"`. Update them to assert that `fileType` is `FileType.ModelContributionRawJson` and `mimeType` is `"application/json"`.
        *   `[âœ…]` 49.d.ii. Update existing tests that verify `rawJsonResponseContent` is set. These tests should be updated to assert that `rawJsonResponseContent` is NOT present in the upload context (it's redundant - `fileContent` IS the raw JSON).
        *   `[âœ…]` 49.d.iii. Verify that tests for continuation jobs still pass. Continuation jobs should also pass `FileType.ModelContributionRawJson` and `mimeType: "application/json"` to save to `*_raw.json` files in `_work/raw_responses/` folder.
    *   `[âœ…]` 49.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: EXECUTE job saves validated JSON to `*_raw.json` with correct fileType and mimeType, RENDER job reads from `*_raw.json` and renders markdown.
        *   `[âœ…]` 49.e.i. Create a test case that sets up an EXECUTE job, mocks an AI response with JSON, calls `executeModelCallAndSave`, and verifies: (1) Only a `*_raw.json` file exists in storage (no separate contribution file), (2) The file is saved to `raw_responses/` folder (not `documents/`), (3) The filename is `{modelSlug}_{attemptCount}_{documentKey}_raw.json` (not `{documentKey}.md`), (4) The `*_raw.json` file contains the validated JSON string (`sanitizationResult.sanitized`), (5) The contribution record has `file_name`, `storage_path`, and `raw_response_storage_path` all pointing to the `*_raw.json` file, (6) The `mime_type` field is `"application/json"` (not `"text/markdown"`).
        *   `[âœ…]` 49.e.ii. Create a test case that simulates a RENDER job reading the `*_raw.json` file saved by the EXECUTE job. Assert that the RENDER job can find and read the `*_raw.json` file using `raw_response_storage_path`, parse the JSON, extract the content, and render it into a markdown document. This test must fail initially if the RENDER job is still trying to read from contribution files instead of `*_raw.json` files.
        *   `[âœ…]` 49.e.iii. This test must fail initially if the fix is not implemented, but should pass after step 49.c is complete (and step 46.c is complete for the renderer fix).
    *   `[âœ…]` 49.f. `[TEST-INT]` **GREEN**: Re-run the test from step 49.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: EXECUTE jobs save validated JSON to `*_raw.json` files with correct fileType (`FileType.ModelContributionRawJson`) and mimeType (`"application/json"`), and RENDER jobs read from `*_raw.json` files and render them properly.
    *   `[âœ…]` 49.g. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 49.h. `[CRITERIA]` All requirements are met: (1) `executeModelCallAndSave` passes `pathContext.fileType = FileType.ModelContributionRawJson` to the file manager (not the document key fileType), (2) `executeModelCallAndSave` passes `mimeType = "application/json"` to the file manager (not `"text/markdown"`), (3) `executeModelCallAndSave` passes `fileContent: sanitizationResult.sanitized` to the file manager (the validated JSON string - this is correct), (4) `executeModelCallAndSave` does NOT include `rawJsonResponseContent` in the upload context (it's redundant - `fileContent` IS the raw JSON), (5) The file manager uploads `fileContent` to `raw_responses/{modelSlug}_{attemptCount}_{documentKey}_raw.json` with `mimeType: "application/json"`, (6) The contribution record is created in the database with `file_name`, `storage_path`, and `raw_response_storage_path` all pointing to the `*_raw.json` file, (7) The `mime_type` field in the contribution record is `"application/json"`, (8) RENDER jobs can read from `*_raw.json` files using the contribution record's file references, (9) All tests pass, including tests that verify correct fileType, mimeType, and fileContent are passed to the file manager, the contribution record points to the raw file with correct MIME type, and the end-to-end flow works correctly, (10) The file is lint-clean, (11) The validated JSON is saved to `*_raw.json` with correct fileType, mimeType, path, and filename, and the contribution record correctly references it.

*   `[âœ…]` 50. **`[STORE]` Fix WALLET_TRANSACTION Type Validation in notificationStore.ts**
    *   `[âœ…]` 50.a. `[DEPS]` The backend sends `newBalance` as a NUMBER (`rpcResult.balance_after_txn` from `tokenWalletService.ts` line 317, which is a PostgreSQL NUMERIC type returned as JavaScript `number`). The frontend validation in `notificationStore.ts` (line 504) currently requires `typeof data['newBalance'] === 'string'`, which rejects valid notifications from the backend. The `_handleWalletUpdateNotification` function in `walletStore.ts` (line 47) expects `newBalance: string`. The validation must accept NUMBER (what the backend sends) and convert it to string when calling the handler.
    *   `[âœ…]` 50.b. `[TEST-UNIT]` **RED**: In `packages/store/src/notificationStore.test.ts`, write tests that reproduce the WALLET_TRANSACTION type validation issue.
        *   `[âœ…]` 50.b.i. Create a test case that mocks a `WALLET_TRANSACTION` notification with `data.newBalance` as a NUMBER (e.g., `newBalance: 1000`), matching the actual backend data structure from `tokenWalletService.ts` line 317 where `rpcResult.balance_after_txn` is a number. Assert that the notification IS rejected with an error logged because `newBalance` is not a string. This test must fail because the validation currently requires `newBalance` to be a string, but the backend sends a number, so valid notifications are being rejected. The test proves the validation is incorrect - it should accept NUMBER (what backend sends).
        *   `[âœ…]` 50.b.ii. Create a test case that mocks a `WALLET_TRANSACTION` notification with `data.newBalance` as `null` or `undefined`, and assert that the notification IS rejected (invalid data). This test should pass initially and must continue to pass.
        *   `[âœ…]` 50.b.iii. Create a test case that mocks a `WALLET_TRANSACTION` notification with `data.newBalance` as a non-numeric value (e.g., object, array, boolean, string), and assert that the notification IS rejected (invalid data). This test should pass initially and must continue to pass.
    *   `[âœ…]` 50.c. `[STORE]` **GREEN**: In `packages/store/src/notificationStore.ts`, fix the WALLET_TRANSACTION validation to accept NUMBER (what the backend sends).
        *   `[âœ…]` 50.c.i. Update the validation check (line 504) to accept `newBalance` as a NUMBER: change `typeof data['newBalance'] === 'string'` to `typeof data['newBalance'] === 'number'`. This validates for the correct type that the backend actually sends. The validation must reject non-numeric values (null, undefined, string, object, array, boolean) to ensure only valid numeric balance data is accepted.
        *   `[âœ…]` 50.c.ii. Convert `newBalance` from number to string before passing it to the wallet handler (line 508): change `newBalance: data['newBalance']` to `newBalance: String(data['newBalance'])`. This is a required type conversion to match the handler signature (`_handleWalletUpdateNotification` expects `newBalance: string`). 
    *   `[âœ…]` 50.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 50.b and ensure they now pass. Also verify that existing notification store tests still pass (update any tests that expect `newBalance` to be a string to use number instead, matching the actual backend behavior).
    *   `[âœ…]` 50.e. `[TEST-INT]` **RED**: In `packages/store/src/dialecticStore.notifications.integration.test.ts` or create a new integration test file, write a test that verifies end-to-end WALLET_TRANSACTION notification handling.
        *   `[âœ…]` 50.e.i. Create a test case that simulates a `WALLET_TRANSACTION` notification with `newBalance` as a NUMBER (matching actual backend behavior from `tokenWalletService.ts` line 317), and verify that: (a) the notification is NOT rejected, (b) the wallet handler IS called with `newBalance` as a string (converted from number), (c) the notification IS added to the notification list. This test must fail initially if the validation fix is not complete.
    *   `[âœ…]` 50.f. `[TEST-INT]` **GREEN**: Re-run the test from step 50.e and ensure it now passes. This verifies that the complete end-to-end WALLET_TRANSACTION notification flow works correctly with the type conversion.
    *   `[âœ…]` 50.g. `[LINT]` Run the linter for `packages/store/src/notificationStore.ts` and `packages/store/src/notificationStore.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 50.h. `[CRITERIA]` All requirements are met: (1) WALLET_TRANSACTION notifications with `newBalance` as a NUMBER (what the backend sends) are correctly processed (validation accepts number, converts to string for the wallet handler), (2) Invalid notifications (null/undefined/non-numeric `newBalance`) are correctly rejected and not added to the notification list, (3) The wallet handler receives `newBalance` as a string as required by its signature, (4) All tests pass, including unit tests that verify type validation and integration tests that verify the complete end-to-end notification flow, (5) The file is lint-clean, (6) Wallet transactions update wallet balances correctly.

*   `[âœ…]` 51. **`[STORE]` Fix document_started and document_completed Tracking in dialecticStore.documents.ts**
    *   `[âœ…]` 51.a. `[DEPS]` The `handleDocumentStartedLogic` function in `dialecticStore.documents.ts` (lines 500-508) incorrectly requires `latestRenderedResourceId` for documents that need rendering. However, `document_started` events are sent at the START of EXECUTE jobs (line 209 in `processSimpleJob.ts`) BEFORE any rendering occurs, so `latestRenderedResourceId` doesn't exist yet. The backend never sends `latestRenderedResourceId` in `document_started` notifications (it's only sent in `render_completed` events). The handler must track document lifecycle by: (1) identifying which models (`modelId`) are working on which `document_key` values, (2) initializing document tracking in `progress.documents[documentKeyValue]` with status `'generating'` when `document_started` arrives, (3) tracking the status of each `document_key` for each `modelId` throughout the lifecycle, (4) only using `latestRenderedResourceId` when it becomes available from `render_completed` events (for multi-chunk documents that have been rendered). The fix requires: (1) removing the `latestRenderedResourceId` requirement check (lines 500-508) that causes `document_started` events to be ignored, (2) initializing document tracking with status `'generating'` and basic metadata (`job_id`, `modelId`, `stepKey`) when `document_started` arrives, (3) deferring version tracking (`upsertStageDocumentVersionLogic`, `ensureStageDocumentContentLogic`) until `latestRenderedResourceId` is actually available (from `render_completed` events), (4) ensuring `document_completed` can find documents that were initialized by `document_started` even when `latestRenderedResourceId` was not provided initially. The type definition `DocumentStartedPayload` allows `latestRenderedResourceId?: string | null`, so the handler must work correctly when it's missing - no defaults, fallbacks, or placeholders are needed, just proper lifecycle tracking.
    *   `[âœ…]` 51.b. `[TEST-UNIT]` **RED**: In `packages/store/src/dialecticStore.documents.test.ts`, write tests that reproduce the document_started and document_completed tracking issues.
        *   `[âœ…]` 51.b.i. Create a test case that mocks a `document_started` event WITHOUT `latestRenderedResourceId` (matching actual backend behavior from `processSimpleJob.ts` line 209-217) for a document that requires rendering (e.g., `document_key: 'business_case'`). Assert that the document IS tracked in `progress.documents[documentKeyValue]` with status `'generating'`. This test must fail because the handler currently ignores the event when `latestRenderedResourceId` is missing.
        *   `[âœ…]` 51.b.ii. Create a test case that mocks a `document_started` event WITHOUT `latestRenderedResourceId` for a document that requires rendering, followed by a `render_completed` event WITH `latestRenderedResourceId`, and assert that the document's `latestRenderedResourceId` is updated to the actual rendered resource ID. This test must fail initially but should pass after the fix.
        *   `[âœ…]` 51.b.iii. Create a test case that mocks a `document_started` event WITHOUT `latestRenderedResourceId` for a document that requires rendering, followed by a `document_completed` event, and assert that the document IS found and updated to `'completed'` status. This test must fail because `document_started` currently ignores the event, so `document_completed` can't find the document.
        *   `[âœ…]` 51.b.iv. Create a test case that mocks a `document_started` event WITH `latestRenderedResourceId` (if the backend ever sends it), and assert that it uses the provided value. This test verifies backward compatibility if the backend behavior changes.
    *   `[âœ…]` 51.c. `[STORE]` **GREEN**: In `packages/store/src/dialecticStore.documents.ts`, fix `handleDocumentStartedLogic` to initialize document tracking without requiring `latestRenderedResourceId`.
        *   `[âœ…]` 51.c.i. Remove the validation block that requires `latestRenderedResourceId` for documents that need rendering (lines 499-509). Delete the entire `if (requiresRendering) { ... }` block that checks and returns early when `latestRenderedResourceId` is missing. This allows `document_started` events to initialize tracking even when `latestRenderedResourceId` is not provided.
        *   `[âœ…]` 51.c.ii. Update the document initialization logic (lines 560-589) to handle optional `latestRenderedResourceId` correctly. For documents that require rendering: (1) Always initialize document tracking in `progress.documents[documentKeyValue]` with status `'generating'`, `job_id`, `modelId`, and `stepKey` when `document_started` arrives, regardless of whether `latestRenderedResourceId` is provided. (2) Only call version tracking functions (`upsertStageDocumentVersionLogic`, `ensureStageDocumentContentLogic`) when `latestRenderedResourceId` is actually available (check `typeof latestRenderedResourceId === 'string' && latestRenderedResourceId.length > 0`). (3) When `latestRenderedResourceId` is missing, create a descriptor with status `'generating'` and basic tracking info, but defer version tracking until `render_completed` provides the actual `latestRenderedResourceId`. This allows document lifecycle tracking to begin immediately, and `render_completed` events will provide the `latestRenderedResourceId` and trigger version tracking at that time.
        *   `[âœ…]` 51.c.iii. Ensure that when `latestRenderedResourceId` is missing, the document descriptor is still created with status `'generating'` and the basic tracking information (`job_id`, `modelId`, `stepKey`, `document_key`), but version tracking (`upsertStageDocumentVersionLogic`, `ensureStageDocumentContentLogic`) is only called when `latestRenderedResourceId` is actually provided. This allows document tracking to be initialized immediately so the system knows: (1) which models are working on which document_keys, (2) the status of each document_key for each model, (3) that a notification matches a document_key a model is working on. Version tracking (which requires `latestRenderedResourceId`) will be set up when `render_completed` events arrive with the actual rendered resource ID.
        *   `[âœ…]` 51.c.iv. Ensure the existing logic for documents that don't require rendering (lines 528-558) remains unchanged - it already handles missing `latestRenderedResourceId` correctly.
    *   `[âœ…]` 51.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 51.b and ensure they now pass. Also verify that existing dialectic store tests still pass (update any tests that expect `document_started` to require `latestRenderedResourceId` to allow it to be optional).
    *   `[âœ…]` 51.e. `[TEST-INT]` **RED**: In `packages/store/src/dialecticStore.notifications.integration.test.ts` or create a new integration test file, write tests that verify end-to-end document lifecycle notification handling.
        *   `[âœ…]` 51.e.i. Create a test case that simulates a `document_started` notification WITHOUT `latestRenderedResourceId` for a document that requires rendering, followed by a `render_completed` notification WITH `latestRenderedResourceId`, and verify that: (a) the document IS tracked after `document_started`, (b) the document's `latestRenderedResourceId` is updated after `render_completed`, (c) a subsequent `document_completed` notification can find and update the document. This test must fail initially if the document tracking fix is not complete.
        *   `[âœ…]` 51.e.ii. Create a test case that simulates a `document_started` notification WITHOUT `latestRenderedResourceId`, followed immediately by a `document_completed` notification, and verify that the document IS found and updated to `'completed'` status. This test verifies the cascading failure is resolved.
    *   `[âœ…]` 51.f. `[TEST-INT]` **GREEN**: Re-run all tests from step 51.e and ensure they now pass. This verifies that the complete end-to-end document lifecycle notification flow works correctly.
    *   `[âœ…]` 51.g. `[LINT]` Run the linter for `packages/store/src/dialecticStore.documents.ts` and `packages/store/src/dialecticStore.documents.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 51.h. `[CRITERIA]` All requirements are met: (1) `document_started` notifications WITHOUT `latestRenderedResourceId` correctly initialize document tracking, (2) `document_completed` notifications can find documents that were initialized by `document_started` even when `latestRenderedResourceId` was not provided initially, (3) `render_completed` notifications correctly update `latestRenderedResourceId` when it becomes available, (4) Document tracking works correctly for both documents that require rendering and documents that don't require rendering, (5) No valid notifications are rejected due to missing optional fields, (6) Invalid notifications are correctly rejected and not processed, (7) All tests pass, including unit tests that verify document tracking initialization and integration tests that verify the complete end-to-end document lifecycle flow, (8) The file is lint-clean, (9) Document lifecycle notifications correctly track document state from start to completion.

*   `[âœ…]` 52. **`[BE]` Fix task_isolator to Remove Unnecessary Content Downloads for Planning**
    *   `[âœ…]` 52.a. `[DEPS]` The `findSourceDocuments` function in `supabase/functions/dialectic-worker/task_isolator.ts` currently downloads document content from storage (lines 472-494) and passes it to mapper functions (`mapContributionToSourceDocument`, `mapResourceToSourceDocument`, `mapFeedbackToSourceDocument`). The mappers require a `content` parameter they don't use - they only pass it through to satisfy the `SourceDocument` type requirement. **THE PROBLEM**: Planners (`planPerSourceDocument`, `planAllToOne`, etc.) only use metadata from `SourceDocument` objects (id, contribution_type, document_relationships, model_name, attempt_count) - they never access `doc.content`. The content is only needed later in `executeModelCallAndSave.gatherArtifacts()` when constructing the actual API call. Downloading potentially large document content during planning is unnecessary I/O that wastes resources and slows down job decomposition. **THE FIX**: (1) Remove the download logic from `findSourceDocuments` (lines 472-494 that download from storage and decode content), (2) Remove the `downloadFromStorage` parameter from `findSourceDocuments` function signature (line 255), (3) Update mapper functions to set `content: ''` internally instead of requiring it as a parameter, (4) Update `planComplexStage` call site (line 559-564) to not pass `deps.downloadFromStorage`, (5) Update all tests that expect content to be downloaded to instead expect empty string content. The `SourceDocument` type still requires `content: string` (for type compatibility), but it will be an empty string during planning - content is fetched later when actually needed.
    *   `[âœ…]` 52.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/task_isolator.findSourceDocuments.test.ts` or create a new test file, write tests that verify `findSourceDocuments` does NOT download content from storage.
        *   `[âœ…]` 52.b.i. Create a test case that mocks `downloadFromStorage` to throw an error or return invalid data, calls `findSourceDocuments` with valid input rules, and asserts that the function DOES NOT call `downloadFromStorage` at all. Assert that `SourceDocument[]` objects are returned with `content: ''` (empty string), not downloaded content. This test must fail because the function currently downloads content from storage.
        *   `[âœ…]` 52.b.ii. Create a test case that calls `findSourceDocuments` with valid input rules and verifies that returned `SourceDocument[]` objects have all required metadata fields (id, contribution_type, document_relationships, model_name, attempt_count, etc.) but `content` is an empty string. Assert that no storage download operations occur. This test must fail because the function currently downloads content.
        *   `[âœ…]` 52.b.iii. Create a test case that verifies `findSourceDocuments` works correctly for all input types (`type: "document"`, `type: "header_context"`, `type: "feedback"`). Assert that all returned `SourceDocument[]` objects have `content: ''` regardless of input type. This test verifies the fix works for all input rule types.
    *   `[âœ…]` 52.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/task_isolator.ts`, remove content download logic from `findSourceDocuments` and update mappers to set empty content internally.
        *   `[âœ…]` 52.c.i. Remove the `downloadFromStorage` parameter from `findSourceDocuments` function signature (line 255). Delete `downloadFromStorage: (bucket: string, path: string) => Promise<DownloadStorageResult>,` from the parameter list.
        *   `[âœ…]` 52.c.ii. Remove the download logic from `findSourceDocuments` (lines 472-494). Delete the entire `Promise.all` block that downloads content from storage (`await downloadFromStorage(record.storage_bucket, fullPath)`) and decodes it (`new TextDecoder().decode(data ?? new ArrayBuffer(0))`). Replace it with direct mapper calls that don't require content.
        *   `[âœ…]` 52.c.iii. Update `mapContributionToSourceDocument` function signature (line 41) to remove the `content: string` parameter. Change `function mapContributionToSourceDocument(row: DialecticContributionRow, content: string): SourceDocument` to `function mapContributionToSourceDocument(row: DialecticContributionRow): SourceDocument`. Set `content: ''` directly in the return statement (line 45): change `return { ...rest, content, ... }` to `return { ...rest, content: '', ... }`.
        *   `[âœ…]` 52.c.iv. Update `mapResourceToSourceDocument` function signature (line 48) to remove the `content: string` parameter. Change `function mapResourceToSourceDocument(row: DialecticProjectResourceRow, content: string): SourceDocument` to `function mapResourceToSourceDocument(row: DialecticProjectResourceRow): SourceDocument`. Set `content: ''` directly in the return object (line 72): change `content: content,` to `content: '',`.
        *   `[âœ…]` 52.c.v. Update `mapFeedbackToSourceDocument` function signature (line 93) to remove the `content: string` parameter. Change `function mapFeedbackToSourceDocument(row: DialecticFeedbackRow, content: string): SourceDocument` to `function mapFeedbackToSourceDocument(row: DialecticFeedbackRow): SourceDocument`. Set `content: ''` directly in the return object (line 108): change `content: content,` to `content: '',`.
        *   `[âœ…]` 52.c.vi. Update the mapper call sites in `findSourceDocuments` (lines 485-490) to call mappers without the content parameter. Change `mapFeedbackToSourceDocument(record, content)` to `mapFeedbackToSourceDocument(record)`, `mapResourceToSourceDocument(record, content)` to `mapResourceToSourceDocument(record)`, and `mapContributionToSourceDocument(record, content)` to `mapContributionToSourceDocument(record)`. Remove the `content` variable declaration (line 483) since it's no longer needed.
        *   `[âœ…]` 52.c.vii. Update `planComplexStage` call site (line 559-564) to remove the `deps.downloadFromStorage` argument. Change `let sourceDocuments = await findSourceDocuments(dbClient, parentJob, recipeStep.inputs_required, deps.downloadFromStorage);` to `let sourceDocuments = await findSourceDocuments(dbClient, parentJob, recipeStep.inputs_required);`.
        *   `[âœ…]` 52.c.viii. Add a comment to each mapper function explaining why content is empty: `// Note: content is set to empty string because planners only need metadata, not content. Content is fetched later in executeModelCallAndSave.gatherArtifacts() when constructing the API call.`
    *   `[âœ…]` 52.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 52.b and ensure they now pass. Also verify that existing `task_isolator` tests still pass (update any tests that expect content to be downloaded or non-empty to expect empty string instead).
        *   `[âœ…]` 52.d.i. Update existing tests in `task_isolator.findSourceDocuments.test.ts` that verify content downloads. These tests currently assert that `downloadFromStorage` is called and content is downloaded. Update them to assert that `downloadFromStorage` is NOT called and returned `SourceDocument[]` objects have `content: ''`.
        *   `[âœ…]` 52.d.ii. Update existing tests in `task_isolator.planComplexStage.test.ts` that verify source documents are passed to planners. These tests currently may assert that `SourceDocument[]` objects have non-empty content. Update them to assert that `content: ''` is acceptable (planners don't use content anyway). The test at line 753 (`assertEquals(receivedDocs[0].content, '')`) should continue to pass - it already expects empty string.
        *   `[âœ…]` 52.d.iii. Verify that tests for planner functions (`planPerSourceDocument`, `planAllToOne`, etc.) still pass. These tests should be unaffected because planners don't use `doc.content` - they only use metadata fields.
    *   `[âœ…]` 52.e. `[TEST-INT]` **RED**: In `supabase/functions/dialectic-worker/task_isolator.planComplexStage.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `planComplexStage` calls `findSourceDocuments` without downloading content, planners receive `SourceDocument[]` with empty content, and child jobs are created correctly.
        *   `[âœ…]` 52.e.i. Create a test case that sets up a PLAN job with input rules, mocks `findSourceDocuments` to return `SourceDocument[]` with `content: ''`, calls `planComplexStage`, and verifies that: (1) `findSourceDocuments` is called without `downloadFromStorage` parameter, (2) Planners receive `SourceDocument[]` objects with all required metadata but `content: ''`, (3) Child job payloads are created correctly (planners don't need content to decompose jobs). This test must fail initially if the fix is not implemented, but should pass after step 52.c is complete.
        *   `[âœ…]` 52.e.ii. Create a test case that verifies `executeModelCallAndSave` can still gather content correctly when it needs it (via `gatherArtifacts()`). This test verifies that removing content from planning doesn't break content gathering during execution. The test should mock an EXECUTE job, call `executeModelCallAndSave`, and verify that `gatherArtifacts()` downloads content from storage when constructing the API call.
    *   `[âœ…]` 52.f. `[TEST-INT]` **GREEN**: Re-run the test from step 52.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: planning doesn't download content (faster, more efficient), but execution still gathers content when needed.
    *   `[âœ…]` 52.g. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/task_isolator.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 52.h. `[CRITERIA]` All requirements are met: (1) `findSourceDocuments` does NOT download content from storage during planning, (2) Mapper functions set `content: ''` internally without requiring it as a parameter, (3) `planComplexStage` calls `findSourceDocuments` without passing `downloadFromStorage`, (4) Planners receive `SourceDocument[]` objects with all required metadata (id, contribution_type, document_relationships, model_name, attempt_count) but `content: ''`, (5) `executeModelCallAndSave.gatherArtifacts()` still downloads content when needed during execution (content gathering is not broken), (6) All tests pass, including unit tests that verify no content downloads during planning and integration tests that verify the complete end-to-end flow, (7) The file is lint-clean, (8) Planning is more efficient (no unnecessary I/O) while execution still has access to content when needed.

*   `[âœ…]` 53. **`[BE]` Fix gatherInputsForStage to Query dialectic_project_resources for Finished Rendered Documents**
    *   `[âœ…]` 53.a. `[DEPS]` The `gatherInputsForStage` function in `supabase/functions/_shared/prompt-assembler/gatherInputsForStage.ts` currently queries ONLY `dialectic_contributions` for document-type inputs (lines 105-186). **THE PROBLEM**: In the target architecture, finished rendered documents are stored in `dialectic_project_resources` with `resource_type = 'rendered_document'`, while `dialectic_contributions` stores raw model output chunks. The function currently queries contributions expecting finished documents, but finished documents are saved to resources by `document_renderer.renderDocument()`. This breaks document retrieval for subsequent stages that need finished documents as inputs. **THE FIX**: The function must query `dialectic_project_resources` for document-type inputs: (1) Query `dialectic_project_resources` for finished rendered documents (`resource_type = 'rendered_document'`) matching `session_id`, `iteration_number`, `stage_slug` (from `rule.slug`), and `file_name` pattern matching `document_key` (extracted via path deconstruction if `rule.document_key` is provided), (2) If no resources are found and the rule is required, throw an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks, defaults, or silent healing), (3) Download content from the resource's `storage_path` and `file_name`, (4) Keep the existing contributions query ONLY for `header_context` and other intermediate artifact types (these are correctly stored in contributions, not a fallback). The function must use `deconstructStoragePath` from `supabase/functions/_shared/utils/path_deconstructor.ts` to extract `documentKey` from `file_name` when matching against `rule.document_key`. Documents that should be rendered MUST be in resources - if they're not found, that indicates the rendering step failed or was not executed, which is an error condition that must be reported, not silently worked around.
    *   `[âœ…]` 53.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/gatherInputsForStage.test.ts` or create a new test file, write tests that verify `gatherInputsForStage` queries `dialectic_project_resources` for finished rendered documents.
        *   `[âœ…]` 53.b.i. Create a test case that mocks a database with a finished rendered document in `dialectic_project_resources` (with `resource_type = 'rendered_document'`, `session_id`, `iteration_number`, `stage_slug`, and `file_name` containing a `document_key`). Call `gatherInputsForStage` with an input rule of type `'document'` matching the document. Assert that: (1) The function queries `dialectic_project_resources` first, (2) The function finds the rendered document, (3) The function downloads content from the resource's `storage_path` and `file_name`, (4) The function adds the document to `gatheredContext.sourceDocuments` with the correct content. This test must fail because the function currently only queries `dialectic_contributions`.
        *   `[âœ…]` 53.b.ii. Create a test case that mocks a database with BOTH a finished rendered document in `dialectic_project_resources` AND a raw chunk in `dialectic_contributions` for the same `document_key` and `stage_slug`. Call `gatherInputsForStage` with an input rule matching the document. Assert that: (1) The function prefers the resource over the contribution (queries resources first), (2) The function uses the rendered document from resources, NOT the raw chunk from contributions, (3) Only one document is added to `gatheredContext.sourceDocuments` (no duplicates). This test must fail because the function currently only queries contributions and doesn't check resources.
        *   `[âœ…]` 53.b.iii. Create a test case that mocks a database with NO finished rendered document in `dialectic_project_resources` for a required document input rule. Call `gatherInputsForStage` with the input rule. Assert that: (1) The function queries resources first and finds nothing, (2) The function throws an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks), (3) The function does NOT query contributions (finished documents must be in resources, not contributions). This test verifies that missing rendered documents are treated as errors, not silently worked around.
        *   `[âœ…]` 53.b.iv. Create a test case that verifies `gatherInputsForStage` continues to query `dialectic_contributions` for `header_context` type inputs (these are correctly stored in contributions, not resources). Assert that header_context inputs are found in contributions and NOT queried from resources. This test should pass initially and must continue to pass.
    *   `[âœ…]` 53.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/gatherInputsForStage.ts`, add query to `dialectic_project_resources` for finished rendered documents.
        *   `[âœ…]` 53.c.i. Before the existing contributions query for document-type inputs (before line 105), add a query to `dialectic_project_resources` for finished rendered documents. The query should: (1) Filter by `resource_type = 'rendered_document'`, (2) Filter by `session_id = session.id`, (3) Filter by `iteration_number = iterationNumber`, (4) Filter by `stage_slug = rule.slug` (use `rule.slug` directly, resources table uses `stage_slug` column), (5) If `rule.document_key` is provided, use `deconstructStoragePath` to extract `documentKey` from each resource's `file_name` and match against `rule.document_key`, (6) Select all columns (`select('*')`) to get full resource records including `storage_bucket`, `storage_path`, and `file_name`.
        *   `[âœ…]` 53.c.ii. After querying resources, check if resources were found. If resources are found: (1) Use the latest version of the resource, (2) Download content from the resource's `storage_path` and `file_name` using `downloadFromStorageFn`, (3) Add the document to `gatheredContext.sourceDocuments` with the downloaded content. If no resources are found and the rule is required: (1) Throw an error immediately: `new Error(`Required rendered document for stage '${displayName}' with document_key '${rule.document_key}' was not found in dialectic_project_resources. This indicates the document was not rendered or the rendering step failed.`)` (fail loud and hard - no fallbacks, defaults, or silent healing), (2) Do NOT query contributions for document-type inputs (finished documents must be in resources).
        *   `[âœ…]` 53.c.iii. Import `deconstructStoragePath` from `supabase/functions/_shared/utils/path_deconstructor.ts` at the top of the file. Use it to extract `documentKey` from resource `file_name` when matching against `rule.document_key`.
        *   `[âœ…]` 53.c.iv. Ensure error handling: if the resources query fails with a database error, throw an error immediately indicating the database query failed (fail loud and hard - no fallbacks). If the resources query succeeds but no resources are found for a required rule, throw an error immediately (fail loud and hard - no fallbacks). Documents that should be rendered MUST be in resources - if they're not found, that indicates the rendering step failed or was not executed, which is an error condition.
        *   `[âœ…]` 53.c.v. Ensure the existing contributions query logic (lines 105-186) remains unchanged ONLY for `header_context` type inputs (these are correctly stored in contributions, not a fallback). Remove the contributions query for document-type inputs entirely - finished documents must be in resources, and if they're not found, that's an error.
        *   `[âœ…]` 53.c.vi. Add logging to track which table was queried and which records were found: log when resources are queried and found, log when resources query returns empty (before throwing error), log when contributions are queried (for header_context only). This helps diagnose issues in production.
    *   `[âœ…]` 53.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 53.b and ensure they now pass. Also verify that existing `gatherInputsForStage` tests still pass (update any tests that expect only contributions to handle both resources and contributions).
        *   `[âœ…]` 53.d.i. Update existing tests in `gatherInputsForStage.test.ts` that test document-type inputs. These tests currently expect only contributions to be queried. Update them to verify resources are queried, and if resources are not found for required rules, errors are thrown (no fallback to contributions).
        *   `[âœ…]` 53.d.ii. Verify that tests for `header_context` type inputs still pass (these should continue to query only contributions, not resources).
    *   `[âœ…]` 53.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/gatherInputsForStage.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `document_renderer.renderDocument()` saves a rendered document to `dialectic_project_resources`, and `gatherInputsForStage` retrieves it for a subsequent stage.
        *   `[âœ…]` 53.e.i. Create a test case that: (1) Sets up a stage with a document that gets rendered (via `document_renderer.renderDocument()`), (2) Verifies the rendered document is saved to `dialectic_project_resources` with `resource_type = 'rendered_document'`, (3) Calls `gatherInputsForStage` for a subsequent stage that requires the document as input, (4) Asserts that `gatherInputsForStage` finds and retrieves the document from `dialectic_project_resources` (not from contributions). This test must fail initially if the fix is not implemented, but should pass after step 53.c is complete.
    *   `[âœ…]` 53.f. `[TEST-INT]` **GREEN**: Re-run the test from step 53.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: rendered documents are saved to resources, and `gatherInputsForStage` retrieves them from resources for subsequent stages.
    *   `[âœ…]` 53.g. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/gatherInputsForStage.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 53.h. `[CRITERIA]` All requirements are met: (1) `gatherInputsForStage` queries `dialectic_project_resources` for finished rendered documents when input rule type is `'document'`, (2) `gatherInputsForStage` throws an error immediately if required rendered documents are not found in resources (fail loud and hard - no fallbacks, defaults, or silent healing), (3) `gatherInputsForStage` does NOT query contributions for document-type inputs (finished documents must be in resources), (4) `gatherInputsForStage` continues to query contributions for `header_context` and other intermediate artifact types (these are correctly stored in contributions, not a fallback), (5) `gatherInputsForStage` uses `deconstructStoragePath` to extract `documentKey` from `file_name` when matching against `rule.document_key`, (6) Error handling throws errors immediately when resources are not found (no fallbacks), (7) All tests pass, including unit tests that verify resources are queried and errors are thrown when missing, and integration tests that verify the complete end-to-end flow, (8) The file is lint-clean, (9) Finished rendered documents are correctly retrieved from `dialectic_project_resources` for subsequent stages, and missing documents are treated as errors, not silently worked around.

*   `[âœ…]` 54. **`[BE]` Fix task_isolator.findSourceDocuments to Query dialectic_project_resources for Finished Documents**
    *   `[âœ…]` 54.a. `[DEPS]` The `findSourceDocuments` function in `supabase/functions/dialectic-worker/task_isolator.ts` currently queries BOTH `dialectic_project_resources` (lines 296-333) AND `dialectic_contributions` (lines 336-358) for document-type inputs, then combines results from both sources (line 361: `const allCandidates = [...resourceCandidates, ...contributionCandidates]`). **THE PROBLEM**: In the target architecture, finished rendered documents are stored in `dialectic_project_resources`, while `dialectic_contributions` stores raw model output chunks. Querying both tables and combining results is inefficient and can return duplicate or conflicting documents. The function should query resources for finished documents and throw errors if they're not found, not fall back to contributions. **THE FIX**: The function must fix the query strategy for document-type inputs: (1) Query `dialectic_project_resources` for finished rendered documents (`resource_type = 'rendered_document'`), (2) If resources are found and match the input rule (after filtering by `document_key`), use resources and skip the contributions query entirely, (3) If no resources are found, throw an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks, defaults, or silent healing), (4) Remove the combination logic (`[...resourceCandidates, ...contributionCandidates]`) and the contributions query for document-type inputs entirely, (5) Keep the existing contributions query for all json type inputs (these are correctly stored in contributions, not a fallback). The function must preserve all existing filtering, deduplication, and selection logic for resources. Documents that should be rendered MUST be in resources - if they're not found, that indicates the rendering step failed or was not executed, which is an error condition that must be reported, not silently worked around.
    *   `[âœ…]` 54.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/task_isolator.findSourceDocuments.test.ts` or create a new test file, write tests that verify `findSourceDocuments` prefers resources over contributions for finished documents.
        *   `[âœ…]` 54.b.i. Create a test case that mocks a database with a finished rendered document in `dialectic_project_resources` (with `resource_type = 'rendered_document'`, matching `session_id`, `iteration_number`, `stage_slug`, and `document_key`). Call `findSourceDocuments` with an input rule of type `'document'` matching the document. Assert that: (1) The function queries `dialectic_project_resources` first, (2) The function finds the rendered document, (3) The function does NOT query `dialectic_contributions` (resources query is sufficient), (4) The function returns the resource as a `SourceDocument`. This test must fail because the function currently queries both tables and combines results.
        *   `[âœ…]` 54.b.ii. Create a test case that mocks a database with BOTH a finished rendered document in `dialectic_project_resources` AND a raw chunk in `dialectic_contributions` for the same `document_key` and `stage_slug`. Call `findSourceDocuments` with an input rule matching the document. Assert that: (1) The function queries resources first and finds the rendered document, (2) The function does NOT query contributions (resources are sufficient), (3) The function returns ONLY the resource, NOT the contribution (no duplicates, resources take precedence). This test must fail because the function currently queries both and combines results.
        *   `[âœ…]` 54.b.iii. Create a test case that mocks a database with NO finished rendered document in `dialectic_project_resources` for a required document input rule. Call `findSourceDocuments` with the input rule. Assert that: (1) The function queries resources first and finds nothing, (2) The function throws an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks), (3) The function does NOT query contributions (finished documents must be in resources, not contributions). This test verifies that missing rendered documents are treated as errors, not silently worked around.
        *   `[âœ…]` 54.b.iv. Create a test case that verifies `findSourceDocuments` continues to query `dialectic_contributions` for `header_context` type inputs (these are correctly stored in contributions, not resources). Assert that header_context inputs are found in contributions and NOT queried from resources. This test should pass initially and must continue to pass.
    *   `[âœ…]` 54.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/task_isolator.ts`, optimize `findSourceDocuments` to prefer resources over contributions for finished documents.
        *   `[âœ…]` 54.c.i. In the `case 'document':` block (lines 294-368), modify the query strategy: (1) Query `dialectic_project_resources` (lines 296-333, keep existing query logic), (2) After filtering and selecting resources (lines 326-333), check if `resourceCandidates.length > 0`, (3) If resources are found, set `sourceRecords = selectRecordsForRule(dedupedResources, allowMultipleMatches, usedRecordKeys)` where `dedupedResources = dedupeByFileName(resourceCandidates)`, (4) If no resources are found (`resourceCandidates.length === 0`), throw an error immediately: `throw new Error(\`Required rendered document for input rule type 'document' with stage '${stageSlugCandidate}' and document_key '${rule.document_key || 'unspecified'}' was not found in dialectic_project_resources. This indicates the document was not rendered or the rendering step failed.\`)` (fail loud and hard - no fallbacks, defaults, or silent healing), (5) Remove the contributions query entirely (lines 336-358) for document-type inputs - do not execute it.
        *   `[âœ…]` 54.c.ii. Remove the combination logic (line 361: `const allCandidates = [...resourceCandidates, ...contributionCandidates]`) and the subsequent deduplication/selection on the combined set (lines 363-367). Replace with: if resources are found, use resources exclusively; if no resources are found, throw an error immediately (no fallbacks).
        *   `[âœ…]` 54.c.iii. Ensure all existing filtering, deduplication, and selection logic for resources (lines 326-333) is preserved. The fix only removes the contributions query for document-type inputs and adds error handling when resources are not found.
        *   `[âœ…]` 54.c.iv. Remove the contributions query logic (lines 336-358) for document-type inputs entirely. Finished documents must be in resources, and if they're not found, that's an error. Keep the contributions query ONLY for `header_context` type inputs (these are correctly stored in contributions, not a fallback).
        *   `[âœ…]` 54.c.v. Ensure the existing `header_context` case (lines 370-408) remains unchanged - it correctly queries only contributions, not resources.
        *   `[âœ…]` 54.c.vi. Add logging to track which table was queried and which records were used: log when resources are queried and found, log when resources query returns empty (before throwing error), log when contributions are queried (for header_context only). This helps diagnose issues in production.
    *   `[âœ…]` 54.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 54.b and ensure they now pass. Also verify that existing `task_isolator.findSourceDocuments` tests still pass (update any tests that expect both tables to be queried to handle the optimized strategy).
        *   `[âœ…]` 54.d.i. Update existing tests in `task_isolator.findSourceDocuments.test.ts` that test document-type inputs. These tests currently expect both resources and contributions to be queried and combined. Update them to verify resources are queried, and if resources are not found, errors are thrown (no fallback to contributions).
        *   `[âœ…]` 54.d.ii. Verify that tests for `header_context` type inputs still pass (these should continue to query only contributions, not resources).
    *   `[âœ…]` 54.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/task_isolator.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `document_renderer.renderDocument()` saves a rendered document to `dialectic_project_resources`, and `findSourceDocuments` retrieves it for planning.
        *   `[âœ…]` 54.e.i. Create a test case that: (1) Sets up a stage with a document that gets rendered (via `document_renderer.renderDocument()`), (2) Verifies the rendered document is saved to `dialectic_project_resources` with `resource_type = 'rendered_document'`, (3) Calls `findSourceDocuments` for a subsequent stage that requires the document as input, (4) Asserts that `findSourceDocuments` finds and retrieves the document from `dialectic_project_resources` (not from contributions), (5) Asserts that `findSourceDocuments` does NOT query contributions when resources are found. This test must fail initially if the fix is not implemented, but should pass after step 54.c is complete.
    *   `[âœ…]` 54.f. `[TEST-INT]` **GREEN**: Re-run the test from step 54.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: rendered documents are saved to resources, and `findSourceDocuments` retrieves them from resources for planning.
    *   `[âœ…]` 54.g. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/task_isolator.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 54.h. `[CRITERIA]` All requirements are met: (1) `findSourceDocuments` queries `dialectic_project_resources` for finished rendered documents when input rule type is `'document'`, (2) `findSourceDocuments` uses resources exclusively when found (does not query contributions), (3) `findSourceDocuments` throws an error immediately if required rendered documents are not found in resources (fail loud and hard - no fallbacks, defaults, or silent healing), (4) `findSourceDocuments` does NOT query contributions for document-type inputs (finished documents must be in resources), (5) `findSourceDocuments` continues to query contributions for `header_context` type inputs (these are correctly stored in contributions, not a fallback), (6) All existing filtering, deduplication, and selection logic is preserved, (7) Error handling throws errors immediately when resources are not found (no fallbacks), (8) All tests pass, including unit tests that verify resources are queried and errors are thrown when missing, and integration tests that verify the complete end-to-end flow, (9) The file is lint-clean, (10) Query strategy is correct (no unnecessary contributions queries for finished documents, missing documents are treated as errors).

*   `[âœ…]` 55. **`[BE]` Fix executeModelCallAndSave.gatherArtifacts to Query dialectic_project_resources for Finished Documents**
    *   `[âœ…]` 55.a. `[DEPS]` The `gatherArtifacts` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 111-255) currently queries BOTH `dialectic_project_resources` (lines 175-208) AND `dialectic_contributions` (lines 139-173) for document-type inputs, then deduplicates by `id` across both sources (lines 247-254). **THE PROBLEM**: In the target architecture, finished rendered documents are stored in `dialectic_project_resources`, while `dialectic_contributions` stores raw model output chunks. Querying both tables and deduplicating is inefficient and can return duplicate or conflicting documents. The function should query resources for finished documents and throw errors if they're not found, not fall back to contributions. **THE FIX**: The function must fix the query strategy for document-type inputs: (1) Query `dialectic_project_resources` for finished rendered documents (`resource_type = 'rendered_document'`), (2) If resources are found and match the input rule (after filtering by `document_key` via path deconstruction), use resources and skip the contributions query entirely, (3) If no resources are found, throw an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks, defaults, or silent healing), (4) Remove the contributions query for document-type inputs entirely (lines 139-173), (5) Remove the deduplication logic across both sources (lines 247-254) since contributions are no longer queried for documents, (6) Keep the existing contributions query ONLY for intermediate artifacts that are correctly stored in contributions (not a fallback). The function must preserve all existing filtering and path deconstruction logic for resources. Documents that should be rendered MUST be in resources - if they're not found, that indicates the rendering step failed or was not executed, which is an error condition that must be reported, not silently worked around.
    *   `[âœ…]` 55.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.gatherArtifacts.test.ts` or create a new test file, write tests that verify `gatherArtifacts` prefers resources over contributions for finished documents.
        *   `[âœ…]` 55.b.i. Create a test case that mocks a database with a finished rendered document in `dialectic_project_resources` (with `resource_type = 'rendered_document'`, matching `session_id`, `iteration_number`, `stage_slug`, and `document_key` extracted via path deconstruction). Call `gatherArtifacts` with an input rule of type `'document'` matching the document. Assert that: (1) The function queries `dialectic_project_resources` first, (2) The function finds the rendered document, (3) The function does NOT query `dialectic_contributions` (resources query is sufficient), (4) The function returns the resource in the `gathered` array. This test must fail because the function currently queries both tables and deduplicates.
        *   `[âœ…]` 55.b.ii. Create a test case that mocks a database with BOTH a finished rendered document in `dialectic_project_resources` AND a raw chunk in `dialectic_contributions` for the same `document_key` and `stage_slug`. Call `gatherArtifacts` with an input rule matching the document. Assert that: (1) The function queries resources first and finds the rendered document, (2) The function does NOT query contributions (resources are sufficient), (3) The function returns ONLY the resource, NOT the contribution (no duplicates, resources take precedence). This test must fail because the function currently queries both and deduplicates.
        *   `[âœ…]` 55.b.iii. Create a test case that mocks a database with NO finished rendered document in `dialectic_project_resources` for a required document input rule. Call `gatherArtifacts` with the input rule. Assert that: (1) The function queries resources first and finds nothing, (2) The function throws an error immediately indicating that the required rendered document was not found (fail loud and hard - no fallbacks), (3) The function does NOT query contributions (finished documents must be in resources, not contributions). This test verifies that missing rendered documents are treated as errors, not silently worked around.
        *   `[âœ…]` 55.b.iv. Create a test case that verifies `gatherArtifacts` continues to query `dialectic_contributions` for intermediate artifacts (if any exist). Assert that intermediate artifacts are found in contributions and NOT queried from resources. This test verifies the function still works for non-document inputs.
    *   `[âœ…]` 55.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, optimize `gatherArtifacts` to prefer resources over contributions for finished documents.
        *   `[âœ…]` 55.c.i. In the `if (rType === 'document')` block (lines 138-209), modify the query strategy: (1) Query `dialectic_project_resources` (lines 175-208) and add `.eq('resource_type', 'rendered_document')` to the query chain after line 179 to filter for rendered documents only (the current code does not filter by resource_type, which is a bug), (2) After filtering and selecting the latest resource (lines 183-207), check if a resource was found (`if (latest && isRecord(latest))`), (3) If a resource is found, add it to `gathered` array (lines 203-205), (4) If no resource is found, throw an error immediately: `throw new Error(\`Required rendered document for input rule type 'document' with stage '${rStage}' and document_key '${rKey}' was not found in dialectic_project_resources. This indicates the document was not rendered or the rendering step failed.\`)` (fail loud and hard - no fallbacks, defaults, or silent healing), (5) Remove the contributions query entirely (lines 139-173) for document-type inputs - do not execute it.
        *   `[âœ…]` 55.c.ii. Remove the deduplication logic across both sources (lines 247-254) since contributions are no longer queried for document-type inputs. The deduplication by `id` is still needed if multiple input rules return the same document, but the cross-source deduplication (contributions vs resources) is no longer necessary.
        *   `[âœ…]` 55.c.iii. Ensure all existing filtering and path deconstruction logic for resources (lines 183-207) is preserved. The fix only removes the contributions query for document-type inputs and adds error handling when resources are not found.
        *   `[âœ…]` 55.c.iv. Remove the contributions query logic (lines 139-173) for document-type inputs entirely. Finished documents must be in resources, and if they're not found, that's an error. Keep the contributions query ONLY for intermediate artifacts that are correctly stored in contributions (not a fallback).
        *   `[âœ…]` 55.c.v. Ensure the existing `feedback` query logic (lines 210-243) remains unchanged - it correctly queries only `dialectic_feedback`, not resources or contributions.
        *   `[âœ…]` 55.c.vi. Add logging to track which table was queried and which records were used: log when resources are queried and found, log when resources query returns empty (before throwing error), log when contributions are queried (for intermediate artifacts only). This helps diagnose issues in production.
    *   `[âœ…]` 55.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 55.b and ensure they now pass. Also verify that existing `executeModelCallAndSave` tests still pass (update any tests that expect both tables to be queried to handle the optimized strategy).
        *   `[âœ…]` 55.d.i. Update existing tests in `executeModelCallAndSave.test.ts` that test `gatherArtifacts`. These tests currently expect both resources and contributions to be queried and deduplicated. Update them to verify resources are queried, and if resources are not found, errors are thrown (no fallback to contributions).
        *   `[âœ…]` 55.d.ii. Verify that tests for feedback inputs still pass (these should continue to query only `dialectic_feedback`, not resources or contributions).
    *   `[âœ…]` 55.e. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/executeModelCallAndSave.integration.test.ts` or create a new integration test file, write a test that verifies the end-to-end flow: `document_renderer.renderDocument()` saves a rendered document to `dialectic_project_resources`, and `gatherArtifacts` retrieves it for execution.
        *   `[âœ…]` 55.e.i. Create a test case that: (1) Sets up a stage with a document that gets rendered (via `document_renderer.renderDocument()`), (2) Verifies the rendered document is saved to `dialectic_project_resources` with `resource_type = 'rendered_document'`, (3) Calls `executeModelCallAndSave` with an EXECUTE job that requires the document as input, (4) Asserts that `gatherArtifacts` finds and retrieves the document from `dialectic_project_resources` (not from contributions), (5) Asserts that `gatherArtifacts` does NOT query contributions when resources are found. This test must fail initially if the fix is not implemented, but should pass after step 55.c is complete.
    *   `[âœ…]` 55.f. `[TEST-INT]` **GREEN**: Re-run the test from step 55.e and ensure it now passes. This verifies that the complete end-to-end flow works correctly: rendered documents are saved to resources, and `gatherArtifacts` retrieves them from resources for execution.
    *   `[âœ…]` 55.g. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 55.h. `[CRITERIA]` All requirements are met: (1) `gatherArtifacts` queries `dialectic_project_resources` for finished rendered documents when input rule type is `'document'`, filtering by `resource_type = 'rendered_document'` to ensure only rendered documents are retrieved, (2) `gatherArtifacts` uses resources exclusively when found (does not query contributions), (3) `gatherArtifacts` throws an error immediately if required rendered documents are not found in resources (fail loud and hard - no fallbacks, defaults, or silent healing), (4) `gatherArtifacts` does NOT query contributions for document-type inputs (finished documents must be in resources), (5) All existing filtering and path deconstruction logic is preserved, (6) Error handling throws errors immediately when resources are not found (no fallbacks), (7) All tests pass, including unit tests that verify resources are queried with correct resource_type filter and errors are thrown when missing, and integration tests that verify the complete end-to-end flow, (8) The file is lint-clean, (9) Query strategy is correct (no unnecessary contributions queries for finished documents, missing documents are treated as errors, resource_type filter ensures only rendered documents are retrieved).