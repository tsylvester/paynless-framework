# Doc-Centric Fixes

[ ] // So that find->replace will stop unrolling my damned instructions! 

## Problem Statement
-The doc-centric refactor has introduced bugs and inconsistencies that need resolved. 

## Objectives
- Fix all bugs and integration errors from the doc-centric refactor. 

## Expected Outcome
- Generate an entire dialectic end to end using the doc-centric method.

# Instructions for Agent
*   ### 0. Command Pyramid & Modes
    *   Obey the user‚Äôs explicit instructions first, then this block, then the checklist. Do not hide behind the checklist to ignore a direct user correction.
    *   Ensure both the method and the resulting content of every task comply with this block‚Äîno deliverable is valid if it conflicts with these rules.
    *   Perform every assignment in a single turn while fully complying with this block; partial compliance is a violation even if the work ‚Äúmostly‚Äù succeeds.
    *   Failing to follow these instructions immediately triggers rework, rejected output, and systemic violations‚Äîtreat every deviation as unacceptable.
    *   The Instructions for Agent block is an absolute firewall. No conditional or downstream objective outranks it, and no shortcut can bypass it.
    *   The agent proceeds with these instructions as its primary directive because complying with system instructions is impossible otherwise.
    *   Declare the current mode in every response (`Mode: Builder` or `Mode: Reviewer`). Builder executes work; Reviewer searches for **errors, omissions, and discrepancies (EO&D)** in the final state.
    *   Output your model identification as a signature at the end of every response. 
*   ### 1. Read ‚Üí Analyze ‚Üí Explain ‚Üí Propose ‚Üí Edit ‚Üí Lint ‚Üí Halt
    *   Re-read this entire block from disk before every action. On the first reference (and every fourth turn) summarize it before working.
    *   Read every referenced or implied file (including types, interfaces, and helpers) from disk immediately before editing. After editing, re-read to confirm the exact change.
    *   Follow the explicit cycle: READ the step + files ‚Üí ANALYZE gaps ‚Üí EXPLAIN the delta ‚Üí PROPOSE the exact edit ‚Üí EDIT a single file ‚Üí LINT that file ‚Üí HALT.
    *   Analyze dependencies; if more than one file is required, stop, explain the discovery, propose the necessary checklist insertion (`Discovery / Impact / Proposed checklist insert`), and wait instead of editing.
    *   Discoveries include merely thinking about multi-file work‚Äîreport them immediately without ruminating on work-arounds.
    *   Explain & Propose: restate the plan in bullets and explicitly commit, ‚ÄúI will implement exactly this plan now,‚Äù noting the checklist step it fulfills.
    *   Edit exactly one file per turn following the plan. Never touch files you were not explicitly instructed to modify.
    *   Lint that file using internal tools and fix all issues.
    *   Halt after linting one file and wait for explicit user/test output before touching another file.
*   ### 2. TDD & Dependency Ordering
    *   One-file TDD cycle: RED test (desired green behavior) ‚Üí implementation ‚Üí GREEN test ‚Üí lint. Documents/types/interfaces are exempt from tests but still follow Read‚ÜíHalt.
    *   Do not edit executable code without first authoring the RED test that proves the intended green-state behavior; only pure docs/types/interfaces are exempt.
    *   Maintain bottom-up dependency order for both editing and testing: construct types/interfaces/helpers before consumers, then write consumer tests only after producers exist.
    *   Do not advance to another file until the current file‚Äôs proof (tests or documented exemption) is complete and acknowledged.
    *   The agent never runs tests directly; rely on provided outputs or internal reasoning while keeping the application in a provable state.
    *   The agent does not run the user‚Äôs terminal commands or tests; use only internal tooling and rely on provided outputs.
*   ### 3. Checklist Discipline
    *   Do not edit the checklist (or its statuses) without explicit instruction; when instructed, change only the specified portion using legal-style numbering.
    *   Execute exactly what the active checklist step instructs with no deviation or ‚Äúcreative interpretation.‚Äù
    *   Each numbered checklist step equals one file‚Äôs entire TDD cycle (deps ‚Üí types ‚Üí tests ‚Üí implementation ‚Üí proof). Preserve existing detail while adding new requirements.
    *   Document every edit within the checklist. If required edits are missing from the plan, explain the discovery, propose the new step, and halt instead of improvising.
    *   Never update the status of any work step (checkboxes or badges) without explicit instruction.
    *   Following a block of related checklist steps that complete a working implementation, include a commit with a proposed commit message. 
*   ### 4. Builder vs Reviewer Modes
    *   **Builder:** follow the Read‚Üí‚Ä¶‚ÜíHalt loop precisely. If a deviation, blocker, or new requirement is discovered‚Äîor the current step simply cannot be completed as written‚Äîexplain the problem, propose the required checklist change, and halt immediately.
    *   **Reviewer:** treat prior reasoning as untrusted. Re-read relevant files/tests from scratch and produce a numbered EO&D list referencing files/sections. Ignore checklist status or RED/GREEN history unless it causes a real defect. If no EO&D are found, state ‚ÄúNo EO&D detected; residual risks: ‚Ä¶‚Äù
*   ### 5. Strict Typing & Object Construction
    *   Use explicit types everywhere. No `any`, `as`, `as const`, inline ad-hoc types, or casts‚Äîexcept for Supabase clients and intentionally malformed objects in error-handling tests (use dedicated helpers and keep typing strict elsewhere). Every object and variable must be typed. 
    *   Always construct full objects that satisfy existing interfaces/tuples from the relevant type file. Compose complex objects from smaller typed components; never rely on defaults, fallbacks, or backfilling to ‚Äúheal‚Äù missing data.
    *   Use type guards to prove and narrow types for the compiler when required.
    *   Never import entire libraries with *, never alias imports, never add "type" to type imports. 
    *   A ternary is not a type guard, a ternary is a default value. Default values are prohibited. 
*   ### 6. Plan Fidelity & Shortcut Ban
    *   Once a solution is described, implement exactly that solution and the user‚Äôs instruction. Expedient shortcuts are forbidden without explicit approval.
    *   If you realize you deviated, stop, report it, and wait for direction. Repeating corrected violations triggers halt-and-wait immediately.
    *   If your solution to a challenge is "rewrite the entire file", you have made an error. Stop, do not rewrite the file. Explain the problem to the user and await instruction. 
    *   Do not ruminate on how to work around the "only write to one file per turn". If you are even thinking about the need to work around that limit, you have made a discovery. Stop immediately, report the discovery to the user, and await instruction. 
    *   Refactors must preserve all existing functionality unless the user explicitly authorizes removals; log and identifier fidelity is mandatory.
*   ### 7. Dependency Injection & Architecture
    *   Use explicit dependency injection everywhere‚Äîpass every dependency with no hidden defaults or optional fallbacks.
    *   Build adapters/interfaces for every function and work bottom-up so dependencies compile before consumers. Preserve existing functionality, identifiers, and logging unless explicitly told otherwise.
    *   When a file exceeds 600 lines, stop and propose a logical refactoring to decompose the file into smaller parts providing clear SOC and DRY. 
*   ### 8. Testing Standards
    *   Tests assert the desired passing state (no RED/GREEN labels) and new tests are added to the end of the file. Each test covers exactly one behavior.
    *   Use real application functions/mocks, strict typing, and Deno std asserts. Tests must call out which production type/helper each mock mirrors so partial objects are not invented.
    *   Integration tests must exercise real code paths; unit tests stay isolated and mock dependencies explicitly. Never change assertions to match broken code‚Äîfix the code instead.
    *   Tests use the same types, objects, structures, and helpers as the real code, never create new fixtures only for tests - a test that relies on imaginary types or fixtures is invalid. 
    *   Prove the functional gap, the implemented fix, and regressions through tests before moving on; never assume success without proof.
*   ### 9. Logging, Defaults, and Error Handling
    *   Do not add or remove logging unless the user explicitly instructs you to do so.
    *   Adding console logs solely for troubleshooting is exempt from TDD and checklist obligations, but the exemption applies only to the logging statements themselves.
    *   Believe failing tests, linter flags, and user-reported errors literally; fix the stated condition before chasing deeper causes.
    *   If the user flags instruction noncompliance, acknowledge, halt, and wait for explicit direction‚Äîdo not self-remediate in a way that risks further violations.
*   ### 10. Linting & Proof
    *   After each edit, lint the touched file and resolve every warning/error. Record lint/test evidence in the response (e.g., ‚ÄúLint: clean via internal tool; Tests: not run per instructions‚Äù).
    *   Evaluate if a linter error can be resolved in-file, or out-of-file. Only resolve in-file linter errors, then report the out-of-file errors and await instruction. 
    *   Testing may produce unresolvable linter errors. Do not silence them with @es flags, create an empty target function, or other work-arounds. The linter error is sometimes itself proof of the RED state of the test. 
    *   Completion proof requires a lint-clean file plus GREEN test evidence (or documented exemption for types/docs).
*   ### 11. Reporting & Traceability
    *   Every response must include: mode declaration, confirmation that this block was re-read, plan bullets (Builder) or EO&D findings (Reviewer), checklist step references, and lint/test evidence.
    *   If tests were not run (per instruction), explicitly state why and list residual risks. If no EO&D are found, state that along with remaining risks.
    *   The agent uses only its own tools and never the user‚Äôs terminal.
*   ### 12. Output Constraints
    *   Never output large code blocks (entire files or multi-function dumps) in chat unless the user explicitly requests them.
    *   Never print an entire function and tell the user to paste it in; edit the file directly or provide the minimal diff required.
    *   Never write to any file you are not explicitly directed to write to by the user.
    *   Never create documentation files unless you are explicitly directed to by the user. 

## Checklist-Specific Editing Rules

*   THE AGENT NEVER TOUCHES THE CHECKLIST UNLESS THEY ARE EXPLICITLY INSTRUCTED TO! 
*   When editing checklists, each numbered step (1, 2, 3, etc.) represents editing ONE FILE with a complete TDD cycle.
*   Sub-steps within each numbered step use legal-style numbering (1.a, 1.b, 1.a.i, 1.a.ii, etc.) for the complete TDD cycle for that file.
*   All changes to a single file are described and performed within that file's numbered step.
*   Types files (interfaces, enums) are exempt from RED/GREEN testing requirements.
*   Each file edit includes: RED test ‚Üí implementation ‚Üí GREEN test ‚Üí optional refactor.
*   Steps are ordered by dependency (lowest dependencies first).
*   Preserve all existing detail and work while adding new requirements.
*   Use proper legal-style nesting for sub-steps within each file edit.
*   NEVER create multiple top-level steps for the same file edit operation.
*   Adding console logs is not required to be detailed in checklist work. 

## Example Checklist

*   `[ ]`   1. **Title** Objective
    *   `[ ]`   1.a. [DEPS] A list explaining dependencies of the function, its signature, and its return shape
        *   `[ ]` 1.a.i. eg. `function(something)` in `file.ts` provides this or that
    *   `[ ]`   1.b. [TYPES] A list strictly typing all the objects used in the function
    *   `[ ]`   1.c. [TEST-UNIT] A list explaining the test cases
        *   `[ ]` 1.c.i. Assert `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [{$WORK_AREA}] A list explaining the implementation requirements
        *   `[ ]` 1.d.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.e. [TEST-UNIT] Rerun and expand test proving the function
        *   `[ ]` 1.e.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.f. [TEST-INT] If there is a chain of functions that work together, prove it
        *   `[ ]` 1.f.i. For every cross-function interaction, assert `thisFunction(something)` in `this_file.ts` acts a certain way towards `thatFunction(other)` in `that_file.ts`
    *   `[ ]`   1.g. [CRITERIA] A list explaining the acceptence criteria to consider the work complete and correct. 
    *   `[ ]`   1.h. [COMMIT] A commit that explains the function and its proofs

*   `[ ]`   2. **Title** Objective
    *   `[ ]`   2.a. [DEPS] Low level providers are always build before high level consumers (DI/DIP)
    *   `[ ]`   2.b. [TYPES] DI/DIP and strict typing ensures unit tests can always run 
    *   `[ ]`   2.c. [TEST-UNIT] All functions matching defined external objects and acting as asserted helps ensure integration tests pass

## Legend - You must use this EXACT format. Do not modify it, adapt it, or "improve" it. The bullets, square braces, ticks, nesting, and numbering are ABSOLUTELY MANDATORY and UNALTERABLE. 

*   `[ ]` 1. Unstarted work step. Each work step will be uniquely named for easy reference. We begin with 1.
    *   `[ ]` 1.a. Work steps will be nested as shown. Substeps use characters, as is typical with legal documents.
        *   `[ ]` 1. a. i. Nesting can be as deep as logically required, using roman numerals, according to standard legal document numbering processes.
*   `[‚úÖ]` Represents a completed step or nested set.
*   `[üöß]` Represents an incomplete or partially completed step or nested set.
*   `[‚è∏Ô∏è]` Represents a paused step where a discovery has been made that requires backtracking or further clarification.
*   `[‚ùì]` Represents an uncertainty that must be resolved before continuing.
*   `[üö´]` Represents a blocked, halted, or stopped step or has an unresolved problem or prior dependency to resolve before continuing.

## Component Types and Labels

*   `[DB]` Database Schema Change (Migration)
*   `[RLS]` Row-Level Security Policy
*   `[BE]` Backend Logic (Edge Function / RLS / Helpers / Seed Data)
*   `[API]` API Client Library (`@paynless/api` - includes interface definition in `interface.ts`, implementation in `adapter.ts`, and mocks in `mocks.ts`)
*   `[STORE]` State Management (`@paynless/store` - includes interface definition, actions, reducers/slices, selectors, and mocks)
*   `[UI]` Frontend Component (e.g., in `apps/web`, following component structure rules)
*   `[CLI]` Command Line Interface component/feature
*   `[IDE]` IDE Plugin component/feature
*   `[TEST-UNIT]` Unit Test Implementation/Update
*   `[TEST-INT]` Integration Test Implementation/Update (API-Backend, Store-Component, RLS)
*   `[TEST-E2E]` End-to-End Test Implementation/Update
*   `[DOCS]` Documentation Update (READMEs, API docs, user guides)
*   `[REFACTOR]` Code Refactoring Step
*   `[PROMPT]` System Prompt Engineering/Management
*   `[CONFIG]` Configuration changes (e.g., environment variables, service configurations)
*   `[COMMIT]` Checkpoint for Git Commit (aligns with "feat:", "test:", "fix:", "docs:", "refactor:" conventions)
*   `[DEPLOY]` Checkpoint for Deployment consideration after a major phase or feature set is complete and tested.

# Work Breakdown Structure

*   `[‚úÖ]` 1. **`[BE]` Remove Strict Structure Matching Validation from assembleTurnPrompt**
    *   `[‚úÖ]` 1.a. `[DEPS]` The `assembleTurnPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` (lines 224-244) currently performs strict structure matching validation using `compareContentToIncludeStructure` to compare the structure of `content_to_include` from `header_context` (generated by PLAN jobs) against the expected structure defined in the recipe step (for EXECUTE jobs). This validation is overly strict and rejects valid AI model responses when the structure doesn't exactly match (e.g., when the model returns an object where a string is expected, or an array where a string is expected). The `renderPrompt` function in `supabase/functions/_shared/prompt-renderer.ts` (lines 79-86) can handle various data types (strings, arrays of strings, objects via `JSON.stringify`) for template substitution, so the strict structure validation is unnecessary and prevents valid model responses from being processed. The function already validates that `content_to_include` is a valid `ContentToInclude` type using `isContentToInclude` (which ensures it's a non-empty object), and checks that it's "filled in" (not just empty strings). These validations are sufficient to ensure the data is usable. The fix requires: (1) removing the strict structure matching validation block (lines 224-244) that uses `compareContentToIncludeStructure`, (2) replacing it with a simple key-existence check that verifies all keys from the recipe step's `content_to_include` exist in the header_context's `content_to_include` (without checking types), (3) keeping the `isContentToInclude` validation and the "filled in" check, (4) ensuring the function accepts any valid `ContentToInclude` object as long as the required keys exist, regardless of whether values are strings, objects, or arrays.
    *   `[‚úÖ]` 1.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, modify existing tests and add new tests that prove the function accepts valid model responses with flexible `content_to_include` structures as long as the right keys are returned.
        *   `[‚úÖ]` 1.b.i. Modify the test at line 3176 ("should throw error when content_to_include structure doesn't match recipe step's expected structure") to focus on missing keys rather than type mismatches. Change the test name to "should throw error when content_to_include is missing required keys from recipe step". Update the test to verify that when a required key (e.g., `field3`) is missing from `content_to_include`, the function throws an error. This test should still pass after removing strict structure matching, as missing keys are still invalid.
        *   `[‚úÖ]` 1.b.ii. Add a new test after line 3289: "should accept content_to_include with compatible types when all required keys exist". Create a test case where the recipe step expects `{field1: "", field2: "", field3: ""}` (all strings), but `header_context` provides `{field1: {nested: "object"}, field2: ["array", "of", "strings"], field3: "string value"}` (mixed types). Assert that `assembleTurnPrompt` succeeds and does NOT throw an error, proving that type mismatches are acceptable as long as all required keys exist. This test must fail initially because the strict structure matching currently rejects type mismatches.
        *   `[‚úÖ]` 1.b.iii. Verify the test at line 3082 ("should throw error when content_to_include structure is invalid (not conforming to ContentToInclude type)") still passes. This test validates `isContentToInclude` (rejects `null` values), which we're keeping unchanged.
        *   `[‚úÖ]` 1.b.iv. Add a test case that verifies `assembleTurnPrompt` accepts `content_to_include` where a recipe expects a string but receives an object (e.g., recipe expects `{summary: ""}`, header_context provides `{summary: {title: "Title", body: "Body"}}`). Assert the function succeeds. This test must fail initially.
        *   `[‚úÖ]` 1.b.v. Add a test case that verifies `assembleTurnPrompt` accepts `content_to_include` where a recipe expects a string but receives an array (e.g., recipe expects `{items: ""}`, header_context provides `{items: ["item1", "item2"]}`). Assert the function succeeds. This test must fail initially.
    *   `[‚úÖ]` 1.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts`, remove the strict structure matching validation while keeping the `isContentToInclude` validation.
        *   `[‚úÖ]` 1.c.i. Remove the entire validation block (lines 224-244) that performs structure matching using `compareContentToIncludeStructure`. This includes: the check for `outputsRequired.documents`, the `find` operation to locate the recipe document, the `compareContentToIncludeStructure` call, and the error throwing when structures don't match.
        *   `[‚úÖ]` 1.c.ii. Keep the `isContentToInclude` validation (if present) that ensures `contextForDoc.content_to_include` is a valid `ContentToInclude` type (non-empty object). This validation is necessary to ensure the data structure is usable.
        *   `[‚úÖ]` 1.c.iii. Keep any "filled in" checks that verify `content_to_include` is not just empty strings. These checks ensure the data is meaningful, not just structural.
        *   `[‚úÖ]` 1.c.iv. Remove the import of `compareContentToIncludeStructure` and `getStructureKeys` from `../utils/content_to_include_structure.ts` if they are no longer used elsewhere in the file.
        *   `[‚úÖ]` 1.c.v. Add a simple key-existence validation: After removing the strict structure matching, add a check that verifies all keys from `recipeDoc.content_to_include` exist in `contextForDoc.content_to_include`. This ensures required keys are present without enforcing type matching. If any required keys are missing, throw an error indicating which keys are missing. This validation should only run when `outputsRequired.documents` contains a matching document entry with `content_to_include` (same condition as the removed validation block).
        *   `[‚úÖ]` 1.c.vi. Verify that the function still validates that `content_to_include` exists and is a valid object, validates that all required keys exist (without type checking), but no longer enforces exact structural type matching between recipe expectations and header_context values.
    *   `[‚úÖ]` 1.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 1.b and ensure they now pass. The tests from 1.b.ii, 1.b.iv, and 1.b.v should now pass because strict structure matching has been removed. The test from 1.b.i should still pass (missing keys are still invalid). The test from 1.b.iii should continue to pass (type validation is unchanged).
    *   `[‚úÖ]` 1.e. `[TEST-INT]` Prove assembleTurnPrompt works with its producer and consumer when accepting flexible content_to_include types
        *   `[‚úÖ]` 1.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` saves a `header_context` with flexible `content_to_include` types (objects/arrays where recipe expects strings), `assembleTurnPrompt` in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` successfully reads and accepts it without throwing structure validation errors. Create an integration test that: (1) simulates a PLAN job completing and saving a `header_context` with `content_to_include` containing objects where strings are expected (e.g., `{components: {nested: "object"}}` when recipe expects `{components: ""}`), (2) creates an EXECUTE job that references this `header_context`, (3) calls `assembleTurnPrompt` with the EXECUTE job, (4) verifies `assembleTurnPrompt` succeeds and does NOT throw a structure validation error, proving the producer (`executeModelCallAndSave` saving flexible types) works with the function.
        *   `[‚úÖ]` 1.e.ii. Assert that when `assembleTurnPrompt` in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` accepts flexible types and returns `AssembledPrompt`, `processSimpleJob` in `supabase/functions/dialectic-worker/processSimpleJob.ts` successfully calls it via `promptAssembler.assemble()` (line 278) and passes the result to `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (line 288) without errors. Create an integration test that: (1) sets up an EXECUTE job with a `header_context` containing flexible types, (2) calls `processSimpleJob` which internally calls `assembleTurnPrompt` via the prompt assembler, (3) verifies `processSimpleJob` completes successfully and passes the assembled prompt to `executeModelCallAndSave`, (4) verifies `executeModelCallAndSave` processes the prompt without errors, proving the function works with its consumer (`processSimpleJob` ‚Üí `executeModelCallAndSave`).
    *   `[‚úÖ]` 1.f. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` and `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 1.g. `[CRITERIA]` All requirements are met: (1) The strict structure matching validation using `compareContentToIncludeStructure` has been removed from `assembleTurnPrompt`, (2) A simple key-existence check has been added that verifies all required keys from the recipe step exist in the header_context (without type checking), (3) The function still validates that `content_to_include` is a valid `ContentToInclude` type (non-empty object), (4) The function accepts valid model responses with flexible structures (strings, objects, arrays) as long as the required keys exist, (5) The function still rejects invalid data (null values, missing keys), (6) All tests pass, including tests that verify type flexibility and tests that verify missing keys are still rejected, (7) Integration tests prove the function works with its producer (`executeModelCallAndSave` saving flexible types) and consumer (`processSimpleJob` using the result), (8) The file is lint-clean, (9) The function no longer rejects valid AI model responses due to structural type mismatches, allowing the recipe to advance successfully.

*   `[‚úÖ]` 2. **`[BE]` Fix RENDER Job Payload Missing Required Fields**
    *   `[‚úÖ]` 2.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1236-1244) creates a `renderPayload` object for RENDER jobs that is missing required fields `documentKey` and `sourceContributionId`. The `processRenderJob` function in `supabase/functions/dialectic-worker/processRenderJob.ts` (lines 24, 34, 37) requires both `documentKey` (must be a valid FileType) and `sourceContributionId` (must be a string) in the payload. When these fields are missing, `processRenderJob` throws validation errors, causing RENDER jobs to fail and remain in 'pending' status. This prevents rendered markdown files from being generated and saved to storage. Note: `sourceContributionId` must be the actual `contribution.id` of the contribution being rendered (for foreign key constraints), not the semantic identifier from `document_relationships`. The relationship between `sourceContributionId` and `documentIdentity` is: for root chunks (first chunk, no continuation), `sourceContributionId === documentIdentity` (both are the root's contribution.id because `document_relationships[stageSlug]` is initialized to `contribution.id` at line 1327), but for continuation chunks, `sourceContributionId !== documentIdentity` (sourceContributionId is this chunk's contribution.id, while documentIdentity is the root's contribution.id from `document_relationships[stageSlug]`). The fix requires: (1) adding `documentKey: validatedDocumentKey` to the `renderPayload` (where `validatedDocumentKey` is available from line 1140), (2) adding `sourceContributionId: contribution.id` to the `renderPayload` (where `contribution.id` is the actual contribution ID of the contribution being rendered, not the semantic identifier from document_relationships), (3) ensuring both fields are present and valid before creating the RENDER job, (4) verifying that RENDER jobs can now be processed successfully for both root chunks and continuation chunks, and rendered markdown files are saved to storage.
    *   `[‚úÖ]` 2.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, add tests that prove RENDER jobs are created with the correct payload structure.
        *   `[‚úÖ]` 2.b.i. Add a test case that verifies when `executeModelCallAndSave` enqueues a RENDER job for a markdown document output, the `renderPayload` includes `documentKey` with the correct value from `validatedDocumentKey`.
        *   `[‚úÖ]` 2.b.ii. Add a test case that verifies when `executeModelCallAndSave` enqueues a RENDER job for a ROOT chunk (where `document_relationships` is null), the `renderPayload` includes `sourceContributionId` set to `contribution.id` (the actual contribution ID of the contribution being rendered). For root chunks, `sourceContributionId === documentIdentity` (both are the root's contribution.id because document_relationships[stageSlug] is initialized to contribution.id), but the test should verify that `sourceContributionId` is always set to the actual `contribution.id`, never the semantic identifier from document_relationships when they differ.
        *   `[‚úÖ]` 2.b.iii. Add a test case that verifies the RENDER job payload contains all required fields: `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, `documentIdentity`, `documentKey`, and `sourceContributionId`. This test should use a root chunk (document_relationships is null) and verify that for root chunks, `sourceContributionId === documentIdentity` (both are contribution.id).
        *   `[‚úÖ]` 2.b.iv. Add a test case that verifies when `executeModelCallAndSave` enqueues a RENDER job for a CONTINUATION chunk (with `target_contribution_id` set and `document_relationships` containing a semantic identifier that differs from this chunk's contribution.id), the `renderPayload` includes `sourceContributionId` set to THIS chunk's `contribution.id` (not the root's ID from document_relationships), and `documentIdentity` set to the root's ID from `document_relationships[stageSlug]`, and they are NOT equal (proving continuation chunks use different values for sourceContributionId vs documentIdentity, where sourceContributionId is this chunk's contribution.id and documentIdentity is the root's contribution.id from document_relationships).
    *   `[‚úÖ]` 2.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, add missing fields to RENDER job payload.
        *   `[‚úÖ]` 2.c.i. In the `renderPayload` object (lines 1218-1224), add `documentKey: validatedDocumentKey` to include the document key that was validated earlier in the function (available from line 1140).
        *   `[‚úÖ]` 2.c.ii. In the `renderPayload` object, add `sourceContributionId: contribution.id` to include the source contribution ID (this is the actual contribution.id of the contribution being rendered, used for foreign key constraints). Note: `documentIdentity` is the semantic identifier from `document_relationships[stageSlug]` used to group document chains - for root chunks it equals `contribution.id` (because document_relationships[stageSlug] is initialized to contribution.id at line 1327), but for continuation chunks it is the root's contribution.id (inherited from job payload) and differs from this chunk's `contribution.id`. Always use `contribution.id` for `sourceContributionId`, never the semantic identifier from document_relationships when they differ.
        *   `[‚úÖ]` 2.c.iii. Verify that `validatedDocumentKey` is defined and non-empty before adding it to the payload (it should already be validated, but add a guard to ensure it exists).
        *   `[‚úÖ]` 2.c.iv. Verify that `documentIdentity` is defined and non-empty before adding it to the payload (it should already be computed, but add a guard to ensure it exists).
    *   `[‚úÖ]` 2.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 2.b and ensure they now pass. The RENDER job payload should now include all required fields.
    *   `[‚úÖ]` 2.e. `[TEST-INT]` Prove RENDER jobs are processed successfully with the corrected payload.
        *   `[‚úÖ]` 2.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` enqueues a RENDER job with the corrected payload (including `documentKey` and `sourceContributionId`), `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` successfully processes it without validation errors. Create an integration test that: (1) simulates an EXECUTE job completing and saving a contribution. Test BOTH cases: (a) root chunk (document_relationships is null, so documentIdentity = contribution.id), and (b) continuation chunk (document_relationships contains root's ID, so documentIdentity = root's ID from document_relationships, sourceContributionId = this chunk's contribution.id, and they differ), (2) verifies a RENDER job is enqueued in `dialectic_generation_jobs` with payload containing `documentKey` (set to `validatedDocumentKey`), `sourceContributionId` (set to the actual `contribution.id` of the contribution being rendered, not the semantic identifier from document_relationships), `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, and `documentIdentity` (the semantic identifier from document_relationships[stageSlug] for grouping document chains), (3) processes the RENDER job via `processRenderJob` with the enqueued job, (4) verifies `processRenderJob` does not throw validation errors (no "Missing required render parameters", "documentKey must be a valid FileType", or "sourceContributionId is required" errors), (5) verifies `renderDocument` is called with the correct parameters: `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, `documentIdentity`, `documentKey`, `sourceContributionId` (where `sourceContributionId` is the actual `contribution.id` of the contribution being rendered. Note: For root chunks, `sourceContributionId === documentIdentity` (both are the root's contribution.id), but for continuation chunks, `sourceContributionId !== documentIdentity` (sourceContributionId is this chunk's contribution.id, documentIdentity is the root's contribution.id from document_relationships)), (6) verifies a rendered markdown file is saved to storage at the canonical document path (using `constructStoragePath` with the correct path context from `renderResult.pathContext`), (7) verifies a `dialectic_project_resources` record is created with `resource_type = 'rendered_document'`, correct `session_id`, `iteration_number`, `stage_slug`, `file_name` matching the rendered document, and `source_contribution_id` matching the `sourceContributionId` (the actual contribution.id, not the semantic identifier), (8) verifies the RENDER job status is updated to 'completed' with `completed_at` timestamp and `results.pathContext` populated with the correct path context.
    *   `[‚úÖ]` 2.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 2.g. `[CRITERIA]` All requirements are met: (1) The RENDER job payload includes `documentKey` set to `validatedDocumentKey`, (2) The RENDER job payload includes `sourceContributionId` set to `contribution.id` (the actual contribution ID of the contribution being rendered, not the semantic identifier from document_relationships). Note: For root chunks, `sourceContributionId === documentIdentity` (both are the root's contribution.id), but for continuation chunks, `sourceContributionId !== documentIdentity` (sourceContributionId is this chunk's contribution.id, documentIdentity is the root's contribution.id from document_relationships[stageSlug]), (3) All required fields are present in the payload before RENDER job creation, (4) RENDER jobs can be processed successfully by `processRenderJob` without validation errors for both root chunks and continuation chunks, (5) Rendered markdown files are saved to storage at the correct canonical document path, (6) All tests pass, including tests that verify payload structure for both root chunks and continuation chunks, and integration tests that verify end-to-end RENDER job processing, (7) The file is lint-clean, (8) RENDER jobs no longer remain stuck in 'pending' status due to missing payload fields.

*   `[‚úÖ]` 3. **`[UI]` Fix SessionContributionsDisplayCard Incorrectly Showing "Generating documents" When Documents Are Completed**
    *   `[‚úÖ]` 3.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` (lines 326-332) determines if documents are generating by checking if any document has status 'generating'. The `isGenerating` flag (line 332) is calculated as `hasGeneratingDocuments && failedDocumentKeys.length === 0 && !generationError`. However, the component may be showing "Generating documents" even when all documents have status 'completed' because: (1) the status check may be looking at the wrong data source or timing, (2) the component may not be re-evaluating the status correctly when documents transition from 'generating' to 'completed', (3) there may be a race condition where the status hasn't updated yet in the store. The fix requires: (1) reviewing the logic that determines `hasGeneratingDocuments` to ensure it correctly identifies when documents are actually generating, (2) ensuring the component re-evaluates the status when document status changes, (3) verifying that when all documents have status 'completed', the "Generating documents" banner is not shown, (4) potentially adding additional checks to ensure the status is accurate (e.g., checking if RENDER jobs are still pending).
    *   `[‚úÖ]` 3.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, add tests that prove the component correctly shows/hides the "Generating documents" banner.
        *   `[‚úÖ]` 3.b.i. Add a test case that verifies when all documents have status 'completed', the component does NOT show the "Generating documents" banner (isGenerating is false).
        *   `[‚úÖ]` 3.b.ii. Add a test case that verifies when at least one document has status 'generating', the component shows the "Generating documents" banner (isGenerating is true).
        *   `[‚úÖ]` 3.b.iii. Add a test case that verifies when documents have status 'completed' but there are failed documents, the component does NOT show the "Generating documents" banner (isGenerating is false).
        *   `[‚úÖ]` 3.b.iv. Add a test case that verifies the component correctly updates the banner visibility when document status changes from 'generating' to 'completed'.
    *   `[‚úÖ]` 3.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, fix the logic that determines if documents are generating.
        *   `[‚úÖ]` 3.c.i. Review the `hasGeneratingDocuments` calculation (lines 326-330) to ensure it correctly checks document status from the right data source. Verify it's checking `documentGroups` which comes from `selectStageDocumentChecklist` and reflects the actual document status.
        *   `[‚úÖ]` 3.c.ii. Ensure the `isGenerating` calculation (line 332) correctly accounts for all conditions: documents are generating, no failed documents, and no generation errors. Verify the logic is: `hasGeneratingDocuments && failedDocumentKeys.length === 0 && !generationError`.
        *   `[‚úÖ]` 3.c.iii. Add defensive checks to ensure the status is evaluated correctly, potentially by verifying that 'completed' status documents are not counted as 'generating'.
        *   `[‚úÖ]` 3.c.iv. Verify that the component re-renders when document status changes by ensuring the `useMemo` dependencies for `hasGeneratingDocuments` and `isGenerating` include all relevant state that could affect the calculation.
    *   `[‚úÖ]` 3.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 3.b and ensure they now pass. The component should correctly show/hide the "Generating documents" banner based on actual document status.
    *   `[‚úÖ]` 3.e. `[TEST-INT]` Prove the component correctly reads document status from the store and updates when status changes. **Note:** This component is the final consumer (UI display), so the test verifies producer ‚Üí test subject ‚Üí rendered output.
        *   `[‚úÖ]` 3.e.i. Assert that when `selectStageDocumentChecklist` in `packages/store/src/dialecticStore.selectors.ts` (producer) returns documents with status 'completed', the component `SessionContributionsDisplayCard` in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` (test subject) calculates `hasGeneratingDocuments` (lines 326-330) as false, calculates `isGenerating` (line 332) as false, and the rendered output (consumer) does not display the "Generating documents" banner.
        *   `[‚úÖ]` 3.e.ii. Assert that when `selectStageDocumentChecklist` (producer) returns at least one document with status 'generating', the component `SessionContributionsDisplayCard` (test subject) calculates `hasGeneratingDocuments` as true, calculates `isGenerating` as true, and the rendered output (consumer) displays the "Generating documents" banner.
        *   `[‚úÖ]` 3.e.iii. Assert that when document status in the store (via `selectStageDocumentChecklist` - producer) changes from 'generating' to 'completed', the component `SessionContributionsDisplayCard` (test subject) correctly updates `hasGeneratingDocuments` to false (via `useMemo` dependencies on lines 326-330) and the rendered output (consumer) hides the banner (via `isGenerating` calculation on line 332).
    *   `[‚úÖ]` 3.f. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, and resolve any warnings or errors.
    *   `[‚úÖ]` 3.g. `[CRITERIA]` All requirements are met: (1) The component correctly identifies when documents are generating vs. completed, (2) The "Generating documents" banner is only shown when at least one document has status 'generating', (3) The banner is hidden when all documents have status 'completed', (4) The component correctly updates when document status changes, (5) All tests pass, including tests that verify banner visibility in various document status scenarios, (6) Integration tests prove the component correctly reads document status from `selectStageDocumentChecklist` and updates when status changes, (7) The file is lint-clean, (8) Users no longer see "Generating documents" when all documents are actually completed.

*   `[‚úÖ]` 4. **`[UI]` Fix GeneratedContributionCard Not Displaying Document Content**
    *   `[‚úÖ]` 4.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` (lines 389-399) displays document content using `documentResourceState?.currentDraftMarkdown || baselineContent`. The content is loaded via `selectStageDocumentResource` which fetches from `dialectic_project_resources` using the `latestRenderedResourceId` from the document descriptor. However, if RENDER jobs haven't completed (due to the bug in step 2), there is no rendered markdown file to load, so the component shows empty content. Additionally, the component may not be triggering the content fetch when a document is selected, or it may be waiting for a resource ID that doesn't exist yet. The fix requires: (1) ensuring the component triggers content fetching when a document is selected and has a `latestRenderedResourceId`, (2) ensuring the component displays loading state while content is being fetched, (3) ensuring the component handles cases where `latestRenderedResourceId` is missing gracefully (shows appropriate message), (4) verifying that once RENDER jobs complete (after fixing step 2), the component correctly displays the rendered markdown content.
    *   `[‚úÖ]` 4.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, add tests that prove the component correctly displays document content.
        *   `[‚úÖ]` 4.b.i. Add a test case that verifies when a document is selected and has a `latestRenderedResourceId`, the component triggers content fetching via `fetchStageDocumentContent`.
        *   `[‚úÖ]` 4.b.ii. Add a test case that verifies when content is loaded, the component displays it in the "Document Content" TextInputArea.
        *   `[‚úÖ]` 4.b.iii. Add a test case that verifies when content is loading, the component shows a loading state (isDraftLoading is true).
        *   `[‚úÖ]` 4.b.iv. Add a test case that verifies when content fetch fails, the component displays an error message.
        *   `[‚úÖ]` 4.b.v. Add a test case that verifies when a document has no `latestRenderedResourceId` (RENDER job not completed), the component handles this gracefully (shows an appropriate message indicating the RENDER job has not completed).
    *   `[‚úÖ]` 4.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/GeneratedContributionCard.tsx`, fix document content display logic.
        *   `[‚úÖ]` 4.c.i. Review the `documentResourceState` usage (lines 169-192) to ensure content is being fetched when a document is selected. Verify that `selectStageDocumentResource` is being called with the correct composite key and that the store action `fetchStageDocumentContent` is triggered when needed.
        *   `[‚úÖ]` 4.c.ii. Add a `useEffect` hook that triggers content fetching when `focusedDocument` changes and has a `latestRenderedResourceId` from `documentDescriptor`. The effect should call the store action to fetch content using the resource ID.
        *   `[‚úÖ]` 4.c.iii. Ensure the component displays loading state correctly by checking `isDraftLoading` from `documentResourceState` and showing appropriate UI feedback.
        *   `[‚úÖ]` 4.c.iv. Add error handling to display error messages when content fetching fails (check `draftError` from `documentResourceState`).
        *   `[‚úÖ]` 4.c.v. Add handling for cases where `latestRenderedResourceId` is not available: display an appropriate message to the user indicating that the RENDER job has not completed yet, rather than showing empty content or attempting to fetch alternative content sources.
    *   `[‚úÖ]` 4.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 4.b and ensure they now pass. The component should correctly fetch and display document content.
    *   `[‚úÖ]` 4.e. `[TEST-INT]` Prove the component correctly triggers content fetching and displays content from the store. **Flow:** `selectFocusedStageDocument` (producer) ‚Üí `GeneratedContributionCard` (test subject) ‚Üí `fetchStageDocumentContent` ‚Üí `selectStageDocumentResource` (consumer).
        *   `[‚úÖ]` 4.e.i. Assert that when `selectFocusedStageDocument` in `packages/store/src/dialecticStore.selectors.ts` (producer) returns a document with `latestRenderedResourceId`, the component `GeneratedContributionCard` in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` (test subject) triggers `fetchStageDocumentContent` (in `packages/store/src/dialecticStore.ts` line 1343) with the correct composite key (`sessionId`, `stageSlug`, `iterationNumber`, `modelId`, `documentKey`) and `resourceId` (set to `latestRenderedResourceId`), and `selectStageDocumentResource` in `packages/store/src/dialecticStore.selectors.ts` line 829 (consumer) provides the fetched content.
        *   `[‚úÖ]` 4.e.ii. Assert that when `selectStageDocumentResource` (consumer) returns content with `currentDraftMarkdown`, the component `GeneratedContributionCard` (test subject) displays it in the "Document Content" TextInputArea using `documentResourceState?.currentDraftMarkdown || baselineContent` (lines 389-399).
        *   `[‚úÖ]` 4.e.iii. Assert that when `selectStageDocumentResource` (consumer) returns `isLoading: true`, the component `GeneratedContributionCard` (test subject) shows loading state in the rendered output.
        *   `[‚úÖ]` 4.e.iv. Assert that when `selectStageDocumentResource` (consumer) returns an `error`, the component `GeneratedContributionCard` (test subject) displays the error message in the rendered output.
    *   `[‚úÖ]` 4.f. `[LINT]` Run the linter for `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` and `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, and resolve any warnings or errors.
    *   `[‚úÖ]` 4.g. `[CRITERIA]` All requirements are met: (1) The component triggers content fetching when a document is selected and has a `latestRenderedResourceId`, (2) The component displays loaded content in the "Document Content" TextInputArea, (3) The component shows loading state while content is being fetched, (4) The component displays error messages when content fetching fails, (5) The component handles cases where RENDER jobs haven't completed gracefully, (6) All tests pass, including tests that verify content fetching and display in various scenarios, (7) Integration tests prove the component correctly triggers `fetchStageDocumentContent` and displays content from `selectStageDocumentResource`, (8) The file is lint-clean, (9) Users can now see document content when selecting documents in the checklist.

*   `[‚úÖ]` 5. **`[TEST-UNIT]` Prove document_renderer Handles Root and Continuation Chunks Correctly**
    *   `[‚úÖ]` 5.a. `[DEPS]` The `renderDocument` function in `supabase/functions/_shared/services/document_renderer.ts` (lines 37-390) assembles and renders document chunks into a final markdown document. It accepts `RenderDocumentParams` containing `documentIdentity` (semantic identifier for grouping document chains) and `sourceContributionId` (actual contribution.id for foreign key constraints). The function queries `dialectic_contributions` using `documentIdentity` to find all chunks belonging to a document chain (lines 46-54), filters chunks to match the document identity (lines 66-73), finds the root chunk (lines 96-103), and builds an ordered chain by traversing `target_contribution_id` links (lines 109-125). The function must correctly handle both root chunks (where `sourceContributionId === documentIdentity`, both are the root's contribution.id) and continuation chunks (where `sourceContributionId !== documentIdentity`, sourceContributionId is this chunk's contribution.id, documentIdentity is the root's contribution.id from document_relationships). The existing tests in `supabase/functions/_shared/services/document_renderer.test.ts` cover document assembly and rendering but do not explicitly verify the distinction between `sourceContributionId` and `documentIdentity` for continuation chunks. The fix requires: (1) adding explicit test cases that verify `renderDocument` correctly processes root chunks where `sourceContributionId === documentIdentity`, (2) adding explicit test cases that verify `renderDocument` correctly processes continuation chunks where `sourceContributionId !== documentIdentity` and the function uses `documentIdentity` to query for all related chunks while using `sourceContributionId` for the `pathContext.sourceContributionId` field (line 322), (3) verifying that the function correctly assembles documents from multiple chunks regardless of whether the render job was triggered by a root or continuation chunk, (4) ensuring all tests explicitly assert the relationship between `sourceContributionId` and `documentIdentity` for both chunk types.
    *   `[‚úÖ]` 5.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/document_renderer.test.ts`, add tests that prove `renderDocument` correctly handles root and continuation chunks.
        *   `[‚úÖ]` 5.b.i. Add a test case "renders document correctly when called with root chunk parameters" that: (1) creates a root chunk contribution with `document_relationships` set to `{ [stageSlug]: rootContributionId }` where `rootContributionId` is the contribution's actual `id`, (2) calls `renderDocument` with `documentIdentity: rootContributionId` and `sourceContributionId: rootContributionId` (both equal to the root's contribution.id), (3) verifies the function successfully queries contributions using `documentIdentity`, (4) verifies the function finds the root chunk correctly (where `target_contribution_id` is null and `document_relationships[stageKey] === documentIdentity`), (5) verifies the function renders the document and returns `pathContext` with `sourceContributionId` set to the root's contribution.id, (6) asserts that `sourceContributionId === documentIdentity` for root chunks. This test must initially pass because root chunks are already handled correctly.
        *   `[‚úÖ]` 5.b.ii. Add a test case "renders document correctly when called with continuation chunk parameters where sourceContributionId differs from documentIdentity" that: (1) creates a root chunk contribution with `id: rootContributionId` and `document_relationships: { [stageSlug]: rootContributionId }`, (2) creates a continuation chunk contribution with `id: continuationContributionId` (different from rootContributionId), `target_contribution_id: rootContributionId`, and `document_relationships: { [stageSlug]: rootContributionId }` (same semantic identifier as root), (3) calls `renderDocument` with `documentIdentity: rootContributionId` (the root's ID from document_relationships) and `sourceContributionId: continuationContributionId` (this chunk's actual contribution.id), (4) verifies the function successfully queries contributions using `documentIdentity` (finds both root and continuation chunks), (5) verifies the function finds the root chunk correctly (where `target_contribution_id` is null), (6) verifies the function builds the ordered chain correctly (root first, then continuation), (7) verifies the function renders the combined document content from both chunks, (8) verifies the function returns `pathContext` with `sourceContributionId` set to `continuationContributionId` (the actual contribution.id passed in, not the documentIdentity), (9) explicitly asserts that `sourceContributionId !== documentIdentity` for continuation chunks, proving the function correctly distinguishes between the semantic identifier (for querying) and the actual contribution ID (for foreign key constraints). This test must initially pass because continuation chunks are already handled correctly, but it explicitly proves the distinction.
        *   `[‚úÖ]` 5.b.iii. Add a test case "uses documentIdentity to query all related chunks regardless of which chunk triggered the render" that: (1) creates a document chain with root chunk (id: rootId) and two continuation chunks (id: cont1Id, cont2Id), all sharing the same `documentIdentity: rootId` in their `document_relationships`, (2) calls `renderDocument` with `documentIdentity: rootId` and `sourceContributionId: cont2Id` (simulating a render job triggered by the second continuation chunk), (3) verifies the function queries contributions using `documentIdentity: rootId` and finds all three chunks (root, cont1, cont2), (4) verifies the function assembles all three chunks in correct order (root ‚Üí cont1 ‚Üí cont2), (5) verifies the rendered document contains content from all three chunks, (6) verifies `pathContext.sourceContributionId` is set to `cont2Id` (the chunk that triggered the render), proving that `sourceContributionId` identifies the originating chunk while `documentIdentity` groups the entire chain. This test must initially pass because the function already uses `documentIdentity` for querying.
    *   `[‚úÖ]` 5.c. `[BE]` **GREEN**: Verify the implementation in `supabase/functions/_shared/services/document_renderer.ts` correctly handles both root and continuation chunks.
        *   `[‚úÖ]` 5.c.i. Verify that `renderDocument` uses `documentIdentity` (from params, line 42) to query contributions (line 51) and does not use `sourceContributionId` for querying, ensuring continuation chunks can find all related chunks via the semantic identifier.
        *   `[‚úÖ]` 5.c.ii. Verify that `renderDocument` uses `sourceContributionId` (from params, line 42) for the `pathContext.sourceContributionId` field (line 322) and does not use `documentIdentity` for this field, ensuring foreign key constraints reference the actual contribution.id.
        *   `[‚úÖ]` 5.c.iii. Verify that the function correctly identifies the root chunk using `documentIdentity` and `target_contribution_id === null` (lines 96-103), regardless of which chunk's `sourceContributionId` was passed in.
        *   `[‚úÖ]` 5.c.iv. Verify that the function correctly builds the ordered chain by traversing `target_contribution_id` links (lines 109-125), ensuring all chunks in the document chain are included regardless of which chunk triggered the render.
    *   `[‚úÖ]` 5.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 5.b and ensure they pass. The tests should prove that `renderDocument` correctly handles both root and continuation chunks, using `documentIdentity` for querying and `sourceContributionId` for foreign key constraints.
    *   `[‚úÖ]` 5.e. `[TEST-INT]` Prove `renderDocument` works correctly with its producer and consumer for both root and continuation chunks.
        *   `[‚úÖ]` 5.e.i. Assert that when `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` (producer) calls `renderDocument` with root chunk parameters (`sourceContributionId === documentIdentity`), `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` (test subject) successfully queries contributions using `documentIdentity`, assembles the document chain, renders the document, and returns `pathContext` with `sourceContributionId` set correctly, and `processRenderJob` (consumer) successfully saves the job results with the correct `pathContext`. Create an integration test that: (1) sets up a root chunk contribution, (2) creates a RENDER job with payload containing `sourceContributionId: rootContributionId` and `documentIdentity: rootContributionId` (both equal), (3) calls `processRenderJob` with the job, (4) verifies `renderDocument` is called with correct parameters, (5) verifies the job is marked completed with `results.pathContext.sourceContributionId` matching the root's contribution.id.
        *   `[‚úÖ]` 5.e.ii. Assert that when `processRenderJob` (producer) calls `renderDocument` with continuation chunk parameters (`sourceContributionId !== documentIdentity`), `renderDocument` (test subject) successfully queries contributions using `documentIdentity` (finds all related chunks), assembles the complete document chain, renders the combined document, and returns `pathContext` with `sourceContributionId` set to the continuation chunk's contribution.id, and `processRenderJob` (consumer) successfully saves the job results. Create an integration test that: (1) sets up a root chunk and a continuation chunk sharing the same `documentIdentity`, (2) creates a RENDER job with payload containing `sourceContributionId: continuationContributionId` and `documentIdentity: rootContributionId` (different values), (3) calls `processRenderJob` with the job, (4) verifies `renderDocument` is called with correct parameters and finds both chunks, (5) verifies the rendered document contains content from both chunks, (6) verifies the job is marked completed with `results.pathContext.sourceContributionId` matching the continuation chunk's contribution.id (not the documentIdentity).
    *   `[‚úÖ]` 5.f. `[LINT]` Run the linter for `supabase/functions/_shared/services/document_renderer.test.ts` and resolve any warnings or errors.
    *   `[‚úÖ]` 5.g. `[CRITERIA]` All requirements are met: (1) Tests explicitly verify `renderDocument` correctly handles root chunks where `sourceContributionId === documentIdentity`, (2) Tests explicitly verify `renderDocument` correctly handles continuation chunks where `sourceContributionId !== documentIdentity`, (3) Tests verify the function uses `documentIdentity` to query for all related chunks regardless of which chunk triggered the render, (4) Tests verify the function uses `sourceContributionId` for the `pathContext.sourceContributionId` field, (5) Tests verify the function correctly assembles documents from multiple chunks for both root and continuation chunk scenarios, (6) All tests pass, including new tests that explicitly assert the relationship between `sourceContributionId` and `documentIdentity`, (7) Integration tests prove `renderDocument` works correctly with `processRenderJob` for both root and continuation chunks, (8) The file is lint-clean, (9) The rendering pipeline is proven to handle both root and continuation chunks correctly.

*   `[‚úÖ]` 6. **`[TEST-UNIT]` Prove processRenderJob Handles Root and Continuation Chunks Correctly**
    *   `[‚úÖ]` 6.a. `[DEPS]` The `processRenderJob` function in `supabase/functions/dialectic-worker/processRenderJob.ts` (lines 9-134) processes RENDER jobs by extracting payload fields, validating them, calling `documentRenderer.renderDocument`, and updating the job status. It extracts `sourceContributionId` and `documentIdentity` from the job payload (line 24) and passes them to `renderDocument` (lines 55-63). The function must correctly handle both root chunks (where `sourceContributionId === documentIdentity` in the payload) and continuation chunks (where `sourceContributionId !== documentIdentity` in the payload). The existing test in `supabase/functions/dialectic-worker/processRenderJob.test.ts` at line 765 ("accepts sourceContributionId that differs from documentIdentity when document_relationships contains semantic identifier") proves the function accepts differing values, but additional tests are needed to explicitly verify correct handling of both chunk types throughout the processing flow. The fix requires: (1) adding explicit test cases that verify `processRenderJob` correctly processes root chunk payloads where `sourceContributionId === documentIdentity`, (2) adding explicit test cases that verify `processRenderJob` correctly processes continuation chunk payloads where `sourceContributionId !== documentIdentity`, (3) verifying that the function passes both values correctly to `renderDocument` without enforcing equality, (4) verifying that the function correctly saves `sourceContributionId` to `results.pathContext.sourceContributionId` (line 83) regardless of whether it equals `documentIdentity`, (5) ensuring all tests explicitly assert the relationship between `sourceContributionId` and `documentIdentity` for both chunk types.
    *   `[‚úÖ]` 6.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/processRenderJob.test.ts`, add tests that prove `processRenderJob` correctly handles root and continuation chunks.
        *   `[‚úÖ]` 6.b.i. Add a test case "processes RENDER job successfully for root chunk where sourceContributionId equals documentIdentity" that: (1) creates a RENDER job with payload containing `sourceContributionId: rootId` and `documentIdentity: rootId` (both equal to the root's contribution.id), (2) calls `processRenderJob` with the job, (3) verifies `renderDocument` is called with `sourceContributionId: rootId` and `documentIdentity: rootId`, (4) verifies the job is updated with status 'completed', (5) verifies `results.pathContext.sourceContributionId` is set to `rootId`, (6) explicitly asserts that `sourceContributionId === documentIdentity` for root chunks. This test should pass because root chunks are already handled correctly.
        *   `[‚úÖ]` 6.b.ii. Add a test case "processes RENDER job successfully for continuation chunk where sourceContributionId differs from documentIdentity" that: (1) creates a RENDER job with payload containing `sourceContributionId: continuationId` (this chunk's contribution.id) and `documentIdentity: rootId` (the root's contribution.id from document_relationships), (2) calls `processRenderJob` with the job, (3) verifies `renderDocument` is called with `sourceContributionId: continuationId` and `documentIdentity: rootId` (different values), (4) verifies the job is updated with status 'completed', (5) verifies `results.pathContext.sourceContributionId` is set to `continuationId` (the actual contribution.id, not the documentIdentity), (6) explicitly asserts that `sourceContributionId !== documentIdentity` for continuation chunks, proving the function correctly handles differing values. This test should pass because the function already accepts differing values (proven by test at line 765), but it explicitly verifies the complete processing flow.
        *   `[‚úÖ]` 6.b.iii. Add a test case "passes sourceContributionId and documentIdentity to renderDocument without enforcing equality" that: (1) creates a RENDER job with payload containing `sourceContributionId: anyId` and `documentIdentity: differentId` (different values), (2) calls `processRenderJob` with the job, (3) verifies `renderDocument` is called with exactly the values from the payload (no coercion or equality checks), (4) verifies the function does not throw errors about `sourceContributionId` not equaling `documentIdentity`, (5) verifies the job completes successfully. This test explicitly proves the function does not enforce equality between the two values.
        *   `[‚úÖ]` 6.b.iv. Modify the existing test at line 122 ("processRenderJob - passes originating contribution id to renderer payload") to remove the assertion at line 150 that enforces `sourceContributionId === documentIdentity`, as this is only true for root chunks. Update the test to verify that `sourceContributionId` is passed correctly to `renderDocument` regardless of whether it equals `documentIdentity`, or split it into separate tests for root and continuation chunks.
    *   `[‚úÖ]` 6.c. `[BE]` **GREEN**: Verify the implementation in `supabase/functions/dialectic-worker/processRenderJob.ts` correctly handles both root and continuation chunks.
        *   `[‚úÖ]` 6.c.i. Verify that `processRenderJob` extracts both `sourceContributionId` and `documentIdentity` from the job payload (line 24) without enforcing equality between them.
        *   `[‚úÖ]` 6.c.ii. Verify that `processRenderJob` passes both values to `renderDocument` (lines 55-63) without modification or coercion.
        *   `[‚úÖ]` 6.c.iii. Verify that `processRenderJob` saves `sourceContributionId` to `results.pathContext.sourceContributionId` (line 83) from the render result, not from `documentIdentity`, ensuring the actual contribution.id is preserved.
        *   `[‚úÖ]` 6.c.iv. Verify that the function does not contain any validation logic that requires `sourceContributionId === documentIdentity`, as this would incorrectly reject continuation chunk payloads.
    *   `[‚úÖ]` 6.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 6.b and ensure they pass. The tests should prove that `processRenderJob` correctly handles both root and continuation chunks, passing both values to `renderDocument` and saving the correct `sourceContributionId` to job results.
    *   `[‚úÖ]` 6.e. `[TEST-INT]` Prove `processRenderJob` works correctly with its producer and consumer for both root and continuation chunks.
        *   `[‚úÖ]` 6.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (producer) enqueues a RENDER job with root chunk payload (`sourceContributionId === documentIdentity`), `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` (test subject) successfully processes it, calls `renderDocument` with correct parameters, and updates the job status to 'completed', and the job results (consumer) contain the correct `pathContext` with `sourceContributionId` matching the root's contribution.id. Create an integration test that: (1) simulates `executeModelCallAndSave` creating a root chunk and enqueuing a RENDER job, (2) processes the RENDER job via `processRenderJob`, (3) verifies the job completes successfully, (4) verifies `results.pathContext.sourceContributionId` equals the root's contribution.id.
        *   `[‚úÖ]` 6.e.ii. Assert that when `executeModelCallAndSave` (producer) enqueues a RENDER job with continuation chunk payload (`sourceContributionId !== documentIdentity`), `processRenderJob` (test subject) successfully processes it, calls `renderDocument` with correct parameters (finds all related chunks via `documentIdentity`), and updates the job status to 'completed', and the job results (consumer) contain the correct `pathContext` with `sourceContributionId` matching the continuation chunk's contribution.id (not the documentIdentity). Create an integration test that: (1) simulates `executeModelCallAndSave` creating a continuation chunk and enqueuing a RENDER job, (2) processes the RENDER job via `processRenderJob`, (3) verifies the job completes successfully, (4) verifies `results.pathContext.sourceContributionId` equals the continuation chunk's contribution.id (different from documentIdentity), (5) verifies the rendered document contains content from all related chunks (proving `documentIdentity` was used correctly for querying).
    *   `[‚úÖ]` 6.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processRenderJob.test.ts` and resolve any warnings or errors.
    *   `[‚úÖ]` 6.g. `[CRITERIA]` All requirements are met: (1) Tests explicitly verify `processRenderJob` correctly handles root chunk payloads where `sourceContributionId === documentIdentity`, (2) Tests explicitly verify `processRenderJob` correctly handles continuation chunk payloads where `sourceContributionId !== documentIdentity`, (3) Tests verify the function passes both values to `renderDocument` without enforcing equality, (4) Tests verify the function saves the correct `sourceContributionId` to job results regardless of whether it equals `documentIdentity`, (5) The existing test at line 122 is updated to remove incorrect equality assertions, (6) All tests pass, including new tests that explicitly assert the relationship between `sourceContributionId` and `documentIdentity`, (7) Integration tests prove `processRenderJob` works correctly with `executeModelCallAndSave` for both root and continuation chunks, (8) The file is lint-clean, (9) The RENDER job processing pipeline is proven to handle both root and continuation chunks correctly.

*   `[‚úÖ]` 7. **`[TEST-INT]` Prove End-to-End RENDER Job Processing for Root and Continuation Chunks**
    *   `[‚úÖ]` 7.a. `[DEPS]` The end-to-end flow for RENDER jobs involves: (1) `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` creating a contribution and enqueuing a RENDER job with payload containing `documentKey`, `sourceContributionId`, and `documentIdentity`, (2) `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` processing the RENDER job and calling `renderDocument`, (3) `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` assembling and rendering the document, (4) the rendered document being saved to storage and a `dialectic_project_resources` record being created with `source_contribution_id` matching the `sourceContributionId`. The existing integration test in `supabase/integration_tests/services/executeModelCallAndSave.integration.test.ts` at line 537 ("should process RENDER jobs successfully with corrected payload including documentKey and sourceContributionId") tests a continuation-like scenario but does not explicitly test both root and continuation chunks separately. The fix requires: (1) adding an explicit integration test that proves the entire flow works correctly for root chunks where `sourceContributionId === documentIdentity`, (2) adding an explicit integration test that proves the entire flow works correctly for continuation chunks where `sourceContributionId !== documentIdentity`, (3) verifying that rendered documents are saved correctly for both chunk types, (4) verifying that `dialectic_project_resources` records are created with the correct `source_contribution_id` for both chunk types, (5) ensuring both tests explicitly assert the relationship between `sourceContributionId` and `documentIdentity` throughout the flow.
    *   `[‚úÖ]` 7.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/executeModelCallAndSave.integration.test.ts`, add integration tests that prove end-to-end RENDER job processing for root and continuation chunks.
        *   `[‚úÖ]` 7.b.i. Add an integration test "should process RENDER jobs end-to-end for root chunks where sourceContributionId equals documentIdentity" that: (1) creates a root chunk contribution via `executeModelCallAndSave` with `document_relationships` set to `{ [stageSlug]: contribution.id }` (so `documentIdentity` equals `contribution.id`), (2) verifies a RENDER job is enqueued with payload containing `sourceContributionId: rootContributionId` and `documentIdentity: rootContributionId` (both equal), (3) processes the RENDER job via `processRenderJob`, (4) verifies `renderDocument` is called and successfully renders the document, (5) verifies a rendered markdown file is saved to storage, (6) verifies a `dialectic_project_resources` record is created with `resource_type = 'rendered_document'` and `source_contribution_id = rootContributionId`, (7) verifies the RENDER job status is 'completed' with `results.pathContext.sourceContributionId = rootContributionId`, (8) explicitly asserts that `sourceContributionId === documentIdentity` for root chunks throughout the flow. This test should pass because root chunks are already handled correctly.
        *   `[‚úÖ]` 7.b.ii. Add an integration test "should process RENDER jobs end-to-end for continuation chunks where sourceContributionId differs from documentIdentity" that: (1) creates a root chunk contribution with `id: rootContributionId` and `document_relationships: { [stageSlug]: rootContributionId }`, (2) creates a continuation chunk contribution via `executeModelCallAndSave` with `id: continuationContributionId` (different from rootContributionId), `target_contribution_id: rootContributionId`, and `document_relationships: { [stageSlug]: rootContributionId }` (same semantic identifier as root), (3) verifies a RENDER job is enqueued with payload containing `sourceContributionId: continuationContributionId` (this chunk's contribution.id) and `documentIdentity: rootContributionId` (the root's contribution.id from document_relationships), (4) verifies `sourceContributionId !== documentIdentity` in the payload, (5) processes the RENDER job via `processRenderJob`, (6) verifies `renderDocument` is called with both values and successfully finds all related chunks (root and continuation) using `documentIdentity`, (7) verifies the rendered document contains content from both chunks, (8) verifies a rendered markdown file is saved to storage, (9) verifies a `dialectic_project_resources` record is created with `resource_type = 'rendered_document'` and `source_contribution_id = continuationContributionId` (the actual contribution.id, not the documentIdentity), (10) verifies the RENDER job status is 'completed' with `results.pathContext.sourceContributionId = continuationContributionId`, (11) explicitly asserts that `sourceContributionId !== documentIdentity` for continuation chunks throughout the flow, proving the entire pipeline correctly handles differing values. This test should pass because continuation chunks are already handled correctly, but it explicitly proves the end-to-end flow.
        *   `[‚úÖ]` 7.b.iii. Add an integration test "should render complete document chain when continuation chunk triggers render job" that: (1) creates a document chain with root chunk (id: rootId) and two continuation chunks (id: cont1Id, cont2Id), all sharing the same `documentIdentity: rootId`, (2) triggers a RENDER job via `executeModelCallAndSave` when the second continuation chunk (cont2Id) is created, (3) verifies the RENDER job payload contains `sourceContributionId: cont2Id` and `documentIdentity: rootId`, (4) processes the RENDER job via `processRenderJob`, (5) verifies `renderDocument` queries using `documentIdentity: rootId` and finds all three chunks, (6) verifies the rendered document contains content from all three chunks in correct order, (7) verifies the `dialectic_project_resources` record has `source_contribution_id = cont2Id` (the chunk that triggered the render), proving that `sourceContributionId` identifies the originating chunk while `documentIdentity` groups the entire chain for rendering. This test explicitly proves that continuation chunks can trigger renders that include the entire document chain.
    *   `[‚úÖ]` 7.c. `[BE]` **GREEN**: Verify the implementation across all functions in the RENDER job processing chain correctly handles both root and continuation chunks.
        *   `[‚úÖ]` 7.c.i. Verify that `executeModelCallAndSave` correctly enqueues RENDER jobs with `sourceContributionId` set to the actual `contribution.id` and `documentIdentity` set to the semantic identifier from `document_relationships[stageSlug]` (or falls back to `contribution.id` for root chunks), ensuring both values are present and correct for both chunk types.
        *   `[‚úÖ]` 7.c.ii. Verify that `processRenderJob` correctly extracts and passes both values to `renderDocument` without enforcing equality, ensuring continuation chunk payloads are processed correctly.
        *   `[‚úÖ]` 7.c.iii. Verify that `renderDocument` correctly uses `documentIdentity` for querying and `sourceContributionId` for `pathContext`, ensuring both root and continuation chunks result in correct document assembly and resource creation.
        *   `[‚úÖ]` 7.c.iv. Verify that `dialectic_project_resources` records are created with `source_contribution_id` matching the `sourceContributionId` from the RENDER job payload, ensuring foreign key constraints reference the actual contribution.id for both chunk types.
    *   `[‚úÖ]` 7.d. `[TEST-INT]` **GREEN**: Re-run all tests from step 7.b and ensure they pass. The tests should prove that the entire RENDER job processing pipeline works correctly for both root and continuation chunks, from job creation through document rendering to resource storage.
    *   `[‚úÖ]` 7.e. `[LINT]` Run the linter for `supabase/integration_tests/services/executeModelCallAndSave.integration.test.ts` and resolve any warnings or errors.
    *   `[‚úÖ]` 7.f. `[CRITERIA]` All requirements are met: (1) Integration tests explicitly verify end-to-end RENDER job processing for root chunks where `sourceContributionId === documentIdentity`, (2) Integration tests explicitly verify end-to-end RENDER job processing for continuation chunks where `sourceContributionId !== documentIdentity`, (3) Tests verify that rendered documents are saved correctly for both chunk types, (4) Tests verify that `dialectic_project_resources` records are created with the correct `source_contribution_id` for both chunk types, (5) Tests verify that continuation chunks can trigger renders that include the entire document chain, (6) All tests pass, including new integration tests that explicitly assert the relationship between `sourceContributionId` and `documentIdentity` throughout the end-to-end flow, (7) The file is lint-clean, (8) The entire RENDER job processing pipeline is proven to handle both root and continuation chunks correctly from job creation through document rendering to resource storage.

*   `[‚úÖ]` 8. **`[API]` Extend GetProjectResourceContentResponse to Include sourceContributionId**
    *   `[‚úÖ]` 8.a. `[DEPS]` The `GetProjectResourceContentResponse` interface in `supabase/functions/dialectic-service/dialectic.interface.ts` (around line 881) and the `getProjectResourceContent` function in `supabase/functions/dialectic-service/getProjectResourceContent.ts` (around line 89) return only `fileName`, `mimeType`, and `content`, not `source_contribution_id`. The store's `fetchStageDocumentContentLogic` in `packages/store/src/dialecticStore.documents.ts` needs `source_contribution_id` from the resource record to store it in `StageDocumentContentState.sourceContributionId`. The resource record in `dialectic_project_resources` has a `source_contribution_id` field (line 25-27 in `getProjectResourceContent.ts` queries the resource but doesn't select it). The fix requires: (1) adding `sourceContributionId: string | null` to `GetProjectResourceContentResponse` interface, (2) modifying `getProjectResourceContent` to select `source_contribution_id` from the resource record and include it in the response.
    *   `[‚úÖ]` 8.b. `[TYPES]` Add `sourceContributionId: string | null` to `GetProjectResourceContentResponse` interface in `supabase/functions/dialectic-service/dialectic.interface.ts` (around line 881).
    *   `[‚úÖ]` 8.c. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-service/getProjectResourceContent.test.ts`, add tests that prove the function returns `sourceContributionId` in the response.
        *   `[‚úÖ]` 8.c.i. Add a test case that verifies when `getProjectResourceContent` successfully fetches a resource with `source_contribution_id` set to a string value, the response includes `sourceContributionId` with that value.
        *   `[‚úÖ]` 8.c.ii. Add a test case that verifies when `getProjectResourceContent` successfully fetches a resource with `source_contribution_id` set to null, the response includes `sourceContributionId: null`.
    *   `[‚úÖ]` 8.d. `[API]` **GREEN**: In `supabase/functions/dialectic-service/getProjectResourceContent.ts`, modify the function to include `sourceContributionId` in the response.
        *   `[‚úÖ]` 8.d.i. Modify the database query (around line 25) to select `source_contribution_id` from the resource record: `.select('project_id, user_id, file_name, mime_type, storage_bucket, storage_path, source_contribution_id')`
        *   `[‚úÖ]` 8.d.ii. Include `sourceContributionId: resource.source_contribution_id ?? null` in the response object (around line 89) when constructing `GetProjectResourceContentResponse`.
    *   `[‚úÖ]` 8.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 8.c and ensure they now pass.
    *   `[‚úÖ]` 8.f. `[TEST-INT]` Prove `getProjectResourceContent` works with its consumer when returning `sourceContributionId`.
        *   `[‚úÖ]` 8.f.i. Assert that when `getProjectResourceContent` in `supabase/functions/dialectic-service/getProjectResourceContent.ts` returns `sourceContributionId` in the response, `fetchStageDocumentContentLogic` in `packages/store/src/dialecticStore.documents.ts` successfully receives it and stores it in `StageDocumentContentState.sourceContributionId`. Create an integration test that: (1) sets up a resource with `source_contribution_id` set, (2) calls `getProjectResourceContent` via the API, (3) verifies the response includes `sourceContributionId`, (4) verifies `fetchStageDocumentContentLogic` stores it correctly.
    *   `[‚úÖ]` 8.g. `[CRITERIA]` All requirements are met: (1) `GetProjectResourceContentResponse` includes `sourceContributionId` field, (2) `getProjectResourceContent` selects `source_contribution_id` from the database, (3) `getProjectResourceContent` returns `sourceContributionId` in the response (null when `source_contribution_id` is null), (4) All tests pass, including tests that verify `sourceContributionId` is returned correctly, (5) Integration tests prove the API works with the store, (6) The file is lint-clean, (7) The API now provides `sourceContributionId` to consumers.
    *   `[‚úÖ]` 8.h. `[COMMIT]` `feat(api): add sourceContributionId to GetProjectResourceContentResponse`

*   `[‚úÖ]` 9. **`[STORE]` Store sourceContributionId in StageDocumentContentState When Fetching Content**
    *   `[‚úÖ]` 9.a. `[DEPS]` The `fetchStageDocumentContentLogic` function in `packages/store/src/dialecticStore.documents.ts` (lines 340-439) fetches resource content via `api.dialectic().getProjectResourceContent({ resourceId })` and stores `baselineMarkdown` in `StageDocumentContentState`, but does not store `sourceContributionId` from the API response. The component `GeneratedContributionCard` in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` (line 296) needs `sourceContributionId` to derive `originalContributionIdToEdit` for `saveContributionEdit`. The API response now includes `sourceContributionId` (after step 8). The fix requires: (1) extracting `sourceContributionId` from the API response, (2) storing it in `StageDocumentContentState.sourceContributionId` when setting `baselineMarkdown`, (3) adding `sourceContributionId: string | null` to `StageDocumentContentState` interface in `packages/types/src/dialectic.types.ts`.
    *   `[‚úÖ]` 9.b. `[TYPES]` Add `sourceContributionId: string | null` to `StageDocumentContentState` interface in `packages/types/src/dialectic.types.ts` (around line 438). This field will store the contribution ID from `dialectic_project_resources.source_contribution_id` when content is fetched.
    *   `[‚úÖ]` 9.c. `[TEST-UNIT]` **RED**: In `packages/store/src/dialecticStore.documents.test.ts`, add tests that prove `fetchStageDocumentContentLogic` stores `sourceContributionId` in the content state.
        *   `[‚úÖ]` 9.c.i. Add a test case that verifies when `fetchStageDocumentContentLogic` successfully fetches content with `sourceContributionId: 'contrib-123'` in the API response, it stores `sourceContributionId: 'contrib-123'` in `StageDocumentContentState.sourceContributionId`.
        *   `[‚úÖ]` 9.c.ii. Add a test case that verifies when the API response has `sourceContributionId: null`, the store sets `sourceContributionId: null` in the content state.
    *   `[‚úÖ]` 9.d. `[STORE]` **GREEN**: Modify `fetchStageDocumentContentLogic` in `packages/store/src/dialecticStore.documents.ts` to extract and store `sourceContributionId`.
        *   `[‚úÖ]` 9.d.i. Extract `sourceContributionId` from the API response (around line 406) when `response.data` is available.
        *   `[‚úÖ]` 9.d.ii. Store `sourceContributionId` in `StageDocumentContentState.sourceContributionId` when setting `baselineMarkdown` (around line 412 in the `reapplyDraftToNewBaseline` call or when initializing the entry in the `set` callback).
    *   `[‚úÖ]` 9.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 9.c and ensure they now pass.
    *   `[‚úÖ]` 9.f. `[TEST-INT]` Prove `fetchStageDocumentContentLogic` works with its producer and consumer when storing `sourceContributionId`.
        *   `[‚úÖ]` 9.f.i. Assert that when `getProjectResourceContent` in `getProjectResourceContent.ts` (producer) returns `sourceContributionId` in the response, `fetchStageDocumentContentLogic` in `dialecticStore.documents.ts` (test subject) successfully stores it in `StageDocumentContentState.sourceContributionId`, and `selectStageDocumentResource` in `dialecticStore.selectors.ts` (consumer) provides it to `GeneratedContributionCard` in `GeneratedContributionCard.tsx`. Create an integration test that: (1) sets up a resource with `source_contribution_id`, (2) calls `fetchStageDocumentContentLogic`, (3) verifies `sourceContributionId` is stored in the state, (4) verifies `selectStageDocumentResource` returns it.
    *   `[‚úÖ]` 9.g. `[CRITERIA]` All requirements are met: (1) `StageDocumentContentState` includes `sourceContributionId` field, (2) `fetchStageDocumentContentLogic` extracts `sourceContributionId` from the API response, (3) `sourceContributionId` is stored in `StageDocumentContentState.sourceContributionId` when content is fetched, (4) Tests verify the store correctly stores `sourceContributionId` from the API response, (5) Integration tests prove the store works with the API and selectors, (6) The file is lint-clean, (7) The store now provides `sourceContributionId` to components via `selectStageDocumentResource`.
    *   `[‚úÖ]` 9.h. `[COMMIT]` `feat(store): store sourceContributionId in StageDocumentContentState when fetching content`

*   `[‚úÖ]` 10. **`[UI]` Update GeneratedContributionCard to Use sourceContributionId for originalContributionIdToEdit**
    *   `[‚úÖ]` 10.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` (line 296) uses `documentResourceState?.lastBaselineVersion?.resourceId` for `originalContributionIdToEdit`, but this is the resource ID (like `'path/to/a1.md'`), not the contribution ID (like `'contrib-orig-123'`). Per the architecture, continuation chunks concatenate to the original, and rendered files are saved to the original identity via `dialectic_project_resources.source_contribution_id`. The component must use the contribution ID from `source_contribution_id`, not the resource ID. The store now provides `sourceContributionId` in `StageDocumentContentState` (after step 9). The fix requires: (1) updating `handleSaveEdit` to use `documentResourceState?.sourceContributionId` instead of `documentResourceState?.lastBaselineVersion?.resourceId` for `originalContributionIdToEdit`.
    *   `[‚úÖ]` 10.b. `[TYPES]` No type changes needed - the component uses existing types from the store.
    *   `[‚úÖ]` 10.c. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, update the test to verify `originalContributionIdToEdit` is derived from `sourceContributionId`.
        *   `[‚úÖ]` 10.c.i. Update the test at line 591 ("should call saveContributionEdit with originalContributionIdToEdit derived from resource state, not dialectic_contributions") to verify that `originalContributionIdToEdit` is derived from `sourceContributionId` in the document resource state, not from `lastBaselineVersion.resourceId`.
        *   `[‚úÖ]` 10.c.ii. Update the test setup to include `sourceContributionId: originalContributionId` in the `documentResourceState` returned by `selectStageDocumentResource` (modify `setupStore` to set `sourceContributionId` in the content state).
        *   `[‚úÖ]` 10.c.iii. Verify the test expects `saveContributionEdit` to be called with `originalContributionIdToEdit` matching the `sourceContributionId` from the document resource state.
    *   `[‚úÖ]` 10.d. `[UI]` **GREEN**: Update `GeneratedContributionCard` in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` to use `sourceContributionId` for `originalContributionIdToEdit`.
        *   `[‚úÖ]` 10.d.i. Update `handleSaveEdit` callback (around line 296) to use `documentResourceState?.sourceContributionId` instead of `documentResourceState?.lastBaselineVersion?.resourceId` for `originalContributionIdToEdit`.
    *   `[‚úÖ]` 10.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 10.c and ensure they pass.
    *   `[‚úÖ]` 10.f. `[TEST-INT]` Prove `GeneratedContributionCard` works with its producer when using `sourceContributionId`.
        *   `[‚úÖ]` 10.f.i. Assert that when `selectStageDocumentResource` in `dialecticStore.selectors.ts` (producer) returns `sourceContributionId` in the document resource state, the component `GeneratedContributionCard` in `GeneratedContributionCard.tsx` (test subject) uses it for `originalContributionIdToEdit` and calls `saveContributionEdit` with the correct contribution ID. Create an integration test that: (1) sets up store state with `sourceContributionId` in the document resource state, (2) renders the component, (3) triggers `handleSaveEdit`, (4) verifies `saveContributionEdit` is called with `originalContributionIdToEdit` matching `sourceContributionId`.
    *   `[‚úÖ]` 10.g. `[CRITERIA]` All requirements are met: (1) The component uses `sourceContributionId` from the document resource state for `originalContributionIdToEdit`, (2) The component no longer uses `lastBaselineVersion.resourceId` (resource ID) for the contribution ID, (3) Tests verify the component correctly derives the contribution ID from `sourceContributionId`, (4) All tests pass, including the updated test that verifies `saveContributionEdit` is called with the correct contribution ID, (5) Integration tests prove the component works with the store, (6) The file is lint-clean, (7) The component can now correctly save edits using the contribution ID from the resource's `source_contribution_id`.
    *   `[‚úÖ]` 10.h. `[COMMIT]` `fix(ui): use sourceContributionId instead of resourceId for originalContributionIdToEdit in GeneratedContributionCard`

*   `[‚úÖ]` 11. **`[BE]` Fix File Path Collision Between Root and Continuation Chunks**
    *   `[‚úÖ]` 11.a. `[DEPS]` The `constructStoragePath` function in `supabase/functions/_shared/utils/path_constructor.ts` (lines 285-290) constructs storage paths for document chunks. For `FileType.ModelContributionRawJson` with `documentKey`, it uses `isContinuation` and `turnIndex` to determine the filename and storage directory. Root chunks (non-continuations) should be saved to `{stageRootPath}/raw_responses/` with filename `{modelSlug}_{attemptCount}_{documentKey}_raw.json` (no continuation suffix). Continuation chunks should be saved to `{stageRootPath}/_work/raw_responses/` with filename `{modelSlug}_{attemptCount}_{documentKey}_continuation_{turnIndex}_raw.json` where `turnIndex` must be > 0. The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (line 1161) sets `turnIndex: isContinuationForStorage ? job.payload.continuation_count ?? 0 : undefined`, which defaults to `0` when `continuation_count` is missing. This causes continuation chunks to have `turnIndex: 0`, creating a path collision risk. Additionally, `path_constructor.ts` does not validate that `turnIndex > 0` for continuations, allowing invalid paths. The fix requires: (1) adding validation in `path_constructor.ts` that throws an error if `isContinuation === true` and `turnIndex` is undefined, not a number, or <= 0, (2) ensuring the continuation suffix is only added when `isContinuation === true` AND `turnIndex > 0`, (3) ensuring root chunks and continuation chunks use different storage directories (`raw_responses/` vs `_work/raw_responses/`), (4) ensuring continuation chunks always have unique paths based on `turnIndex`.
    *   `[‚úÖ]` 11.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/path_constructor.test.ts`, add tests that prove path construction prevents collisions between root and continuation chunks.
        *   `[‚úÖ]` 11.b.i. Add a test case that verifies when `isContinuation: false` and `turnIndex: undefined`, the constructed path for `FileType.ModelContributionRawJson` with `documentKey` does NOT include a continuation suffix in the filename and uses storage path `{stageRootPath}/raw_responses/`.
        *   `[‚úÖ]` 11.b.ii. Add a test case that verifies when `isContinuation: true` and `turnIndex: 1`, the constructed path includes `_continuation_1` suffix in the filename and uses storage path `{stageRootPath}/_work/raw_responses/`.
        *   `[‚úÖ]` 11.b.iii. Add a test case that verifies when `isContinuation: true` and `turnIndex: 2`, the constructed path includes `_continuation_2` suffix in the filename and uses storage path `{stageRootPath}/_work/raw_responses/`.
        *   `[‚úÖ]` 11.b.iv. Add a test case that verifies when `isContinuation: true` and `turnIndex: undefined`, the function throws an error indicating `turnIndex` is required and must be > 0 for continuations.
        *   `[‚úÖ]` 11.b.v. Add a test case that verifies when `isContinuation: true` and `turnIndex: 0`, the function throws an error indicating `turnIndex` must be > 0 for continuations.
        *   `[‚úÖ]` 11.b.vi. Add a test case that verifies when `isContinuation: true` and `turnIndex: -1`, the function throws an error indicating `turnIndex` must be > 0 for continuations.
        *   `[‚úÖ]` 11.b.vii. Add a test case that verifies root chunk path and continuation chunk path (with `turnIndex: 1`) are different, proving no collision is possible.
        *   `[‚úÖ]` 11.b.viii. Add a test case that verifies multiple continuation chunks with different `turnIndex` values (1, 2, 3) have unique paths, proving no collision between continuation chunks.
    *   `[‚úÖ]` 11.c. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/path_constructor.ts`, add validation to prevent path collisions.
        *   `[‚úÖ]` 11.c.i. Before line 287 (where `continuationSuffix` is constructed), add validation: if `isContinuation === true`, verify that `turnIndex` is defined, is a number, and is > 0. If any condition fails, throw an error with a clear message indicating that `turnIndex` is required and must be > 0 for continuation chunks.
        *   `[‚úÖ]` 11.c.ii. At line 287, ensure `continuationSuffix` is only constructed when `isContinuation === true` AND `turnIndex > 0` (after validation passes). The format should be `_continuation_${turnIndex}`.
        *   `[‚úÖ]` 11.c.iii. At line 289, ensure root chunks (`isContinuation === false`) use storage path `{stageRootPath}/raw_responses/` and continuation chunks (`isContinuation === true`) use storage path `{stageRootPath}/_work/raw_responses/`.
        *   `[‚úÖ]` 11.c.iv. Verify that the function never allows `turnIndex: 0` or `turnIndex: undefined` for continuation chunks, ensuring all continuation paths are unique and distinct from root chunk paths.
    *   `[‚úÖ]` 11.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 11.b and ensure they now pass. The validation should prevent invalid `turnIndex` values for continuations, and paths should be guaranteed to be unique.
    *   `[‚úÖ]` 11.e. `[TEST-INT]` Prove `constructStoragePath` works correctly with its producer when preventing path collisions.
        *   `[‚úÖ]` 11.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (producer) calls `constructStoragePath` with root chunk parameters (`isContinuation: false`, `turnIndex: undefined`), `constructStoragePath` in `supabase/functions/_shared/utils/path_constructor.ts` (test subject) returns a path in `raw_responses/` without a continuation suffix, and when called with continuation chunk parameters (`isContinuation: true`, `turnIndex: 1`), it returns a path in `_work/raw_responses/` with `_continuation_1` suffix, and the two paths are different. Create an integration test that: (1) calls `executeModelCallAndSave` to create a root chunk, (2) verifies the root chunk's storage path is in `raw_responses/` without continuation suffix, (3) calls `executeModelCallAndSave` to create a continuation chunk with `continuation_count: 1`, (4) verifies the continuation chunk's storage path is in `_work/raw_responses/` with `_continuation_1` suffix, (5) verifies the two paths are different, proving no collision.
    *   `[‚úÖ]` 11.f. `[LINT]` Run the linter for `supabase/functions/_shared/utils/path_constructor.ts` and `supabase/functions/_shared/utils/path_constructor.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 11.g. `[CRITERIA]` All requirements are met: (1) `constructStoragePath` validates that `turnIndex` is required and > 0 for continuation chunks, (2) Root chunks use `raw_responses/` directory without continuation suffix, (3) Continuation chunks use `_work/raw_responses/` directory with `_continuation_{turnIndex}` suffix where `turnIndex > 0`, (4) The function throws errors for invalid `turnIndex` values (undefined, 0, or negative) for continuations, (5) Root chunk paths and continuation chunk paths are guaranteed to be different, (6) Multiple continuation chunks with different `turnIndex` values have unique paths, (7) All tests pass, including tests that verify validation and path uniqueness, (8) Integration tests prove paths are constructed correctly and do not collide, (9) The file is lint-clean, (10) File path collisions between root and continuation chunks are impossible.

*   `[‚úÖ]` 12. **`[BE]` Fix executeModelCallAndSave to Require continuation_count for Continuation Chunks**
    *   `[‚úÖ]` 12.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (line 1161) sets `turnIndex: isContinuationForStorage ? job.payload.continuation_count ?? 0 : undefined` in the `pathContext`. This defaults to `0` when `continuation_count` is missing, which violates the requirement that continuation chunks must have `turnIndex > 0`. Root chunks must have `turnIndex: undefined` because they are not continuations. Continuation chunks must have `continuation_count` explicitly provided and > 0. The `constructStoragePath` function (after step 11) will validate that `turnIndex > 0` for continuations, but `executeModelCallAndSave` must also validate that `continuation_count` is provided before constructing the path context. The fix requires: (1) adding validation after line 1083 (`isContinuationForStorage` check) that throws an error if `isContinuationForStorage === true` and `job.payload.continuation_count` is undefined, not a number, or <= 0, (2) removing the `?? 0` default at line 1161, setting `turnIndex` to `job.payload.continuation_count` when `isContinuationForStorage === true` (validation ensures it exists and > 0), (3) ensuring root chunks always have `turnIndex: undefined`, (4) ensuring continuation chunks always have `turnIndex` set to the provided `continuation_count` value (> 0).
    *   `[‚úÖ]` 12.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, add tests that prove `executeModelCallAndSave` requires `continuation_count` for continuation chunks.
        *   `[‚úÖ]` 12.b.i. Add a test case that verifies when `executeModelCallAndSave` processes a root chunk (no `target_contribution_id`), the `pathContext` has `isContinuation: false` and `turnIndex: undefined`.
        *   `[‚úÖ]` 12.b.ii. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk with `continuation_count: 1`, the `pathContext` has `isContinuation: true` and `turnIndex: 1`.
        *   `[‚úÖ]` 12.b.iii. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk with `continuation_count: 2`, the `pathContext` has `isContinuation: true` and `turnIndex: 2`.
        *   `[‚úÖ]` 12.b.iv. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk (has `target_contribution_id`) but `continuation_count` is undefined, the function throws an error indicating `continuation_count` is required and must be > 0 for continuation chunks.
        *   `[‚úÖ]` 12.b.v. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk with `continuation_count: 0`, the function throws an error indicating `continuation_count` must be > 0 for continuation chunks.
        *   `[‚úÖ]` 12.b.vi. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk with `continuation_count: -1`, the function throws an error indicating `continuation_count` must be > 0 for continuation chunks.
        *   `[‚úÖ]` 12.b.vii. Add a test case that verifies when `executeModelCallAndSave` processes a continuation chunk with `continuation_count` as a non-number (e.g., string), the function throws an error indicating `continuation_count` must be a number > 0.
    *   `[‚úÖ]` 12.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, add validation and fix `turnIndex` assignment.
        *   `[‚úÖ]` 12.c.i. After line 1083 (where `isContinuationForStorage` is set), add validation: if `isContinuationForStorage === true`, verify that `job.payload.continuation_count` is defined, is a number, and is > 0. If any condition fails, throw an error with a clear message indicating that `continuation_count` is required and must be a number > 0 for continuation chunks.
        *   `[‚úÖ]` 12.c.ii. At line 1161, change `turnIndex: isContinuationForStorage ? job.payload.continuation_count ?? 0 : undefined` to `turnIndex: isContinuationForStorage ? job.payload.continuation_count : undefined`. Remove the `?? 0` default since validation ensures `continuation_count` exists and > 0 when `isContinuationForStorage === true`.
        *   `[‚úÖ]` 12.c.iii. Verify that root chunks (where `isContinuationForStorage === false`) always have `turnIndex: undefined` in the `pathContext`.
        *   `[‚úÖ]` 12.c.iv. Verify that continuation chunks (where `isContinuationForStorage === true`) always have `turnIndex` set to the validated `job.payload.continuation_count` value (> 0) in the `pathContext`.
    *   `[‚úÖ]` 12.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 12.b and ensure they now pass. The validation should prevent invalid `continuation_count` values, and `turnIndex` should be correctly set for both root and continuation chunks.
    *   `[‚úÖ]` 12.e. `[TEST-INT]` Prove `executeModelCallAndSave` works correctly with its consumer when requiring `continuation_count` for continuation chunks.
        *   `[‚úÖ]` 12.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (test subject) processes a root chunk and calls `fileManager.uploadAndRegisterFile` with `pathContext` containing `isContinuation: false` and `turnIndex: undefined`, `constructStoragePath` in `supabase/functions/_shared/utils/path_constructor.ts` (consumer) constructs a path in `raw_responses/` without continuation suffix, and when processing a continuation chunk with `continuation_count: 1` and calling `fileManager.uploadAndRegisterFile` with `pathContext` containing `isContinuation: true` and `turnIndex: 1`, `constructStoragePath` constructs a path in `_work/raw_responses/` with `_continuation_1` suffix. Create an integration test that: (1) calls `executeModelCallAndSave` to create a root chunk, (2) verifies the contribution's `raw_response_storage_path` is in `raw_responses/` without continuation suffix, (3) calls `executeModelCallAndSave` to create a continuation chunk with `continuation_count: 1`, (4) verifies the continuation chunk's `raw_response_storage_path` is in `_work/raw_responses/` with `_continuation_1` suffix, (5) verifies the two paths are different, proving no collision and correct path construction throughout the chain.
    *   `[‚úÖ]` 12.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 12.g. `[CRITERIA]` All requirements are met: (1) `executeModelCallAndSave` validates that `continuation_count` is required and > 0 for continuation chunks, (2) Root chunks have `isContinuation: false` and `turnIndex: undefined` in `pathContext`, (3) Continuation chunks have `isContinuation: true` and `turnIndex` set to the provided `continuation_count` value (> 0) in `pathContext`, (4) The function throws errors for invalid `continuation_count` values (undefined, 0, negative, or non-number) for continuations, (5) The `?? 0` default has been removed, preventing `turnIndex: 0` for continuations, (6) All tests pass, including tests that verify validation and correct `turnIndex` assignment, (7) Integration tests prove `executeModelCallAndSave` works correctly with `constructStoragePath` to prevent path collisions, (8) The file is lint-clean, (9) File path collisions between root and continuation chunks are impossible due to validation and correct `turnIndex` assignment.
    *   `[‚úÖ]` 12.h. `[COMMIT]` `fix(be): require continuation_count > 0 for continuation chunks and prevent file path collisions`

*   `[‚úÖ]` 13. **`[TEST-INT]` Prove FileManagerService Saves Root and Continuation Chunk Raw JSON Files to Separate Storage Paths Without Collision**
    *   `[‚úÖ]` 13.a. `[DEPS]` The `FileManagerService.uploadAndRegisterFile` function in `supabase/functions/_shared/services/file_manager.ts` (lines 50-411) is responsible for uploading files to storage and creating database records. When called with `FileType.ModelContributionRawJson` and `isContinuation: true` with `turnIndex: 1`, it should save continuation chunk raw JSON files to `{stageRootPath}/_work/raw_responses/` with filename `{modelSlug}_{attemptCount}_{documentKey}_continuation_{turnIndex}_raw.json`. When called with `isContinuation: false` (root chunks), it should save to `{stageRootPath}/raw_responses/` with filename `{modelSlug}_{attemptCount}_{documentKey}_raw.json` (no continuation suffix). The test output from `executeModelCallAndSave.integration.test.ts` shows that when `renderDocument` reads the root chunk's raw JSON file, it finds concatenated JSON objects like `{"content":"..."}{"content":"..."}`, suggesting that continuation chunks may be writing to the root chunk's storage path instead of their own unique path, or files are being concatenated during upload. The database records show correct `raw_response_storage_path` values (root chunks in `raw_responses/`, continuation chunks in `_work/raw_responses/` with `_continuation_N` suffix), but the actual storage files may contain concatenated content. The existing unit tests in `file_manager.upload.test.ts` verify path construction and database record creation, but do not verify that the actual storage files are separate and contain only single JSON objects. The fix requires: (1) creating a new integration test file `supabase/integration_tests/services/file_manager.integration.test.ts` that directly tests `FileManagerService.uploadAndRegisterFile` as the test subject, (2) writing an integration test that creates a root chunk and a continuation chunk via `uploadAndRegisterFile`, (3) verifying that the database records have correct `raw_response_storage_path` values (root in `raw_responses/` without continuation suffix, continuation in `_work/raw_responses/` with `_continuation_1` suffix), (4) downloading the actual files from storage using the `raw_response_storage_path` values, (5) verifying that each file contains only one valid JSON object (not concatenated), (6) verifying that the root chunk's file content does not equal the continuation chunk's file content (proving they are separate files), (7) verifying that both files can be downloaded successfully from their respective paths, (8) if the test fails (proving the flaw exists), fixing the implementation in `file_manager.ts` to ensure files are saved to separate paths and not concatenated, (9) re-running the test to prove the fix works.
    *   `[‚úÖ]` 13.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/file_manager.integration.test.ts` (new file), add an integration test that proves `FileManagerService.uploadAndRegisterFile` saves root and continuation chunk raw JSON files to separate storage paths without collision.
        *   `[‚úÖ]` 13.b.i. Create the test file `supabase/integration_tests/services/file_manager.integration.test.ts` following the standard integration test pattern (imports, `describe` block, `beforeAll` setup with project/session creation, `afterAll` cleanup, `it` test cases).
        *   `[‚úÖ]` 13.b.ii. Add an integration test "should save root and continuation chunk raw JSON files to separate storage paths without collision" that: (1) **Producer Setup**: Creates a `ModelContributionUploadContext` for a root chunk with `FileType.ModelContributionRawJson`, `isContinuation: false`, `turnIndex: undefined`, `documentKey: 'business_case'`, and unique JSON content (e.g., `{"content": "# Root Chunk Content"}`), (2) **Producer Setup**: Creates a `ModelContributionUploadContext` for a continuation chunk with `FileType.ModelContributionRawJson`, `isContinuation: true`, `turnIndex: 1`, `documentKey: 'business_case'`, `target_contribution_id` pointing to the root chunk's contribution ID, and different unique JSON content (e.g., `{"content": "\n\n## Continuation Chunk Content"}`), (3) **Test Subject**: Calls `fileManager.uploadAndRegisterFile(rootContext)` and verifies it succeeds, storing the root contribution record, (4) **Test Subject**: Calls `fileManager.uploadAndRegisterFile(continuationContext)` and verifies it succeeds, storing the continuation contribution record, (5) **Consumer Assertion**: Queries the database to get both contribution records and extracts `raw_response_storage_path` from each, (6) **Consumer Assertion**: Asserts that the root chunk's `raw_response_storage_path` does NOT contain `/_work/` and does NOT contain `_continuation_`, (7) **Consumer Assertion**: Asserts that the continuation chunk's `raw_response_storage_path` DOES contain `/_work/raw_responses/` and DOES contain `_continuation_1`, (8) **Consumer Assertion**: Downloads the root chunk's raw JSON file from storage using `downloadFromStorage` with the root chunk's `raw_response_storage_path`, verifies the download succeeds, and asserts the file content parses as valid JSON (single object, not concatenated), (9) **Consumer Assertion**: Downloads the continuation chunk's raw JSON file from storage using `downloadFromStorage` with the continuation chunk's `raw_response_storage_path`, verifies the download succeeds, and asserts the file content parses as valid JSON (single object, not concatenated), (10) **Consumer Assertion**: Asserts that the root chunk's file content does NOT equal the continuation chunk's file content (proving they are separate files), (11) **Consumer Assertion**: Asserts that the root chunk's file content matches the original root JSON content passed to `uploadAndRegisterFile`, (12) **Consumer Assertion**: Asserts that the continuation chunk's file content matches the original continuation JSON content passed to `uploadAndRegisterFile`. This test must initially fail if the flaw exists (files are concatenated or written to wrong paths), proving the exact condition causing the `renderDocument` error in the `executeModelCallAndSave` integration tests.
    *   `[‚úÖ]` 13.c. `[BE]` **GREEN**: If the test from step 13.b fails, fix the implementation in `supabase/functions/_shared/services/file_manager.ts` to ensure root and continuation chunk raw JSON files are saved to separate storage paths without collision.
        *   `[‚úÖ]` 13.c.i. If the test fails because files are concatenated, investigate the upload logic in `uploadAndRegisterFile` (lines 88-128) to ensure each call to `this.supabase.storage.from(this.storageBucket).upload()` writes to a unique path and does not append to existing files. Verify that `upsert: false` is used for model contributions (line 100) to prevent overwriting, and that collision handling (lines 113-124) increments `attemptCount` to create unique paths rather than concatenating content.
        *   `[‚úÖ]` 13.c.ii. If the test fails because continuation chunks write to root chunk paths, verify that `constructStoragePath` is called with the correct `pathContext` containing `isContinuation: true` and `turnIndex: 1` for continuation chunks (lines 89-93), and that the constructed path includes `/_work/raw_responses/` and `_continuation_1` suffix. Verify that `pathContextForStorage` (line 55) preserves `isContinuation` and `turnIndex` from the input context without modification.
        *   `[‚úÖ]` 13.c.iii. If the test fails because `raw_response_storage_path` in the database points to the wrong file, verify that line 252 sets `raw_response_storage_path: `${finalMainContentFilePath}/${finalFileName}`` using the actual `finalMainContentFilePath` and `finalFileName` from the successful upload (lines 110-112), not a constructed path that may differ from the actual storage location.
        *   `[‚úÖ]` 13.c.iv. Ensure that each call to `uploadAndRegisterFile` results in a single, atomic file upload to a unique storage path, and that the database record's `raw_response_storage_path` accurately reflects the actual storage location of the uploaded file.
    *   `[‚úÖ]` 13.d. `[TEST-INT]` **GREEN**: Re-run the test from step 13.b and ensure it now passes. The test should prove that root and continuation chunk raw JSON files are saved to separate storage paths, each containing only one valid JSON object, and that the database records accurately reflect the actual storage locations.
    *   `[‚úÖ]` 13.e. `[LINT]` Run the linter for `supabase/integration_tests/services/file_manager.integration.test.ts` and resolve any warnings or errors.
    *   `[‚úÖ]` 13.f. `[CRITERIA]` All requirements are met: (1) A new integration test file `file_manager.integration.test.ts` exists and follows the standard integration test pattern, (2) The integration test proves `FileManagerService.uploadAndRegisterFile` saves root chunk raw JSON files to `raw_responses/` without continuation suffix, (3) The integration test proves `FileManagerService.uploadAndRegisterFile` saves continuation chunk raw JSON files to `_work/raw_responses/` with `_continuation_N` suffix, (4) The test verifies that database records have correct `raw_response_storage_path` values, (5) The test verifies that actual storage files are separate (can be downloaded from their respective paths), (6) The test verifies that each file contains only one valid JSON object (not concatenated), (7) The test verifies that root and continuation chunk file contents are different (proving separate files), (8) The test verifies that file contents match the original JSON content passed to `uploadAndRegisterFile`, (9) If the test initially failed (proving the flaw), the implementation in `file_manager.ts` has been fixed to ensure files are saved correctly, (10) All tests pass, proving the flaw is resolved or does not exist, (11) The file is lint-clean, (12) The integration test provides definitive proof of whether `FileManagerService` correctly isolates root and continuation chunk raw JSON files in storage, which will explain the concatenated JSON error observed in `renderDocument` when reading root chunk files in the `executeModelCallAndSave` integration tests.

*   `[‚úÖ]` 14. **`[BE]` Fix Root File Corruption When Continuation Chunk Uploads**
    *   `[‚úÖ]` 14.a. `[DEPS]` The `FileManagerService.uploadAndRegisterFile` function in `supabase/functions/_shared/services/file_manager.ts` (lines 88-128) handles filename collisions for model contributions by retrying uploads with incremented `attemptCount` values. The collision handling loop (lines 88-128) attempts uploads with `attemptCount` from 0 to `MAX_UPLOAD_ATTEMPTS - 1`, constructing a new path for each attempt using `constructStoragePath(attemptPathContext)` where `attemptPathContext` spreads `pathContextForStorage` and overrides `attemptCount`. The integration test in `executeModelCallAndSave.integration.test.ts` (test 7.b.ii) proves that when a root chunk is uploaded first (80 bytes, correct content), then a continuation chunk is uploaded, the root chunk's file becomes corrupted (164 bytes = 80 + 84, concatenated JSON). The continuation chunk's database record shows the correct path (`_work/raw_responses/..._continuation_1_raw.json`), and the continuation chunk's file is correct (84 bytes), but the root chunk's file contains concatenated content. This suggests that during the continuation chunk's upload attempt loop, one of the attempts (likely `attemptCount = 0`) is constructing a path that matches the root chunk's path, attempting to upload to that path, and somehow corrupting the root file despite `upsert: false` and collision detection. The fix requires: (1) adding comprehensive logging inside the collision handling loop to capture each upload attempt's `attemptCount`, constructed path, file content length, and upload result (success/error), (2) creating unit tests that replicate the exact scenario from the integration test (upload root chunk, then upload continuation chunk, then verify root file is not corrupted), (3) using the logging and unit tests to prove the exact upload attempt that causes the corruption (which `attemptCount`, which path, what content), (4) fixing the source code to prevent the continuation chunk from attempting to upload to the root chunk's path or corrupting the root file, (5) re-running the unit tests and integration tests to prove the fix works.
    *   `[‚úÖ]` 14.b. `[BE]` Add comprehensive logging to `FileManagerService.uploadAndRegisterFile` to capture each upload attempt's behavior.
        *   `[‚úÖ]` 14.b.i. Inside the collision handling loop (lines 88-128), before the `upload` call (line 96), add logging that captures: `attemptCount`, `isContinuation` from `attemptPathContext`, `turnIndex` from `attemptPathContext`, the constructed `fullPathForUpload`, the `fileContent` length, and a hash or first/last 50 characters of `fileContent` for verification.
        *   `[‚úÖ]` 14.b.ii. After the `upload` call (line 96), add logging that captures: the upload result (success/error), the error message if present, whether the upload succeeded or failed, and if it failed, whether it was a 409 collision error.
        *   `[‚úÖ]` 14.b.iii. After the loop completes (line 128), add logging that captures: the final `finalMainContentFilePath`, `finalFileName`, and `raw_response_storage_path` that will be stored in the database.
        *   `[‚úÖ]` 14.b.iv. Ensure all logging uses a consistent prefix (e.g., `[FileManagerService] UPLOAD_ATTEMPT`) to enable filtering and analysis of the upload sequence.
    *   `[‚úÖ]` 14.c. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/file_manager.upload.test.ts`, add unit tests that replicate the exact scenario from the integration test to prove the flaw exists.
        *   `[‚úÖ]` 14.c.i. Add a test case "root file is corrupted when continuation chunk uploads after root chunk" that: (1) creates a mock Supabase storage client that tracks all upload calls, (2) creates a `ModelContributionUploadContext` for a root chunk with `isContinuation: false`, `turnIndex: undefined`, `documentKey: 'business_case'`, and unique JSON content `rootContent = '{"content":"# Root Chunk"}'`, (3) calls `fileManager.uploadAndRegisterFile(rootContext)` and verifies it succeeds, storing the root contribution record and capturing the root file's storage path, (4) creates a `ModelContributionUploadContext` for a continuation chunk with `isContinuation: true`, `turnIndex: 1`, `documentKey: 'business_case'`, `target_contribution_id` pointing to the root chunk's contribution ID, and different unique JSON content `continuationContent = '{"content":"\\n\\n## Continuation"}'`, (5) calls `fileManager.uploadAndRegisterFile(continuationContext)` and verifies it succeeds, storing the continuation contribution record, (6) queries the mock storage to get the root file's content using the root chunk's `storage_path` and `file_name` (canonical access method: `${rootChunk.storage_path}/${rootChunk.file_name}`), (7) asserts that the root file's content equals `rootContent` (not concatenated), (8) queries the mock storage to get the continuation file's content using the continuation chunk's `storage_path` and `file_name` (canonical access method: `${continuationChunk.storage_path}/${continuationChunk.file_name}`), (9) asserts that the continuation file's content equals `continuationContent`, (10) asserts that the root file's content does NOT equal the continuation file's content, (11) verifies from the mock storage that no upload attempt targeted the root file's path during the continuation chunk upload (proving the continuation chunk did not attempt to write to the root path). This test must initially fail if the flaw exists, proving that the root file is corrupted when the continuation chunk uploads.
        *   `[‚úÖ]` 14.c.ii. Add a test case "continuation chunk upload attempts are logged with correct path context" that: (1) creates a mock logger that captures all log calls, (2) creates a root chunk and uploads it, (3) creates a continuation chunk and uploads it, (4) verifies the logger captured upload attempt logs for the continuation chunk, (5) verifies each log entry includes `attemptCount`, `isContinuation: true`, `turnIndex: 1`, the constructed path, and the file content length, (6) verifies that no upload attempt log shows a path matching the root chunk's path (proving the continuation chunk did not attempt to upload to the root path), (7) verifies that all upload attempt paths for the continuation chunk include `/_work/raw_responses/` and `_continuation_1` suffix. This test proves the logging captures the necessary information to diagnose the flaw.
        *   `[‚úÖ]` 14.c.iii. Add a test case "continuation chunk upload with collision retry does not corrupt root file" that: (1) creates a mock storage client that returns 409 collision errors for the first continuation chunk upload attempt (`attemptCount = 0`), then succeeds on the second attempt (`attemptCount = 1`), (2) creates and uploads a root chunk, (3) creates and uploads a continuation chunk (which will trigger collision retry), (4) verifies the root file's content is still correct (not corrupted), (5) verifies the continuation chunk's database record shows the correct path (with `attemptCount = 1` in the filename), (6) verifies from the mock storage that the first upload attempt (`attemptCount = 0`) targeted a path that does NOT match the root chunk's path, (7) verifies the second upload attempt (`attemptCount = 1`) succeeded and targeted the continuation chunk's unique path. This test proves that collision retries do not cause root file corruption.
    *   `[‚úÖ]` 14.d. `[BE]` **GREEN**: Use the logging and unit tests to identify and fix the exact flaw causing root file corruption.
        *   `[‚úÖ]` 14.d.i. Run the unit tests from step 14.c and analyze the logging output to identify the exact upload attempt that causes the root file corruption. Determine: (1) which `attemptCount` value constructs a path matching the root chunk's path, (2) whether the upload succeeds or fails for that attempt, (3) what content is being written during that attempt, (4) whether `upsert: false` is being respected, (5) whether the collision detection is working correctly.
        *   `[‚úÖ]` 14.d.ii. Based on the proven flaw from step 14.d.i, fix the implementation in `supabase/functions/_shared/services/file_manager.ts`. Possible fixes include: (1) ensuring `attemptPathContext` preserves `isContinuation` and `turnIndex` correctly when spreading `pathContextForStorage` (verify the spread operator does not lose these values), (2) ensuring `constructStoragePath` uses `isContinuation` and `turnIndex` from `attemptPathContext` correctly (verify the path construction logic), (3) ensuring the collision handling loop does not attempt to upload to paths that match existing root chunk paths (add explicit path validation), (4) ensuring `upsert: false` prevents any write to existing files (verify Supabase Storage behavior), (5) ensuring failed upload attempts do not corrupt existing files (verify error handling).
        *   `[‚úÖ]` 14.d.iii. Verify that the fix ensures continuation chunks never attempt to upload to root chunk paths, and that root files are never modified after their initial successful upload.
        *   `[‚úÖ]` 14.d.iv. Verify that the fix preserves all existing functionality: root chunks still upload correctly, continuation chunks still upload correctly, collision handling still works, database records are still created correctly.
    *   `[‚úÖ]` 14.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 14.c and ensure they now pass. The tests should prove that root files are not corrupted when continuation chunks upload, that logging captures all necessary information, and that collision retries work correctly without corrupting root files.
    *   `[‚úÖ]` 14.f. `[TEST-INT]` Re-run the integration test from step 7.b.ii and verify it now passes. The integration test should prove that the end-to-end flow works correctly: root chunks upload and remain uncorrupted, continuation chunks upload to their own paths, and `renderDocument` can successfully read root chunk files without encountering concatenated JSON.
    *   `[‚úÖ]` 14.g. `[LINT]` Run the linter for `supabase/functions/_shared/services/file_manager.ts` and `supabase/functions/_shared/services/file_manager.upload.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 14.h. `[CRITERIA]` All requirements are met: (1) Comprehensive logging has been added to capture each upload attempt's `attemptCount`, path, content, and result, (2) Unit tests replicate the exact scenario from the integration test and prove the flaw exists (root file corruption), (3) The logging and unit tests prove the exact upload attempt causing the corruption (which `attemptCount`, which path, what behavior), (4) The source code has been fixed to prevent continuation chunks from corrupting root files, (5) All unit tests pass, proving the fix works, (6) The integration test from step 7.b.ii passes, proving the end-to-end flow works correctly, (7) Root files remain uncorrupted after continuation chunk uploads, (8) Continuation chunks upload to their own unique paths without attempting to write to root chunk paths, (9) The file is lint-clean, (10) The exact flaw causing root file corruption has been proven and fixed.

*   `[‚úÖ]` 15. **`[BE]` Fix assembleAndSaveFinalDocument to Read from storage_path/file_name (Canonical Access Method), Concatenate JSON Properly, and Save to AssembledDocumentJson Path**
    *   `[‚úÖ]` 15.a. `[DEPS]` The `assembleAndSaveFinalDocument` function in `supabase/functions/_shared/services/file_manager.ts` (lines 487-582) is dead code from the old architecture that concatenates raw JSON chunks and saves them as a single file. In the new doc-centric architecture, raw JSON chunks are stored separately and the document renderer (`renderDocument`) reads them and renders them into markdown via RENDER jobs. However, `assembleAndSaveFinalDocument` is still being called unconditionally in `executeModelCallAndSave` (line 1420) for all final chunks, which corrupts root raw JSON files by concatenating JSON strings (creating invalid concatenated JSON like `{"content":"..."}{"content":"..."}`) and overwriting the root file with `upsert: true`. The function should only be called for JSON-only artifacts (like `AssembledDocumentJson`) that don't get rendered to markdown, not for rendered documents. For JSON-only artifacts, `storage_path`/`file_name` points to the raw JSON file (the canonical access method for the file the contribution represents). The function currently reads from `storage_path`/`file_name` which is correct for JSON-only artifacts, but concatenates JSON strings directly (creating invalid JSON), and saves to the root chunk's path (overwriting raw JSON files). The fix requires: (1) ensuring `assembleAndSaveFinalDocument` reads from `storage_path`/`file_name` (the canonical access method), (2) fixing `assembleAndSaveFinalDocument` to merge JSON properly by creating a valid merged JSON object (not string concatenation), (3) fixing `assembleAndSaveFinalDocument` to save to a new `AssembledDocumentJson` path in `_work/assembled_json/` (not overwrite raw JSON files), (4) ensuring the function creates valid JSON that can be parsed and used by consumers, (5) updating tests to verify proper JSON merging and correct path usage.
    *   `[‚úÖ]` 15.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/file_manager.assemble.test.ts`, add tests that prove `assembleAndSaveFinalDocument` merges JSON properly and saves to the correct path.
        *   `[‚úÖ]` 15.b.i. Add a test case "assembles JSON chunks into valid merged JSON object" that: (1) creates a root chunk contribution with `storage_path: 'path/to'`, `file_name: 'root_raw.json'` and mock storage content `'{"content":"# Root"}'`, (2) creates a continuation chunk contribution with `storage_path: 'path/to'`, `file_name: 'continuation_1_raw.json'` and mock storage content `'{"content":"\\n\\n## Continuation"}'`, (3) calls `assembleAndSaveFinalDocument(rootContributionId)`, (4) verifies the function reads from `storage_path`/`file_name` (the canonical access method: `${chunk.storage_path}/${chunk.file_name}`), (5) verifies the function downloads both files from storage, (6) verifies the function merges the JSON objects into a valid merged JSON object: `{"content":"# Root\\n\\n## Continuation"}` (concatenating string values for 'content' key), (7) verifies the function uploads the assembled JSON to a path in `_work/assembled_json/` (not the root chunk's path), (8) verifies the uploaded content can be parsed as valid JSON using `JSON.parse()`, (9) verifies the parsed JSON is a merged object (not an array) with concatenated content from both chunks.
        *   `[‚úÖ]` 15.b.ii. Add a test case "assembles JSON chunks into valid merged JSON object when chunks contain compatible keys" that: (1) creates chunks with JSON objects containing different keys (e.g., root: `{"title":"Document","content":"# Root"}`, continuation: `{"content":"\\n\\n## Continuation","metadata":{"author":"AI"}}`), (2) calls `assembleAndSaveFinalDocument`, (3) verifies the function creates a valid merged JSON object: `{"title":"Document","content":"# Root\\n\\n## Continuation","metadata":{"author":"AI"}}` (merging compatible keys, concatenating string values for 'content'), (4) verifies the uploaded content can be parsed as valid JSON, (5) verifies the parsed JSON is an object with all expected keys.
        *   `[‚úÖ]` 15.b.iii. Add a test case "saves assembled JSON to AssembledDocumentJson path, not raw JSON path" that: (1) creates root and continuation chunks with `storage_path` and `file_name` values, (2) calls `assembleAndSaveFinalDocument`, (3) verifies the function constructs a path using `FileType.AssembledDocumentJson` (not `FileType.ModelContributionRawJson`), (4) verifies the path includes `/_work/assembled_json/` directory, (5) verifies the path does NOT match the root chunk's canonical path (`${rootChunk.storage_path}/${rootChunk.file_name}`), (6) verifies the function uploads to the new path (not overwriting raw JSON files), (7) verifies the root chunk's raw JSON file remains unchanged (can still be downloaded using `${rootChunk.storage_path}/${rootChunk.file_name}` with original content).
        *   `[‚úÖ]` 15.b.iv. Add a test case "handles single chunk (root only, no continuations)" that: (1) creates only a root chunk with no continuation chunks, (2) calls `assembleAndSaveFinalDocument`, (3) verifies the function creates a valid JSON object matching the root chunk: `{"content":"# Root"}` (no merge needed for single chunk), (4) verifies the assembled JSON is saved to the correct `AssembledDocumentJson` path.
        *   `[‚úÖ]` 15.b.v. Add a test case "throws error when storage_path or file_name is missing" that: (1) creates a chunk with `storage_path: null` or `file_name: null`, (2) calls `assembleAndSaveFinalDocument`, (3) verifies the function throws an error indicating `storage_path` and `file_name` are required.
        *   `[‚úÖ]` 15.b.vi. Add a test case "throws error when raw JSON file cannot be downloaded" that: (1) creates chunks with `storage_path` and `file_name` values, (2) mocks storage download to return an error for one chunk when downloading from `${chunk.storage_path}/${chunk.file_name}`, (3) calls `assembleAndSaveFinalDocument`, (4) verifies the function throws an error indicating the download failed.
        *   `[‚úÖ]` 15.b.vii. Add a test case "throws error when raw JSON content is invalid JSON" that: (1) creates chunks with `storage_path` and `file_name` values, (2) mocks storage content to return invalid JSON (e.g., `'not json'`) when downloading from `${chunk.storage_path}/${chunk.file_name}`, (3) calls `assembleAndSaveFinalDocument`, (4) verifies the function throws an error indicating the JSON is invalid.
    *   `[‚úÖ]` 15.c. `[BE]` **GREEN**: In `supabase/functions/_shared/services/file_manager.ts`, fix `assembleAndSaveFinalDocument` to read from `storage_path`/`file_name` (canonical access method), concatenate JSON properly, and save to the correct path.
        *   `[‚úÖ]` 15.c.i. Ensure the database query (line 505) selects `storage_path` and `file_name`: `.select('id, document_relationships, storage_path, file_name, created_at, target_contribution_id')`. Do not select `raw_response_storage_path` as it is not the canonical access method.
        *   `[‚úÖ]` 15.c.ii. In the download loop (lines 530-538), ensure it uses `storage_path`/`file_name` (the canonical access method). Verify the code constructs the path as `const fullPath = `${chunk.storage_path}/${chunk.file_name}`;` and add validation: check if `chunk.storage_path` and `chunk.file_name` exist and are strings, throw an error if either is missing, then use the constructed path.
        *   `[‚úÖ]` 15.c.iii. Replace the string concatenation logic (line 538: `finalContent += await data.text();`) with proper JSON merging. After downloading all chunks, parse each JSON string into an object using `JSON.parse()`, then merge all parsed objects into a single object using a merge strategy: concatenate string values for 'content' key, deep-merge nested objects, and override/add other values. Stringify the merged object: `const parsedChunks = orderedChunks.map(chunk => JSON.parse(await downloadText(...))); const mergedObject = mergeObjects(parsedChunks[0], ...parsedChunks.slice(1)); const finalContent = JSON.stringify(mergedObject);`. This creates a valid merged JSON object like `{"content":"...concatenated..."}` instead of invalid concatenated strings.
        *   `[‚úÖ]` 15.c.iv. Replace the upload path logic (lines 543-550). Instead of uploading to the root chunk's `storage_path`/`file_name` (which would overwrite raw JSON files), construct a new path using `constructStoragePath` with `FileType.AssembledDocumentJson`. Extract path context from the root chunk (projectId, sessionId, iteration, stageSlug, documentKey, modelSlug, attemptCount) and call `constructStoragePath({ ...pathContext, fileType: FileType.AssembledDocumentJson })` to get the correct path in `_work/assembled_json/`. Use this path for the upload instead of the root chunk's path.
        *   `[‚úÖ]` 15.c.v. Change the upload `contentType` from `'text/markdown'` (line 548) to `'application/json'` since we're now saving merged JSON objects.
        *   `[‚úÖ]` 15.c.vi. Add error handling for JSON parsing: wrap `JSON.parse()` calls in try-catch blocks and throw descriptive errors if any chunk's content is not valid JSON.
        *   `[‚úÖ]` 15.c.vii. Verify that the function never modifies raw JSON files (only reads from them), and always saves assembled JSON to a separate `AssembledDocumentJson` path.
    *   `[‚úÖ]` 15.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 15.b and ensure they now pass. The tests should prove that `assembleAndSaveFinalDocument` merges JSON properly into valid merged JSON objects, and that it saves to the correct `AssembledDocumentJson` path without corrupting raw JSON files.
    *   `[‚úÖ]` 15.e. `[TEST-INT]` Prove `assembleAndSaveFinalDocument` works correctly with its producer and consumer for all code paths. Create a new integration test file `supabase/integration_tests/services/file_manager.assemble.integration.test.ts` following the standard integration test pattern (imports, `describe` block, `beforeAll` setup with project/session creation, `afterAll` cleanup, `it` test cases). This file will contain integration tests specific to `assembleAndSaveFinalDocument` that exercise real code paths with producer ‚Üí test subject ‚Üí consumer flows.
        *   `[‚úÖ]` 15.e.i. Add an integration test "should call assembleAndSaveFinalDocument for JSON-only artifacts and create valid assembled JSON" that: (1) **Producer Setup**: Creates a root chunk contribution via `executeModelCallAndSave` with a JSON-only artifact output type (e.g., `FileType.HeaderContext` or a custom JSON artifact) that triggers `shouldEnqueueRenderJob` to return `false`, sets `resolvedFinish: 'stop'` to make it a final chunk, and sets `document_relationships: { [stageSlug]: rootContributionId }`, with unique JSON content `rootContent = '{"header":"Root Header","context":{"key":"value"}}'`, (2) **Producer Setup**: Creates a continuation chunk contribution via `executeModelCallAndSave` with the same output type, `target_contribution_id` pointing to the root chunk, `resolvedFinish: 'stop'`, and different unique JSON content `continuationContent = '{"header":"Continuation Header","context":{"key2":"value2"}}'`, (3) **Test Subject**: Calls `fileManager.assembleAndSaveFinalDocument(rootContributionId)` directly, (4) **Consumer Assertion**: Verifies the function reads from `storage_path`/`file_name` (the canonical access method: `${chunk.storage_path}/${chunk.file_name}`), (5) **Consumer Assertion**: Verifies `assembleAndSaveFinalDocument` was called (by checking that an assembled JSON file exists in storage at the `AssembledDocumentJson` path), (6) **Consumer Assertion**: Downloads the assembled JSON file from storage using the path from `constructStoragePath` with `FileType.AssembledDocumentJson`, (7) **Consumer Assertion**: Verifies the downloaded content can be parsed as valid JSON using `JSON.parse()`, (8) **Consumer Assertion**: Verifies the parsed JSON is a merged object (not an array) with header overridden by continuation and context deep-merged: `{"header":"Continuation Header","context":{"key":"value","key2":"value2"}}`, (9) **Consumer Assertion**: Verifies the root chunk's raw JSON file remains unchanged (can be downloaded using `${rootChunk.storage_path}/${rootChunk.file_name}` with original `rootContent`), (10) **Consumer Assertion**: Verifies the continuation chunk's raw JSON file remains unchanged (can be downloaded using `${continuationChunk.storage_path}/${continuationChunk.file_name}` with original `continuationContent`), (11) **Consumer Assertion**: Verifies the assembled JSON file path is in `_work/assembled_json/` and does NOT match either raw JSON file path (constructed from `storage_path`/`file_name`). This test proves that `assembleAndSaveFinalDocument` creates valid assembled JSON without corrupting raw JSON files.
        *   `[‚úÖ]` 15.e.ii. Add an integration test "should handle single chunk JSON-only artifact (root only, no continuations)" that: (1) **Producer Setup**: Creates only a root chunk contribution via `executeModelCallAndSave` with a JSON-only artifact output type, `resolvedFinish: 'stop'`, and `document_relationships: { [stageSlug]: rootContributionId }`, (2) **Test Subject**: Calls `fileManager.assembleAndSaveFinalDocument(rootContributionId)` directly, (3) **Consumer Assertion**: Verifies `assembleAndSaveFinalDocument` created an assembled JSON file, (4) **Consumer Assertion**: Verifies the assembled JSON is a valid merged object (not an array) matching the root chunk's content (no merge needed for single chunk), (5) **Consumer Assertion**: Verifies the root chunk's raw JSON file remains unchanged (can be downloaded using `${rootChunk.storage_path}/${rootChunk.file_name}`). This test proves the function works correctly for single-chunk artifacts.
        *   `[‚úÖ]` 15.e.iii. Add an integration test "should preserve raw JSON files when assembling JSON-only artifacts" that: (1) **Producer Setup**: Creates root and continuation chunks with JSON-only artifact output type, (2) **Test Subject**: Calls `fileManager.assembleAndSaveFinalDocument(rootContributionId)` directly, (3) **Consumer Assertion**: Downloads both raw JSON files using their canonical paths (`${rootChunk.storage_path}/${rootChunk.file_name}` and `${continuationChunk.storage_path}/${continuationChunk.file_name}`), (4) **Consumer Assertion**: Verifies both raw JSON files contain only single JSON objects (not concatenated), (5) **Consumer Assertion**: Verifies both raw JSON files can be parsed as valid JSON, (6) **Consumer Assertion**: Verifies the raw JSON file contents match the original content passed to `uploadAndRegisterFile`, (7) **Consumer Assertion**: Verifies the assembled JSON file is separate and contains the properly merged object. This test proves that raw JSON files are never corrupted by `assembleAndSaveFinalDocument`.
    *   `[‚úÖ]` 15.f. `[LINT]` Run the linter for `supabase/functions/_shared/services/file_manager.ts` and `supabase/functions/_shared/services/file_manager.assemble.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 15.g. `[CRITERIA]` All requirements are met: (1) `assembleAndSaveFinalDocument` reads from `storage_path`/`file_name` (the canonical access method: `${chunk.storage_path}/${chunk.file_name}`), (2) `assembleAndSaveFinalDocument` merges JSON properly by creating valid merged JSON objects instead of string concatenation, (3) `assembleAndSaveFinalDocument` saves to a new `AssembledDocumentJson` path in `_work/assembled_json/` (not overwriting raw JSON files), (4) The assembled JSON can be parsed as valid JSON and contains all chunks merged in the correct order, (5) Raw JSON files remain unchanged after assembly (never corrupted), (6) All unit tests pass, including tests that verify proper JSON merging and correct path usage, (7) Integration tests prove `assembleAndSaveFinalDocument` works correctly and preserves raw JSON files, (8) The new integration test file `file_manager.assemble.integration.test.ts` exists and contains comprehensive tests for `assembleAndSaveFinalDocument`, (9) All files are lint-clean, (10) The function produces valid JSON artifacts without corrupting raw JSON files.
    *   `[‚úÖ]` 15.h. `[COMMIT]` `fix(be): assembleAndSaveFinalDocument reads from storage_path/file_name (canonical access method), concatenates JSON properly, and saves to AssembledDocumentJson path`

*   `[‚úÖ]` 16. **`[BE]` Fix executeModelCallAndSave to Only Call assembleAndSaveFinalDocument for JSON-Only Artifacts**
    *   `[‚úÖ]` 16.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (line 1420) calls `assembleAndSaveFinalDocument` unconditionally for all final chunks when `rootIdFromSaved` exists. However, `assembleAndSaveFinalDocument` should only be called for JSON-only artifacts (like `AssembledDocumentJson`) that don't get rendered to markdown, not for rendered documents. The `shouldEnqueueRenderJob` function in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` determines if an output type should be rendered (returns `true` for markdown documents, `false` for JSON-only artifacts). The `shouldRender` value is computed earlier in `executeModelCallAndSave` (line 1213) from `shouldEnqueueRenderJob`. For rendered documents (`shouldRender === true`), RENDER jobs should handle document assembly via `renderDocument`, not `assembleAndSaveFinalDocument`. For JSON-only artifacts (`shouldRender === false`), `assembleAndSaveFinalDocument` should be called to create the assembled JSON artifact. The fix requires: (1) making the call to `assembleAndSaveFinalDocument` conditional in `executeModelCallAndSave` - only call it when `rootIdFromSaved` exists AND `shouldRender === false` (JSON-only artifacts), (2) ensuring that for rendered documents (`shouldRender === true`), `assembleAndSaveFinalDocument` is never called, allowing RENDER jobs to handle document assembly, (3) ensuring that for JSON-only artifacts (`shouldRender === false`), `assembleAndSaveFinalDocument` is called to create the assembled JSON artifact, (4) updating tests to verify the conditional logic.
    *   `[‚úÖ]` 16.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, add tests that prove `assembleAndSaveFinalDocument` is only called for JSON-only artifacts.
        *   `[‚úÖ]` 16.b.i. Add a test case that verifies when `executeModelCallAndSave` processes a final chunk with `shouldRender === true` (markdown document), `assembleAndSaveFinalDocument` is NOT called. Mock `shouldEnqueueRenderJob` to return `true`, create a final chunk with `resolvedFinish === 'stop'`, call `executeModelCallAndSave`, and assert `fileManager.assembleAndSaveFinalDocument.calls.length === 0`.
        *   `[‚úÖ]` 16.b.ii. Add a test case that verifies when `executeModelCallAndSave` processes a final chunk with `shouldRender === false` (JSON-only artifact), `assembleAndSaveFinalDocument` IS called with the root contribution ID. Mock `shouldEnqueueRenderJob` to return `false`, create a final chunk with `resolvedFinish === 'stop'` and `document_relationships` containing a root ID, call `executeModelCallAndSave`, and assert `fileManager.assembleAndSaveFinalDocument.calls.length === 1` with the correct root ID.
        *   `[‚úÖ]` 16.b.iii. Add a test case that verifies when `executeModelCallAndSave` processes a non-final chunk (`resolvedFinish !== 'stop'`), `assembleAndSaveFinalDocument` is NOT called regardless of `shouldRender` value. Create a continuation chunk with `resolvedFinish === 'length'`, call `executeModelCallAndSave`, and assert `fileManager.assembleAndSaveFinalDocument.calls.length === 0`.
        *   `[‚úÖ]` 16.b.iv. Add a test case that verifies when `executeModelCallAndSave` processes a final chunk with `shouldRender === false` but no `rootIdFromSaved` (document_relationships is null or empty), `assembleAndSaveFinalDocument` is NOT called. Create a final chunk with `resolvedFinish === 'stop'`, `document_relationships: null`, mock `shouldEnqueueRenderJob` to return `false`, call `executeModelCallAndSave`, and assert `fileManager.assembleAndSaveFinalDocument.calls.length === 0`.
    *   `[‚úÖ]` 16.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, make the call to `assembleAndSaveFinalDocument` conditional.
        *   `[‚úÖ]` 16.c.i. At line 1419, before calling `assembleAndSaveFinalDocument`, add a check: only call it if `rootIdFromSaved` exists AND `shouldRender === false`. Use the `shouldRender` value computed earlier in the function (line 1213) from `shouldEnqueueRenderJob`. Change the condition from `if (rootIdFromSaved)` to `if (rootIdFromSaved && !shouldRender)`.
        *   `[‚úÖ]` 16.c.ii. Verify that for rendered documents (`shouldRender === true`), `assembleAndSaveFinalDocument` is never called, allowing RENDER jobs to handle document assembly via `renderDocument`.
        *   `[‚úÖ]` 16.c.iii. Verify that for JSON-only artifacts (`shouldRender === false`), `assembleAndSaveFinalDocument` is called to create the assembled JSON artifact.
    *   `[‚úÖ]` 16.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 16.b and ensure they now pass. The tests should prove that `assembleAndSaveFinalDocument` is only called for JSON-only artifacts, not for rendered documents.
    *   `[‚úÖ]` 16.e. `[TEST-INT]` Prove `executeModelCallAndSave` works correctly with its producer and consumer when conditionally calling `assembleAndSaveFinalDocument`. Add integration tests to `supabase/integration_tests/services/file_manager.assemble.integration.test.ts` (created in step 15.e) that exercise the full producer ‚Üí test subject ‚Üí consumer flow.
        *   `[‚úÖ]` 16.e.i. Add an integration test "should NOT call assembleAndSaveFinalDocument for rendered documents (shouldRender === true)" that: (1) **Producer Setup**: Creates a root chunk contribution via `executeModelCallAndSave` with a markdown document output type (e.g., `FileType.business_case`) that triggers `shouldEnqueueRenderJob` to return `true`, sets `resolvedFinish: 'stop'` to make it a final chunk, and sets `document_relationships: { [stageSlug]: rootContributionId }`, (2) **Test Subject**: Calls `executeModelCallAndSave` with the final chunk job, (3) **Consumer Assertion**: Verifies `shouldEnqueueRenderJob` returned `true` (producer behavior), (4) **Consumer Assertion**: Verifies a RENDER job was enqueued in `dialectic_generation_jobs` with the correct payload, (5) **Consumer Assertion**: Queries the database to verify `assembleAndSaveFinalDocument` was NOT called (no assembled JSON file exists in `_work/assembled_json/`), (6) **Consumer Assertion**: Verifies the root chunk's file remains unchanged (can be downloaded using `${rootChunk.storage_path}/${rootChunk.file_name}` with original content), (7) **Consumer Assertion**: Processes the RENDER job via `processRenderJob` and verifies a rendered markdown file is created (proving the RENDER job path works correctly). This test proves the conditional logic: rendered documents use RENDER jobs, not `assembleAndSaveFinalDocument`.
        *   `[‚úÖ]` 16.e.ii. Add an integration test "should call assembleAndSaveFinalDocument for JSON-only artifacts (shouldRender === false) via executeModelCallAndSave" that: (1) **Producer Setup**: Creates a root chunk contribution via `executeModelCallAndSave` with a JSON-only artifact output type (e.g., `FileType.HeaderContext` or a custom JSON artifact) that triggers `shouldEnqueueRenderJob` to return `false`, sets `resolvedFinish: 'stop'` to make it a final chunk, and sets `document_relationships: { [stageSlug]: rootContributionId }`, with unique JSON content `rootContent = '{"header":"Root Header","context":{"key":"value"}}'`, (2) **Producer Setup**: Creates a continuation chunk contribution via `executeModelCallAndSave` with the same output type, `target_contribution_id` pointing to the root chunk, `resolvedFinish: 'stop'`, and different unique JSON content `continuationContent = '{"header":"Continuation Header","context":{"key2":"value2"}}'`, (3) **Test Subject**: Calls `executeModelCallAndSave` with the continuation chunk job (final chunk), (4) **Consumer Assertion**: Verifies `shouldEnqueueRenderJob` returned `false` (producer behavior), (5) **Consumer Assertion**: Verifies NO RENDER job was enqueued (proving JSON-only artifacts don't trigger rendering), (6) **Consumer Assertion**: Verifies `assembleAndSaveFinalDocument` was called (by checking that an assembled JSON file exists in storage at the `AssembledDocumentJson` path), (7) **Consumer Assertion**: Downloads the assembled JSON file from storage using the path from `constructStoragePath` with `FileType.AssembledDocumentJson`, (8) **Consumer Assertion**: Verifies the downloaded content can be parsed as valid JSON using `JSON.parse()`, (9) **Consumer Assertion**: Verifies the parsed JSON is an array with 2 elements: `[{"header":"Root Header","context":{"key":"value"}},{"header":"Continuation Header","context":{"key2":"value2"}}]`, (10) **Consumer Assertion**: Verifies the root chunk's raw JSON file remains unchanged (can be downloaded using `${rootChunk.storage_path}/${rootChunk.file_name}` with original `rootContent`), (11) **Consumer Assertion**: Verifies the continuation chunk's raw JSON file remains unchanged (can be downloaded using `${continuationChunk.storage_path}/${continuationChunk.file_name}` with original `continuationContent`), (12) **Consumer Assertion**: Verifies the assembled JSON file path is in `_work/assembled_json/` and does NOT match either raw JSON file path (constructed from `storage_path`/`file_name`). This test proves the conditional logic: JSON-only artifacts use `assembleAndSaveFinalDocument` to create valid assembled JSON without corrupting raw JSON files.
    *   `[‚úÖ]` 16.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 16.g. `[CRITERIA]` All requirements are met: (1) `assembleAndSaveFinalDocument` is only called when `shouldRender === false` (JSON-only artifacts), not for rendered documents, (2) The call is conditional in `executeModelCallAndSave` based on `shouldRender` value from `shouldEnqueueRenderJob`, (3) For rendered documents (`shouldRender === true`), `assembleAndSaveFinalDocument` is never called, allowing RENDER jobs to handle document assembly via `renderDocument`, (4) For JSON-only artifacts (`shouldRender === false`), `assembleAndSaveFinalDocument` is called to create the assembled JSON artifact, (5) All unit tests pass, including tests that verify conditional logic, (6) Integration tests prove all code paths: rendered documents use RENDER jobs (not `assembleAndSaveFinalDocument`), JSON-only artifacts use `assembleAndSaveFinalDocument` (not RENDER jobs), and raw JSON files are preserved, (7) All files are lint-clean, (8) The dead code issue is resolved: `assembleAndSaveFinalDocument` only runs in the correct context and produces valid JSON artifacts without corrupting raw JSON files.
    *   `[‚úÖ]` 16.h. `[COMMIT]` `fix(be): conditionally call assembleAndSaveFinalDocument only for JSON-only artifacts in executeModelCallAndSave`

*   `[‚úÖ]` 17. **`[BE]` Fix deconstructStoragePath to Extract documentKey for JSON-Only Artifacts**
    *   `[‚úÖ]` 17.a. `[DEPS]` The `deconstructStoragePath` function in `supabase/functions/_shared/utils/path_deconstructor.ts` (lines 291-313) uses the pattern `docCentricRawJsonPatternString` to extract path components from raw JSON file paths. When `assembleAndSaveFinalDocument` in `supabase/functions/_shared/services/file_manager.ts` (line 566) calls `deconstructStoragePath` to extract path context from a root contribution with `documentKey: FileType.HeaderContext` (value `"header_context"`), the function fails to extract `documentKey` correctly, returning `documentKey: undefined`. This causes `assembleAndSaveFinalDocument` to fail at line 572 with error "Cannot construct AssembledDocumentJson path: missing required path context. DocumentKey: undefined". The path being deconstructed is like `d2008f0e-2170-4200-90d7-7762a4195a5f/session_06a3c47d/iteration_1/1_thesis/raw_responses/mock-model_0_header_context_raw.json`. The regex pattern `docCentricRawJsonPatternString` (line 65) should match this path and extract `matches[7]` as the `documentKey` (value `"header_context"`). The code at lines 301-306 checks if `ambiguousPart` (from `matches[7]`) is a contribution type using `isContributionType()`, and if not, sets `info.documentKey = ambiguousPart`. However, `documentKey` ends up as `undefined`, indicating either the regex doesn't match correctly, `matches[7]` is wrong, or the `documentKey` assignment logic has a flaw. The fix requires: (1) writing a RED unit test that proves `deconstructStoragePath` fails to extract `documentKey` for JSON-only artifact paths like `header_context`, (2) fixing `deconstructStoragePath` to correctly extract `documentKey` for all JSON-only artifact types (including `FileType.HeaderContext`, `FileType.SynthesisHeaderContext`, and other JSON-only artifacts), (3) ensuring the regex pattern correctly captures the `documentKey` portion of the filename, (4) verifying the `documentKey` assignment logic works correctly when `ambiguousPart` is not a contribution type.
    *   `[‚úÖ]` 17.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/path_deconstructor.test.ts`, add tests that prove `deconstructStoragePath` correctly extracts `documentKey` for JSON-only artifact paths.
        *   `[‚úÖ]` 17.b.i. Add a test case "extracts documentKey for header_context JSON-only artifact" that: (1) constructs a path like `projectId/session_shortId/iteration_1/1_thesis/raw_responses/mock-model_0_header_context_raw.json`, (2) calls `deconstructStoragePath({ storageDir: 'projectId/session_shortId/iteration_1/1_thesis/raw_responses', fileName: 'mock-model_0_header_context_raw.json' })`, (3) verifies `deconstructedInfo.documentKey === 'header_context'`, (4) verifies `deconstructedInfo.modelSlug === 'mock-model'`, (5) verifies `deconstructedInfo.attemptCount === 0`, (6) verifies `deconstructedInfo.stageSlug === 'thesis'`. This test must initially fail, proving the flaw exists.
        *   `[‚úÖ]` 17.b.ii. Add a test case "extracts documentKey for synthesis_header_context JSON-only artifact" that: (1) constructs a path like `projectId/session_shortId/iteration_1/1_synthesis/raw_responses/mock-model_0_synthesis_header_context_raw.json`, (2) calls `deconstructStoragePath` with the path, (3) verifies `deconstructedInfo.documentKey === 'synthesis_header_context'`, (4) verifies all other path components are extracted correctly. This test must initially fail if the flaw exists for this artifact type.
        *   `[‚úÖ]` 17.b.iii. Add a test case "extracts documentKey for other JSON-only artifacts with underscores" that: (1) constructs paths with documentKeys containing underscores (e.g., `custom_json_artifact`, `another_json_type`), (2) calls `deconstructStoragePath` for each path, (3) verifies `documentKey` is extracted correctly for each. This test ensures the fix works for all JSON-only artifact types, not just `header_context`.
    *   `[‚úÖ]` 17.c. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/path_deconstructor.ts`, fix `deconstructStoragePath` to correctly extract `documentKey` for JSON-only artifacts.
        *   `[‚úÖ]` 17.c.i. Verify the regex pattern `docCentricRawJsonPatternString` (line 65) correctly matches paths with JSON-only artifact documentKeys. The pattern should be: `^([^/]+)/session_([^/]+)/iteration_(\\d+)/([^/]+)/raw_responses/(.+)_(\\d+)_(.+?)(_continuation_(\\d+))?_raw\\.json$` where `matches[7]` captures the `documentKey` portion. For a path like `mock-model_0_header_context_raw.json`, `matches[7]` should be `"header_context"` (the non-greedy `.+?` should capture everything between the attemptCount and the `_raw.json` suffix).
        *   `[‚úÖ]` 17.c.ii. Verify the logic at lines 301-306 correctly assigns `documentKey` when `ambiguousPart` is not a contribution type. The code should: (1) extract `ambiguousPart = matches[7]`, (2) check if `isContributionType(ambiguousPart)` returns `false` for `"header_context"`, (3) if false, set `info.documentKey = ambiguousPart`. If `documentKey` is still `undefined`, add explicit assignment: `info.documentKey = ambiguousPart` after the `isContributionType` check, or ensure the else branch (line 305) is executed correctly.
        *   `[‚úÖ]` 17.c.iii. Add validation to ensure `documentKey` is set before returning from the `docCentricRawJsonPatternString` match block. If `documentKey` is still `undefined` after the assignment logic, throw a descriptive error indicating the path could not be parsed correctly.
        *   `[‚úÖ]` 17.c.iv. Verify the fix works for all JSON-only artifact types by ensuring the regex pattern and assignment logic handle documentKeys with underscores correctly (e.g., `header_context`, `synthesis_header_context`, `custom_json_artifact`).
    *   `[‚úÖ]` 17.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 17.b and ensure they now pass. The tests should prove that `deconstructStoragePath` correctly extracts `documentKey` for all JSON-only artifact paths, enabling `assembleAndSaveFinalDocument` to construct `AssembledDocumentJson` paths correctly.
    *   `[‚úÖ]` 17.e. `[LINT]` Run the linter for `supabase/functions/_shared/utils/path_deconstructor.ts` and `supabase/functions/_shared/utils/path_deconstructor.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 17.f. `[CRITERIA]` All requirements are met: (1) `deconstructStoragePath` correctly extracts `documentKey` for JSON-only artifact paths like `header_context`, (2) The regex pattern `docCentricRawJsonPatternString` correctly matches and captures the `documentKey` portion of filenames, (3) The assignment logic correctly sets `info.documentKey` when `ambiguousPart` is not a contribution type, (4) All unit tests pass, including tests that verify `documentKey` extraction for various JSON-only artifact types, (5) The fix enables `assembleAndSaveFinalDocument` to successfully construct `AssembledDocumentJson` paths without throwing "missing required path context" errors, (6) All files are lint-clean, (7) Integration tests in step 15.e that use `FileType.HeaderContext` should now pass after this fix.
    *   `[‚úÖ]` 17.g. `[COMMIT]` `fix(be): deconstructStoragePath correctly extracts documentKey for JSON-only artifacts`

*   `[‚úÖ]` 18. **`[BE]` Fix assembleAndSaveFinalDocument to Handle Rendered Documents Gracefully**
    *   `[‚úÖ]` 18.a. `[DEPS]` The `assembleAndSaveFinalDocument` function in `supabase/functions/_shared/services/file_manager.ts` (lines 487-582) is designed to assemble JSON-only artifacts (like `AssembledDocumentJson`) that don't get rendered to markdown. However, when called directly with a rendered document contribution (e.g., `FileType.business_case`), the function attempts to construct an `AssembledDocumentJson` path using `deconstructStoragePath` and `constructStoragePath`. If `deconstructStoragePath` successfully extracts `documentKey` for rendered documents, the function will create an assembled JSON file even though rendered documents should use RENDER jobs instead of `assembleAndSaveFinalDocument`. The integration test "should NOT create assembled JSON for rendered documents (markdown documents that use RENDER jobs)" in `supabase/integration_tests/services/file_manager.assemble.integration.test.ts` (lines 661-775) calls `assembleAndSaveFinalDocument` directly with a rendered document and expects no assembled JSON file to exist. However, the function may create the file anyway, or fail with an unclear error. The fix requires: (1) writing a RED unit test that proves `assembleAndSaveFinalDocument` fails gracefully or does not create files for rendered documents, (2) adding validation in `assembleAndSaveFinalDocument` to detect rendered document types and fail early with a clear error message, (3) ensuring the function never creates assembled JSON files for rendered documents (they should use RENDER jobs via `renderDocument` instead), (4) updating the error message to clearly indicate that `assembleAndSaveFinalDocument` should only be called for JSON-only artifacts.
    *   `[‚úÖ]` 18.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/services/file_manager.assemble.test.ts`, add tests that prove `assembleAndSaveFinalDocument` handles rendered documents correctly.
        *   `[‚úÖ]` 18.b.i. Add a test case "throws error when called with rendered document contribution" that: (1) creates a root chunk contribution with `documentKey: FileType.business_case` (a rendered document type), `storage_path: 'project/session/iteration/stage/raw_responses'`, `file_name: 'model_0_business_case_raw.json'`, and mock storage content, (2) mocks `deconstructStoragePath` to return `documentKey: 'business_case'` (simulating successful extraction), (3) calls `assembleAndSaveFinalDocument(rootContributionId)`, (4) verifies the function throws an error with a message indicating that `assembleAndSaveFinalDocument` should only be called for JSON-only artifacts, not rendered documents, (5) verifies no assembled JSON file is created in storage. This test must initially fail, proving the flaw exists.
        *   `[‚úÖ]` 18.b.ii. Add a test case "throws error when documentKey corresponds to a rendered document type" that: (1) creates contributions with various rendered document types (e.g., `FileType.business_case`, `FileType.feature_spec`, `FileType.technical_approach`), (2) calls `assembleAndSaveFinalDocument` for each, (3) verifies the function throws an appropriate error for each rendered document type, (4) verifies no assembled JSON files are created. This test ensures the validation works for all rendered document types.
        *   `[‚úÖ]` 18.b.iii. Add a test case "allows JSON-only artifact documentKeys" that: (1) creates contributions with JSON-only artifact documentKeys (e.g., `FileType.HeaderContext`, `FileType.SynthesisHeaderContext`), (2) calls `assembleAndSaveFinalDocument` for each, (3) verifies the function succeeds and creates assembled JSON files. This test ensures the validation doesn't incorrectly reject valid JSON-only artifacts.
    *   `[‚úÖ]` 18.c. `[BE]` **GREEN**: In `supabase/functions/_shared/services/file_manager.ts`, add validation to `assembleAndSaveFinalDocument` to handle rendered documents gracefully.
        *   `[‚úÖ]` 18.c.i. After extracting `pathInfo` from `deconstructStoragePath` (line 566), add validation to check if `pathInfo.documentKey` corresponds to a rendered document type. Use `shouldEnqueueRenderJob` or a direct check against known rendered document types (e.g., `FileType.business_case`, `FileType.feature_spec`, etc.) to determine if the `documentKey` is a rendered document. If it is a rendered document, throw an error with a clear message: "assembleAndSaveFinalDocument should only be called for JSON-only artifacts (shouldRender === false), not for rendered documents. Rendered documents should use RENDER jobs via renderDocument instead."
        *   `[‚úÖ]` 18.c.ii. Ensure the validation happens before any file operations (downloads, uploads) to fail fast and avoid unnecessary work. Place the validation after `pathInfo` extraction (line 569) but before the validation check at line 571.
        *   `[‚úÖ]` 18.c.iii. Verify that the error message clearly indicates the function should only be called for JSON-only artifacts, helping developers understand the correct usage pattern.
        *   `[‚úÖ]` 18.c.iv. Ensure the validation doesn't incorrectly reject valid JSON-only artifacts. The check should only reject documentKeys that correspond to rendered document types (those that return `true` from `shouldEnqueueRenderJob`).
    *   `[‚úÖ]` 18.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 18.b and ensure they now pass. The tests should prove that `assembleAndSaveFinalDocument` correctly rejects rendered documents with clear error messages and does not create assembled JSON files for them.
    *   `[‚úÖ]` 18.e. `[LINT]` Run the linter for `supabase/functions/_shared/services/file_manager.ts` and `supabase/functions/_shared/services/file_manager.assemble.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 18.f. `[CRITERIA]` All requirements are met: (1) `assembleAndSaveFinalDocument` validates that `documentKey` is not a rendered document type before processing, (2) The function throws a clear error message when called with rendered documents, indicating they should use RENDER jobs instead, (3) The function never creates assembled JSON files for rendered documents, (4) The validation doesn't incorrectly reject valid JSON-only artifacts, (5) All unit tests pass, including tests that verify rendered documents are rejected and JSON-only artifacts are accepted, (6) Integration tests in step 15.e that test negative cases for rendered documents should now pass, (7) All files are lint-clean, (8) The function provides clear guidance to developers about when it should and shouldn't be called.
    *   `[‚úÖ]` 18.g. `[COMMIT]` `fix(be): assembleAndSaveFinalDocument validates and rejects rendered documents with clear error message`

*   `[‚úÖ]` 19. **`[BE]` Fix RENDER Jobs Stuck in Pending Status Due to Missing user_jwt in Payload**
    *   `[‚úÖ]` 19.a. `[DEPS]` The RENDER job processing chain requires complete payload with ALL required properties: (1) `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1249-1257) creates RENDER job payload, (2) Database trigger `invoke_worker_on_status_change` in `supabase/migrations/20251119160820_retrying_trigger.sql` (line 228) extracts `user_jwt` from payload and uses it in Authorization header, (3) Worker entry routes RENDER jobs via `processJob` in `supabase/functions/dialectic-worker/processJob.ts` (line 57-60) to `processRenderJob`, (4) `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` validates ALL payload fields (lines 24-54): requires `projectId` (string, validated at line 40), `sessionId` (string, validated at line 43), `iterationNumber` (number, validated at line 29), `stageSlug` (string, validated at line 46), `documentIdentity` (string, validated at line 49), `documentKey` (FileType, validated at line 34), `sourceContributionId` (string, validated at line 52), (5) `processRenderJob` passes these fields to `renderDocument` via `RenderDocumentParams` interface in `supabase/functions/_shared/services/document_renderer.interface.ts` (lines 27-35) which requires the same 7 fields, (6) The trigger requires `user_jwt` in payload (line 228) for authentication. Current `renderPayload` (lines 1249-1257) includes: `projectId` ‚úì, `sessionId` ‚úì, `iterationNumber` ‚úì, `stageSlug` ‚úì, `documentIdentity` ‚úì, `documentKey` ‚úì, `sourceContributionId` ‚úì, but is MISSING: `user_jwt` ‚úó. The parent EXECUTE job's payload contains `user_jwt` (extracted as `userAuthToken` at line 915 in `executeModelCallAndSave.ts`, validated at lines 910-920). The fix requires: (1) adding `user_jwt: userAuthToken` to the `renderPayload` object (line 1249) to include ALL 8 required payload fields, (2) verifying ALL required fields are present in payload before job creation: `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, `documentIdentity`, `documentKey`, `sourceContributionId`, and `user_jwt`, (3) verifying the trigger can extract `user_jwt` and make authenticated HTTP call to worker, (4) verifying `processRenderJob` validates all 7 processing fields successfully, (5) verifying the worker processes RENDER jobs successfully through the complete chain.
    *   `[‚úÖ]` 19.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, add tests that prove RENDER job payload includes `user_jwt`.
        *   `[‚úÖ]` 19.b.i. Add a test case "enqueues RENDER job with ALL required payload fields including user_jwt" that: (1) creates a mock EXECUTE job with payload containing `user_jwt: 'test-jwt-token'` and all required fields, (2) calls `executeModelCallAndSave` to process the job and enqueue a RENDER job, (3) verifies the RENDER job insert includes payload with ALL 8 required fields: `user_jwt: 'test-jwt-token'` (for trigger authentication), `projectId` (string, for processRenderJob validation and renderDocument), `sessionId` (string, for processRenderJob validation and renderDocument), `iterationNumber` (number, for processRenderJob validation and renderDocument), `stageSlug` (string, for processRenderJob validation and renderDocument), `documentIdentity` (string, for processRenderJob validation and renderDocument), `documentKey` (FileType, for processRenderJob validation and renderDocument), `sourceContributionId` (string, for processRenderJob validation and renderDocument), (4) asserts each field type matches requirements (strings for projectId/sessionId/stageSlug/documentIdentity/sourceContributionId, number for iterationNumber, FileType for documentKey, string for user_jwt). This test must initially fail because `user_jwt` is missing from the payload.
        *   `[‚úÖ]` 19.b.ii. Add a test case "throws error when parent job payload lacks user_jwt and RENDER job would be enqueued" that: (1) creates a mock EXECUTE job with payload missing `user_jwt`, (2) calls `executeModelCallAndSave`, (3) verifies the function throws "payload.user_jwt required" error before attempting to enqueue RENDER job, (4) verifies no RENDER job is enqueued. This test ensures the parent job validation prevents creating invalid RENDER jobs.
        *   `[‚úÖ]` 19.b.iii. Add a test case "RENDER job payload user_jwt matches parent job payload user_jwt exactly" that: (1) creates a mock EXECUTE job with payload containing `user_jwt: 'specific-token-value'`, (2) calls `executeModelCallAndSave` to enqueue a RENDER job, (3) extracts the RENDER job payload from the insert call, (4) asserts `renderPayload.user_jwt === 'specific-token-value'` (exact match, no modification). This test ensures the token is passed through unchanged.
    *   `[‚úÖ]` 19.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, add `user_jwt` to RENDER job payload.
        *   `[‚úÖ]` 19.c.i. Extract `userAuthToken` from parent job payload (already extracted at lines 910-920, stored in `userAuthToken` variable). This token is validated to exist and be non-empty before use.
        *   `[‚úÖ]` 19.c.ii. At line 1249, in the `renderPayload` object construction, add `user_jwt: userAuthToken` to include the authentication token. The `userAuthToken` variable is in scope at this location (it's extracted earlier in the function and validated).
        *   `[‚úÖ]` 19.c.iii. Verify the `renderPayload` object now includes ALL 8 required fields: `projectId` (string, from parent job payload), `sessionId` (string, from parent job payload), `iterationNumber` (number, from parent job payload), `stageSlug` (string, from parent job payload), `documentIdentity` (string, computed from document_relationships), `documentKey: validatedDocumentKey` (FileType, validated earlier), `sourceContributionId: contribution.id` (string, from saved contribution), and `user_jwt: userAuthToken` (string, extracted and validated from parent job payload at lines 910-920).
        *   `[‚úÖ]` 19.c.iv. Ensure the `userAuthToken` is still validated at lines 910-920 before use (the existing validation ensures it's a non-empty string), so no additional validation is needed when adding it to `renderPayload`.
    *   `[‚úÖ]` 19.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 19.b and ensure they now pass. The RENDER job payload should now include `user_jwt` extracted from the parent job payload.
    *   `[‚úÖ]` 19.e. `[TEST-INT]` Prove the complete RENDER job processing chain works end-to-end: job creation ‚Üí trigger ‚Üí worker entry ‚Üí routing ‚Üí processing ‚Üí completion.
        *   `[‚úÖ]` 19.e.i. Create a new integration test file `supabase/integration_tests/services/processRenderJob.integration.test.ts` following the standard integration test pattern (imports, `describe` block, `beforeAll` setup with project/session creation, `afterAll` cleanup, `it` test cases).
        *   `[‚úÖ]` 19.e.ii. Add an integration test "should process RENDER jobs end-to-end when payload includes ALL required fields" that: (1) **Producer Setup**: Creates a root chunk contribution via `executeModelCallAndSave` with a markdown document output type (e.g., `business_case`) that triggers RENDER job enqueue, sets `resolvedFinish: 'stop'` to make it final, includes `user_jwt: 'valid-jwt-token'` in the EXECUTE job payload, and sets `document_relationships: { [stageSlug]: rootContributionId }`, (2) **Producer Assertion**: Verifies a RENDER job is enqueued in `dialectic_generation_jobs` with status 'pending', payload containing ALL 8 required fields: `user_jwt: 'valid-jwt-token'` (string, for trigger authentication), `projectId` (string, for processRenderJob validation), `sessionId` (string, for processRenderJob validation), `iterationNumber` (number, for processRenderJob validation), `stageSlug` (string, for processRenderJob validation), `documentIdentity` (string, for processRenderJob validation), `documentKey` (FileType, for processRenderJob validation), `sourceContributionId` (string, for processRenderJob validation), (3) **Trigger Simulation**: Queries `dialectic_trigger_logs` to verify the trigger fired and attempted HTTP call with Authorization header containing the JWT token (verify log shows `jwt_exists: true`), (4) **Worker Entry**: Calls `handleJob` in `supabase/functions/dialectic-worker/index.ts` directly with the RENDER job record and valid auth token to simulate trigger invocation, (5) **Routing Assertion**: Verifies `processJob` routes the job to `processRenderJob` (check via logs or mocks that `processRenderJob` is called), (6) **Processing Assertion**: Verifies `processRenderJob` successfully validates ALL payload fields: `projectId` (string check passes at line 40), `sessionId` (string check passes at line 43), `iterationNumber` (number check passes at line 29), `stageSlug` (string check passes at line 46), `documentIdentity` (string check passes at line 49), `documentKey` (FileType check passes at line 34), `sourceContributionId` (string check passes at line 52), with no validation errors, calls `renderDocument` with all 7 required `RenderDocumentParams` fields, and updates job status to 'completed' with `completed_at` timestamp and `results.pathContext` populated, (7) **Consumer Assertion**: Verifies a rendered markdown file is saved to storage at the canonical document path, (8) **Consumer Assertion**: Verifies a `dialectic_project_resources` record is created with `resource_type = 'rendered_document'`, correct `session_id`, `iteration_number`, `stage_slug`, `file_name` matching the rendered document, and `source_contribution_id` matching the `sourceContributionId` from the payload. This test proves the entire function chain works: job creation with ALL required fields ‚Üí trigger authentication ‚Üí worker entry ‚Üí routing ‚Üí complete payload validation ‚Üí processing ‚Üí completion.
        *   `[‚úÖ]` 19.e.iii. Add an integration test "should fail to process RENDER jobs when payload lacks user_jwt (negative test)" that: (1) **Producer Setup**: Creates a RENDER job directly in the database with status 'pending' and payload missing `user_jwt` (simulates the bug condition), (2) **Trigger Simulation**: Verifies the trigger logs show `jwt_exists: false` or warning about missing `user_jwt`, (3) **Worker Entry**: Attempts to call `handleJob` with the RENDER job, (4) **Consumer Assertion**: Verifies the worker entry point throws "Missing authorization header" error when `user_jwt` is NULL, (5) **Consumer Assertion**: Verifies the RENDER job remains in 'pending' status (not processed), proving the authentication failure prevents processing. This test proves the authentication requirement is enforced.
    *   `[‚úÖ]` 19.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 19.g. `[CRITERIA]` All requirements are met: (1) RENDER job payload includes ALL 8 required fields: `user_jwt` (string, extracted from parent EXECUTE job payload for trigger authentication), `projectId` (string, for processRenderJob and renderDocument), `sessionId` (string, for processRenderJob and renderDocument), `iterationNumber` (number, for processRenderJob and renderDocument), `stageSlug` (string, for processRenderJob and renderDocument), `documentIdentity` (string, for processRenderJob and renderDocument), `documentKey` (FileType, for processRenderJob and renderDocument), `sourceContributionId` (string, for processRenderJob and renderDocument), (2) Each field type matches requirements (strings for projectId/sessionId/stageSlug/documentIdentity/sourceContributionId/user_jwt, number for iterationNumber, FileType for documentKey), (3) Database trigger can extract `user_jwt` from payload and make authenticated HTTP call to worker with valid Authorization header, (4) Worker entry point successfully authenticates and routes RENDER jobs to `processRenderJob`, (5) `processRenderJob` successfully validates ALL 7 processing fields (projectId, sessionId, iterationNumber, stageSlug, documentIdentity, documentKey, sourceContributionId) with correct type checks and processes RENDER jobs, (6) `renderDocument` receives all 7 required `RenderDocumentParams` fields, (7) Rendered markdown files are saved to storage and `dialectic_project_resources` records are created, (8) All unit tests pass, including tests that verify ALL 8 required fields are included in payload with correct types, (9) Integration tests prove the complete function chain works end-to-end: job creation with ALL required fields ‚Üí trigger authentication ‚Üí worker entry ‚Üí routing ‚Üí complete payload validation ‚Üí processing ‚Üí completion, (10) Negative integration test proves RENDER jobs without `user_jwt` fail authentication and remain pending, (11) All files are lint-clean, (12) RENDER jobs no longer remain stuck in 'pending' status due to missing or incomplete payload fields.
    *   `[‚úÖ]` 19.h. `[COMMIT]` `fix(be): add user_jwt to RENDER job payload to enable trigger authentication and job processing`

*   `[‚úÖ]` 20. **`[BE]` Fix documentIdentity Extraction Sequencing Bug Causing renderDocument to Fail Finding Chunks**
    *   `[‚úÖ]` 20.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` has a sequencing bug where `documentIdentity` is extracted from `contribution.document_relationships` (lines 1291-1301) BEFORE `document_relationships` is persisted to the database. For continuation chunks, `document_relationships` is updated from the job payload at lines 1408-1421 (AFTER RENDER job creation at lines 1283-1369). For root chunks, `document_relationships` is initialized at lines 1423-1437 (AFTER RENDER job creation). This causes `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` (line 45) to query with an incorrect `documentIdentity` when the RENDER job is processed, resulting in "No contribution chunks found for requested document" errors. Additionally, the extraction logic (lines 1294-1299) iterates all keys in `document_relationships` and takes the first string value found, rather than using the specific `stageSlug` key. This may extract the wrong value if multiple stage keys exist. The fix requires: (1) moving `document_relationships` initialization/update to occur IMMEDIATELY after the contribution is saved (before RENDER job creation), ensuring it is persisted and available in the `contribution` object when `documentIdentity` is extracted, (2) fixing `documentIdentity` extraction to specifically use `document_relationships[stageSlug]` and throwing an error if `document_relationships` is null or the `stageSlug` key is missing (data must be complete and correct, no fallbacks), (3) ensuring root chunks have `document_relationships` initialized before RENDER job creation so `documentIdentity` equals the root's contribution.id from `document_relationships[stageSlug]`, (4) ensuring continuation chunks have `document_relationships` persisted before RENDER job creation so `documentIdentity` equals the root's contribution.id from `document_relationships[stageSlug]` (not this chunk's contribution.id), (5) throwing errors if `document_relationships` persistence fails or if `document_relationships[stageSlug]` is missing after persistence (incomplete data is an error condition), (6) verifying that when `renderDocument` queries using the extracted `documentIdentity`, it finds all related chunks in the document chain.
    *   `[‚úÖ]` 20.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.render.test.ts`, add tests that prove `documentIdentity` extraction happens after `document_relationships` is persisted.
        *   `[‚úÖ]` 20.b.i. Add a test case "extracts documentIdentity from document_relationships[stageSlug] for root chunks after relationships are initialized" that: (1) creates a mock EXECUTE job for a root chunk (no `target_contribution_id`), (2) mocks `fileManager.uploadAndRegisterFile` to return a contribution with `id: 'root-id'` and `document_relationships: null` initially, (3) calls `executeModelCallAndSave`, (4) verifies the database update for `document_relationships` is called BEFORE the RENDER job insert (check call order in mocks/spies), (5) verifies the RENDER job payload contains `documentIdentity: 'root-id'` (extracted from `document_relationships[stageSlug]` after initialization), (6) verifies `documentIdentity === contribution.id` for root chunks (both equal the root's contribution.id). This test must initially fail because the sequencing is incorrect.
        *   `[‚úÖ]` 20.b.ii. Add a test case "extracts documentIdentity from document_relationships[stageSlug] for continuation chunks after relationships are persisted" that: (1) creates a mock EXECUTE job for a continuation chunk with `target_contribution_id: 'root-id'` and `document_relationships: { [stageSlug]: 'root-id' }` in the payload, (2) mocks `fileManager.uploadAndRegisterFile` to return a contribution with `id: 'continuation-id'` and `document_relationships: null` initially (before update), (3) calls `executeModelCallAndSave`, (4) verifies the database update for `document_relationships` is called BEFORE the RENDER job insert, (5) verifies the RENDER job payload contains `documentIdentity: 'root-id'` (extracted from `document_relationships[stageSlug]` after persistence, not `continuation-id`), (6) verifies `documentIdentity !== sourceContributionId` for continuation chunks (`documentIdentity` is root's ID, `sourceContributionId` is continuation's ID). This test must initially fail because the sequencing is incorrect.
        *   `[‚úÖ]` 20.b.iii. Add a test case "extracts documentIdentity using stageSlug key specifically, not first available key" that: (1) creates a contribution with `document_relationships: { 'different_stage': 'wrong-id', [stageSlug]: 'correct-id' }` (multiple keys), (2) calls `executeModelCallAndSave`, (3) verifies the RENDER job payload contains `documentIdentity: 'correct-id'` (the value for `stageSlug`, not `'wrong-id'` from the first key). This test must initially fail because the extraction uses the first key found.
        *   `[‚úÖ]` 20.b.iv. Add a test case "throws error when document_relationships is null after persistence" that: (1) creates a contribution where `document_relationships` persistence fails or remains null after save (simulating a persistence failure), (2) calls `executeModelCallAndSave`, (3) verifies the function throws an error indicating `document_relationships` is required and must be persisted before RENDER job creation, (4) verifies no RENDER job is enqueued when `document_relationships` is missing (incomplete data prevents job creation).
    *   `[‚úÖ]` 20.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, fix the sequencing and extraction logic.
        *   `[‚úÖ]` 20.c.i. Move the `document_relationships` initialization for root chunks (currently lines 1423-1437) to occur IMMEDIATELY after the contribution is saved (after line 1281, before line 1283). This ensures `document_relationships` is persisted and available in the `contribution` object before `documentIdentity` extraction. Update the `contribution` object's `document_relationships` property after the database update succeeds.
        *   `[‚úÖ]` 20.c.ii. Move the `document_relationships` update for continuation chunks (currently lines 1408-1421) to occur IMMEDIATELY after the contribution is saved (after line 1281, before line 1283). This ensures `document_relationships` is persisted and available in the `contribution` object before `documentIdentity` extraction. Update the `contribution` object's `document_relationships` property after the database update succeeds.
        *   `[‚úÖ]` 20.c.iii. Fix the `documentIdentity` extraction logic (lines 1291-1301) to specifically use `document_relationships[stageSlug]` and throw an error if data is incomplete. Change from iterating all keys and taking the first value to: validate that `document_relationships` is a record (throw error if null), validate that `document_relationships[stageSlug]` exists and is a non-empty string (throw error if missing), use `document_relationships[stageSlug]` as `documentIdentity`. Throw descriptive errors indicating that `document_relationships` and `document_relationships[stageSlug]` are required and must be persisted before RENDER job creation. No fallbacks - incomplete data must cause an error.
        *   `[‚úÖ]` 20.c.iv. Ensure the updated `contribution` object (with persisted `document_relationships`) is used when extracting `documentIdentity` (lines 1291-1301). The `contribution` variable should reference the updated object after `document_relationships` persistence, not the original saved result.
        *   `[‚úÖ]` 20.c.v. Add validation after `document_relationships` persistence/initialization to throw errors if data is incomplete: verify that `document_relationships` is not null after the database update, verify that `document_relationships[stageSlug]` exists and is a non-empty string after persistence. If either validation fails, throw a descriptive error and do not create the RENDER job. Verify that for root chunks, after successful `document_relationships` initialization, `document_relationships[stageSlug] === contribution.id`, so `documentIdentity` equals the root's contribution.id. Verify that for continuation chunks, after successful `document_relationships` persistence, `document_relationships[stageSlug]` equals the root's contribution.id (not this chunk's ID), so `documentIdentity` equals the root's contribution.id for querying all related chunks.
    *   `[‚úÖ]` 20.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 20.b and ensure they now pass. The tests should prove that `documentIdentity` is extracted correctly after `document_relationships` is persisted, using the `stageSlug` key specifically.
    *   `[‚úÖ]` 20.e. `[TEST-INT]` Prove the complete RENDER job processing chain works correctly with the fixed sequencing.
        *   `[‚úÖ]` 20.e.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (producer) creates a root chunk contribution and enqueues a RENDER job with the correct `documentIdentity` (extracted after `document_relationships` initialization), `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` (test subject) successfully processes it, calls `renderDocument` with `documentIdentity`, and `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` (consumer) successfully queries contributions using `.contains("document_relationships", { [stageSlug]: documentIdentity })` and finds the root chunk. Create an integration test that: (1) creates a root chunk via `executeModelCallAndSave`, (2) verifies `document_relationships` is initialized BEFORE the RENDER job is created, (3) verifies the RENDER job payload contains `documentIdentity` extracted from `document_relationships[stageSlug]`, (4) processes the RENDER job via `processRenderJob`, (5) verifies `renderDocument` successfully queries and finds the root chunk, (6) verifies the document is rendered successfully.
        *   `[‚úÖ]` 20.e.ii. Assert that when `executeModelCallAndSave` (producer) creates a continuation chunk contribution and enqueues a RENDER job with the correct `documentIdentity` (root's ID extracted from `document_relationships[stageSlug]` after persistence), `processRenderJob` (test subject) successfully processes it, calls `renderDocument` with `documentIdentity`, and `renderDocument` (consumer) successfully queries contributions using `documentIdentity` and finds ALL related chunks (root and continuation) in the document chain. Create an integration test that: (1) creates a root chunk and a continuation chunk via `executeModelCallAndSave`, (2) verifies both chunks have `document_relationships[stageSlug]` set to the root's contribution.id, (3) verifies the continuation chunk's RENDER job payload contains `documentIdentity` equal to the root's ID (not the continuation's ID), (4) processes the RENDER job via `processRenderJob`, (5) verifies `renderDocument` successfully queries using `documentIdentity: rootId` and finds both chunks, (6) verifies the document is rendered successfully with content from both chunks.
    *   `[‚úÖ]` 20.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.render.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 20.g. `[CRITERIA]` All requirements are met: (1) `document_relationships` initialization for root chunks occurs IMMEDIATELY after contribution save, before RENDER job creation, (2) `document_relationships` persistence for continuation chunks occurs IMMEDIATELY after contribution save, before RENDER job creation, (3) `documentIdentity` extraction specifically uses `document_relationships[stageSlug]` and throws errors if `document_relationships` is null or `document_relationships[stageSlug]` is missing (data must be complete and correct, no fallbacks), (4) Validation throws errors if `document_relationships` persistence fails or if `document_relationships[stageSlug]` is missing after persistence (incomplete data is an error condition), (5) For root chunks, after successful `document_relationships` initialization, `documentIdentity` equals the root's contribution.id (extracted from `document_relationships[stageSlug]`), (6) For continuation chunks, after successful `document_relationships` persistence, `documentIdentity` equals the root's contribution.id (extracted from `document_relationships[stageSlug]`, not this chunk's ID), (7) When `renderDocument` queries using the extracted `documentIdentity`, it successfully finds all related chunks in the document chain, (8) All unit tests pass, including tests that verify sequencing, correct key extraction, and error throwing when data is incomplete, (9) Integration tests prove the complete RENDER job processing chain works correctly for both root and continuation chunks, with `renderDocument` successfully finding and rendering all related chunks, (10) The file is lint-clean, (11) RENDER jobs no longer fail with "No contribution chunks found for requested document" errors due to incorrect `documentIdentity` extraction sequencing, (12) The application throws errors when data is incomplete or incorrect, never using fallbacks or defaults.

*   `[ ]` 21. **`[TEST-INT]` Prove Complete End-to-End Document Generation Flow: EXECUTE Job ‚Üí Contribution Save ‚Üí RENDER Job Enqueue ‚Üí RENDER Job Process ‚Üí Document Render ‚Üí Stage Completion**
    *   `[ ]` 21.a. `[DEPS]` The document generation flow involves multiple interdependent functions that must work together correctly: (1) `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` saves AI model responses as contributions and conditionally enqueues RENDER jobs, (2) `shouldEnqueueRenderJob` in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` determines if an output type requires rendering by querying recipe steps, (3) `processRenderJob` in `supabase/functions/dialectic-worker/processRenderJob.ts` processes RENDER jobs and calls `renderDocument`, (4) `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` assembles document chunks and renders markdown files, (5) `processComplexJob` in `supabase/functions/dialectic-worker/processComplexJob.ts` orchestrates stage execution and determines when stages are complete, (6) `handle_job_completion` trigger in `supabase/migrations/20250905142613_fix_auth_header.sql` checks if all jobs for a stage are complete. The two critical problems are: (A) RENDER jobs are never enqueued despite EXECUTE jobs completing with valid JSON, and (B) stages restart even though all jobs complete. These problems occur because the flow has multiple data dependencies that must be satisfied in the correct sequence: `document_relationships` must be persisted before `documentIdentity` extraction, all 8 required RENDER job payload fields must be present (`user_jwt`, `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, `documentIdentity`, `documentKey`, `sourceContributionId`), `shouldEnqueueRenderJob` must correctly identify markdown documents, RENDER jobs must be processed successfully, and stage completion logic must correctly account for RENDER job completion. The fix requires: (1) creating a comprehensive integration test that exercises the ENTIRE flow from EXECUTE job creation through stage completion, (2) verifying ALL data dependencies at each step: contribution save ‚Üí `document_relationships` persistence ‚Üí `shouldEnqueueRenderJob` check ‚Üí RENDER job payload construction ‚Üí RENDER job insertion ‚Üí trigger invocation ‚Üí worker routing ‚Üí `processRenderJob` validation ‚Üí `renderDocument` query ‚Üí document rendering ‚Üí stage completion check, (3) proving that when an AI model returns valid JSON for a markdown document, the complete flow succeeds: RENDER job is enqueued with ALL required fields, RENDER job is processed successfully, document is rendered and saved, and stage is marked complete, (4) proving that `processComplexJob` correctly identifies stage completion when all EXECUTE and RENDER jobs are complete, (5) proving that `handle_job_completion` trigger correctly identifies stage completion when no jobs remain in pending/processing/retrying states.
    *   `[ ]` 21.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/executeModelCallAndSave.document.integration.test.ts`, add a comprehensive end-to-end integration test that proves the complete document generation flow works correctly.
        *   `[ ]` 21.b.i. Add an integration test "should generate complete document end-to-end: EXECUTE job ‚Üí contribution save ‚Üí RENDER job enqueue ‚Üí RENDER job process ‚Üí document render ‚Üí stage completion" that exercises the ENTIRE flow: (1) **Producer Setup**: Create a test session with a stage (e.g., 'thesis') that has an active recipe instance with recipe steps defining a markdown document output (e.g., `business_case` with `file_type: 'markdown'` in `outputs_required`), create an EXECUTE job with payload containing: `job_type: 'EXECUTE'`, `projectId` (string), `sessionId` (string), `iterationNumber` (number), `stageSlug: 'thesis'` (string), `output_type: 'business_case'` (string), `user_jwt` (string, valid JWT token), `document_key: 'business_case'` (string), `model_id` (string), and all other required EXECUTE job payload fields, (2) **Test Subject - Step 1**: Call `executeModelCallAndSave` with the EXECUTE job and verify: (a) the AI model response is saved as a contribution in `dialectic_contributions` with `output_type: 'business_case'`, `storage_path` and `file_name` populated, (b) for root chunks: `document_relationships` is initialized IMMEDIATELY after contribution save (before RENDER job creation) with `document_relationships[stageSlug] = contribution.id`, verify the database update occurs and the in-memory `contribution.document_relationships` is updated, (c) for continuation chunks: `document_relationships` is persisted IMMEDIATELY after contribution save (before RENDER job creation) from the job payload, verify the database update occurs and the in-memory `contribution.document_relationships` is updated, (d) `shouldEnqueueRenderJob` is called with `{ outputType: 'business_case', stageSlug: 'thesis' }` and returns `true` (proving the output type is correctly identified as a markdown document), (e) `documentIdentity` is extracted from `contribution.document_relationships[stageSlug]` AFTER `document_relationships` is persisted (verify extraction happens after persistence by checking call order or database state), (f) a RENDER job is inserted into `dialectic_generation_jobs` with: `job_type: 'RENDER'`, `parent_job_id` pointing to the EXECUTE job ID, `status: 'pending'`, and payload containing ALL 8 required fields: `user_jwt` (string, matching EXECUTE job payload), `projectId` (string, matching EXECUTE job payload), `sessionId` (string, matching EXECUTE job payload), `iterationNumber` (number, matching EXECUTE job payload), `stageSlug: 'thesis'` (string, matching EXECUTE job payload), `documentIdentity` (string, extracted from `document_relationships[stageSlug]`), `documentKey: 'business_case'` (FileType, validated), `sourceContributionId` (string, matching `contribution.id`), (g) the EXECUTE job status is updated to `'completed'` with `completed_at` timestamp, (3) **Test Subject - Step 2**: Verify the database trigger `invoke_worker_on_status_change` fires when the RENDER job is inserted with status 'pending', and verify: (a) the trigger extracts `user_jwt` from the RENDER job payload, (b) the trigger makes an HTTP POST call to the worker endpoint with Authorization header containing the JWT token, (c) the trigger logs the invocation in `dialectic_trigger_logs`, (4) **Test Subject - Step 3**: Simulate the worker entry point by calling `processJob` in `supabase/functions/dialectic-worker/processJob.ts` with the RENDER job record and verify: (a) `processJob` routes the RENDER job to `processRenderJob` (verify via function call or logs), (b) `processRenderJob` successfully validates ALL 7 required payload fields: `projectId` (string check passes), `sessionId` (string check passes), `iterationNumber` (number check passes), `stageSlug` (string check passes), `documentIdentity` (string check passes), `documentKey` (FileType check passes), `sourceContributionId` (string check passes), with no validation errors, (c) `processRenderJob` calls `renderDocument` with all 7 required `RenderDocumentParams` fields, (5) **Test Subject - Step 4**: Verify `renderDocument` in `supabase/functions/_shared/services/document_renderer.ts` successfully: (a) queries `dialectic_contributions` using `documentIdentity` with `.contains("document_relationships", { [stageSlug]: documentIdentity })` and finds the contribution(s) (for root chunks, finds 1 contribution; for continuation chunks, finds all related chunks in the document chain), (b) assembles the document chain correctly (root chunk first, then continuation chunks in order via `target_contribution_id` links), (c) renders the combined markdown content from all chunks, (d) saves the rendered markdown file to storage at the canonical document path (using `constructStoragePath` with correct path context), (e) returns `pathContext` with `sourceContributionId` set correctly, (6) **Test Subject - Step 5**: Verify `processRenderJob` successfully: (a) updates the RENDER job status to `'completed'` with `completed_at` timestamp, (b) saves `results.pathContext` with the correct path context from `renderDocument`, (c) creates a `dialectic_project_resources` record with: `resource_type = 'rendered_document'`, `session_id` matching the session ID, `iteration_number` matching the iteration number, `stage_slug: 'thesis'`, `file_name` matching the rendered document filename, `source_contribution_id` matching the `sourceContributionId` from the payload, (7) **Test Subject - Step 6**: Verify stage completion logic: (a) query `dialectic_generation_jobs` for all jobs with the same `sessionId`, `stageSlug`, and `iterationNumber`, (b) verify ALL EXECUTE jobs have status `'completed'`, (c) verify ALL RENDER jobs have status `'completed'` (not `'pending'` or `'processing'`), (d) call `processComplexJob` with the parent PLAN job and verify: it queries all child jobs (EXECUTE and RENDER jobs), it correctly identifies that all steps are complete (no jobs in pending/processing/retrying states), it marks the parent PLAN job as `'completed'` with `completed_at` timestamp, (e) verify the `handle_job_completion` trigger correctly identifies stage completion: when the last RENDER job reaches `'completed'` status, the trigger queries for jobs with status LIKE 'pending%' OR 'processing%' OR 'retrying%' and finds NONE, so it sets `v_is_stage_complete = true` and updates the session status accordingly, (8) **Consumer Assertion**: Verify the final state: (a) the rendered markdown file exists in storage and can be downloaded, (b) the `dialectic_project_resources` record exists with correct `source_contribution_id`, (c) the stage is marked complete (no jobs remain in pending/processing/retrying states), (d) the session status reflects stage completion. This test must initially fail if ANY of the data dependencies are missing or incorrect, proving the exact flaw in the flow.
        *   `[ ]` 21.b.ii. Add an integration test "should NOT enqueue RENDER job when shouldEnqueueRenderJob returns false (JSON-only artifacts)" that: (1) creates an EXECUTE job with `output_type: 'header_context'` (a JSON-only artifact that `shouldEnqueueRenderJob` should return `false` for), (2) calls `executeModelCallAndSave`, (3) verifies `shouldEnqueueRenderJob` is called and returns `false`, (4) verifies NO RENDER job is inserted into `dialectic_generation_jobs`, (5) verifies the contribution is saved correctly, (6) verifies `assembleAndSaveFinalDocument` is called for final chunks (if applicable). This test proves the conditional logic works correctly.
        *   `[ ]` 21.b.iii. Add an integration test "should handle continuation chunks correctly in end-to-end flow" that: (1) creates a root chunk EXECUTE job and calls `executeModelCallAndSave`, verifying a RENDER job is enqueued, (2) creates a continuation chunk EXECUTE job with `target_contribution_id` pointing to the root chunk and `document_relationships: { [stageSlug]: rootContributionId }` in the payload, (3) calls `executeModelCallAndSave` for the continuation chunk, (4) verifies `document_relationships` is persisted from the job payload BEFORE RENDER job creation, (5) verifies the continuation chunk's RENDER job payload contains `documentIdentity: rootContributionId` (not the continuation's ID) and `sourceContributionId: continuationContributionId` (the continuation's ID), (6) processes the continuation chunk's RENDER job via `processRenderJob`, (7) verifies `renderDocument` queries using `documentIdentity: rootContributionId` and finds BOTH the root chunk and continuation chunk, (8) verifies the rendered document contains content from both chunks in correct order, (9) verifies the stage completion logic correctly accounts for both RENDER jobs (root and continuation) when determining if the stage is complete. This test proves continuation chunks work correctly in the end-to-end flow.
        *   `[ ]` 21.b.iv. Add an integration test "should correctly identify stage completion when all EXECUTE and RENDER jobs complete" that: (1) creates a stage with multiple recipe steps, each producing a markdown document, (2) creates EXECUTE jobs for all steps and processes them via `executeModelCallAndSave`, (3) verifies RENDER jobs are enqueued for all markdown documents, (4) processes all RENDER jobs via `processRenderJob`, (5) verifies ALL RENDER jobs reach `'completed'` status, (6) calls `processComplexJob` with the parent PLAN job and verifies: it queries all child jobs (EXECUTE and RENDER), it correctly identifies that ALL steps are complete (no jobs in pending/processing/retrying states), it marks the parent PLAN job as `'completed'`, (7) verifies the `handle_job_completion` trigger correctly identifies stage completion when the last job reaches `'completed'` status, (8) verifies the session status is updated to reflect stage completion. This test proves the stage completion logic works correctly when all jobs complete.
        *   `[ ]` 21.b.v. Add an integration test "should NOT mark stage complete when RENDER jobs are stuck in pending status" that: (1) creates an EXECUTE job and processes it via `executeModelCallAndSave`, (2) verifies a RENDER job is enqueued with status `'pending'`, (3) simulates a condition where the RENDER job cannot be processed (e.g., missing `user_jwt` in payload, invalid payload fields), (4) verifies the RENDER job remains in `'pending'` status, (5) calls `processComplexJob` with the parent PLAN job and verifies: it queries all child jobs, it correctly identifies that the RENDER job is still pending, it does NOT mark the parent PLAN job as `'completed'`, it attempts to re-plan the step (if applicable), (6) verifies the `handle_job_completion` trigger correctly identifies that the stage is NOT complete (RENDER job is still in `'pending'` status). This test proves the stage completion logic correctly accounts for RENDER job status.
    *   `[ ]` 21.c. `[BE]` **GREEN**: Fix any flaws identified by the integration tests in step 21.b by ensuring ALL data dependencies are satisfied in the correct sequence.
        *   `[ ]` 21.c.i. If the test from 21.b.i fails because `document_relationships` is not persisted before RENDER job creation, verify that the fixes from step 20 are correctly applied: `document_relationships` initialization for root chunks occurs IMMEDIATELY after contribution save (before RENDER job creation), `document_relationships` persistence for continuation chunks occurs IMMEDIATELY after contribution save (before RENDER job creation), the in-memory `contribution.document_relationships` is updated after database persistence, `documentIdentity` extraction happens AFTER `document_relationships` is persisted.
        *   `[ ]` 21.c.ii. If the test from 21.b.i fails because RENDER job payload is missing required fields, verify that ALL 8 required fields are included: `user_jwt` (extracted from parent EXECUTE job payload at lines 910-920 in `executeModelCallAndSave.ts`), `projectId` (from parent job payload), `sessionId` (from parent job payload), `iterationNumber` (from parent job payload), `stageSlug` (from parent job payload), `documentIdentity` (extracted from `document_relationships[stageSlug]` after persistence), `documentKey` (validated FileType from `validatedDocumentKey`), `sourceContributionId` (from `contribution.id`). Verify each field is validated before RENDER job creation and that errors are thrown if any field is missing or invalid.
        *   `[ ]` 21.c.iii. If the test from 21.b.i fails because `shouldEnqueueRenderJob` returns `false` incorrectly, verify that: the function correctly queries `dialectic_stages` to get `active_recipe_instance_id`, the function correctly queries recipe steps (from `dialectic_stage_recipe_steps` if cloned, or `dialectic_recipe_template_steps` if not cloned), the function correctly extracts markdown document keys from `outputs_required` JSONB field, the function correctly identifies when `outputType` matches an extracted markdown document key. Add logging to trace the query results and extraction logic if needed.
        *   `[ ]` 21.c.iv. If the test from 21.b.i fails because `processRenderJob` validation fails, verify that: ALL 7 required payload fields are present and of correct types (`projectId` is string, `sessionId` is string, `iterationNumber` is number, `stageSlug` is string, `documentIdentity` is string, `documentKey` is FileType, `sourceContributionId` is string), validation errors are thrown with descriptive messages if any field is missing or invalid, the function does not proceed with rendering if validation fails.
        *   `[ ]` 21.c.v. If the test from 21.b.i fails because `renderDocument` cannot find contributions, verify that: `documentIdentity` is correctly extracted from `document_relationships[stageSlug]` (not from other keys or fallback values), `document_relationships` is persisted to the database before `documentIdentity` extraction, the query uses `.contains("document_relationships", { [stageSlug]: documentIdentity })` correctly, all related chunks have `document_relationships[stageSlug]` set to the same `documentIdentity` value (root's contribution.id).
        *   `[ ]` 21.c.vi. If the test from 21.b.iv fails because stage completion logic incorrectly identifies incomplete stages, verify that: `processComplexJob` queries ALL child jobs (EXECUTE and RENDER jobs) when determining step completion, `processComplexJob` correctly identifies jobs in pending/processing/retrying states and does NOT mark steps as complete if any jobs are in these states, `processComplexJob` correctly marks the parent PLAN job as `'completed'` only when ALL non-skipped, validated steps have ALL their child jobs (EXECUTE and RENDER) in terminal states (`'completed'`, `'retry_loop_failed'`, or `'failed'`), `handle_job_completion` trigger correctly queries for jobs with status LIKE 'pending%' OR 'processing%' OR 'retrying%' and identifies stage completion when no such jobs exist, the trigger correctly updates session status when stage completion is detected.
        *   `[ ]` 21.c.vii. Ensure ALL fixes preserve existing functionality: root chunks still work correctly, continuation chunks still work correctly, JSON-only artifacts still work correctly, stage completion logic still works for non-rendered outputs, error handling still works correctly.
    *   `[ ]` 21.d. `[TEST-INT]` **GREEN**: Re-run all tests from step 21.b and ensure they now pass. The tests should prove that the complete end-to-end flow works correctly: EXECUTE jobs complete and enqueue RENDER jobs with ALL required fields, RENDER jobs are processed successfully, documents are rendered and saved, and stages are marked complete when all jobs finish.
    *   `[ ]` 21.e. `[LINT]` Run the linter for all files modified in step 21.c and resolve any warnings or errors.
    *   `[ ]` 21.f. `[CRITERIA]` All requirements are met: (1) A comprehensive integration test exercises the ENTIRE flow from EXECUTE job creation through stage completion, (2) ALL data dependencies are verified at each step: contribution save ‚Üí `document_relationships` persistence ‚Üí `shouldEnqueueRenderJob` check ‚Üí RENDER job payload construction ‚Üí RENDER job insertion ‚Üí trigger invocation ‚Üí worker routing ‚Üí `processRenderJob` validation ‚Üí `renderDocument` query ‚Üí document rendering ‚Üí stage completion check, (3) When an AI model returns valid JSON for a markdown document, the complete flow succeeds: RENDER job is enqueued with ALL 8 required fields (`user_jwt`, `projectId`, `sessionId`, `iterationNumber`, `stageSlug`, `documentIdentity`, `documentKey`, `sourceContributionId`), RENDER job is processed successfully (all 7 validation fields pass), document is rendered and saved to storage, `dialectic_project_resources` record is created with correct `source_contribution_id`, and stage is marked complete, (4) `processComplexJob` correctly identifies stage completion when all EXECUTE and RENDER jobs are complete (no jobs in pending/processing/retrying states), (5) `handle_job_completion` trigger correctly identifies stage completion when no jobs remain in pending/processing/retrying states, (6) Continuation chunks work correctly in the end-to-end flow: `document_relationships` is persisted from job payload, RENDER job payload contains correct `documentIdentity` (root's ID) and `sourceContributionId` (continuation's ID), `renderDocument` finds all related chunks and renders them correctly, (7) Stage completion logic correctly accounts for RENDER job status when determining if stages are complete (stages are NOT marked complete if RENDER jobs are stuck in pending status), (8) All integration tests pass, proving the complete flow works correctly for root chunks, continuation chunks, multiple documents, and stage completion, (9) All files are lint-clean, (10) The two critical problems are resolved: RENDER jobs are enqueued when EXECUTE jobs complete with valid JSON for markdown documents, and stages are correctly marked complete when all EXECUTE and RENDER jobs finish (stages do NOT restart unnecessarily).
    *   `[ ]` 21.g. `[COMMIT]` `fix(be): ensure complete end-to-end document generation flow works correctly with all data dependencies satisfied`

*   `[‚úÖ]` 22. **`[BE]` Fix DialecticRenderJobPayload Type Definition (RENDER)**
    *   `[‚úÖ]` 22.a. `[DEPS]` The `DialecticRenderJobPayload` definition in `supabase/functions/dialectic-service/dialectic.interface.ts` (line 723) incorrectly includes `job_type: 'RENDER'`, but the database stores `job_type` in a dedicated column (`dialectic_generation_jobs.job_type`), not in the payload JSON blob. This mismatch violates the "Database as Source of Truth" principle and forces producers (like `executeModelCallAndSave.ts`) to use workarounds like `Omit` to avoid type errors when constructing payloads that match the DB shape. It also breaks strict type validation in consumers that read payloads from the database, as the stored JSON lacks the property required by the interface. The fix requires: (1) removing the `job_type` property from the interface definition to align it with the actual stored JSON structure, (2) updating the `isDialecticRenderJobPayload` type guard to validate the payload based on its unique structural properties (`documentIdentity`, `documentKey`, `sourceContributionId`) rather than checking for a `job_type` property that no longer exists in the interface or data.
        *   `[‚úÖ]` 22.a.i. Open `supabase/functions/dialectic-service/dialectic.interface.ts`. Locate the `DialecticRenderJobPayload` interface definition (approx line 723). Delete the line `job_type: 'RENDER';` entirely.
    *   `[‚úÖ]` 22.b. `[TYPES]` Update Type Guards to match the new interface.
        *   `[‚úÖ]` 22.b.i. `[TEST-UNIT]` Open `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts`. Locate the `isDialecticRenderJobPayload` test suite. Remove any test assertions that verify the function returns false when `job_type` is missing or incorrect (since the property is removed). Ensure tests verify that valid payloads (containing `documentIdentity`, `documentKey`, `sourceContributionId`) return `true`, and invalid payloads (missing unique fields) return `false`.
        *   `[‚úÖ]` 22.b.ii. `[BE]` Open `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts`. In the `isDialecticRenderJobPayload` function (approx line 1048), delete the statement: `if (payload.job_type !== 'RENDER') throw new Error("Invalid job_type: expected 'RENDER'");`. This removes the runtime check for the nonexistent property.
    *   `[‚úÖ]` 22.c. `[CRITERIA]` The `DialecticRenderJobPayload` definition accurately reflects the database schema (no `job_type`); `isDialecticRenderJobPayload` validates structure based on unique fields only.
    *   `[‚úÖ]` 22.d. `[COMMIT]` `refactor(be): update DialecticRenderJobPayload definition and type guards`

*   `[‚úÖ]` 23. **`[BE]` Update `executeModelCallAndSave.ts` (Producer of RENDER)**
    *   `[‚úÖ]` 23.a. `[DEPS]` The `executeModelCallAndSave.ts` function currently works around the incorrect interface by using `Omit<DialecticRenderJobPayload, 'job_type'>` when typing the `renderPayload` variable, and then potentially manually adding `job_type: 'RENDER'` back into the object literal or relying on the DB insert to set the column. With the interface fixed in Step 22, the `Omit` utility is no longer needed, and the `job_type` property must be removed from the payload construction to match the new interface. The `job_type` column on the `dialectic_generation_jobs` table will be set explicitly during insert, separate from the payload JSON.
    *   `[‚úÖ]` 23.b. `[TEST-UNIT]` Open `supabase/functions/dialectic-worker/executeModelCallAndSave*.test.ts`.
        *   `[‚úÖ]` 23.b.i. Locate the `makeRenderJob` helper function (or any test setup that constructs mock RENDER payloads). Remove the `job_type: 'RENDER'` property from the returned object to match the new interface.
        *   `[‚úÖ]` 23.b.ii. Search for any individual test cases that manually construct expected payload objects (e.g., in `toEqual` assertions) and remove `job_type: 'RENDER'` from them.
    *   `[‚úÖ]` 23.c. `[BE]` Open `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`.
        *   `[‚úÖ]` 23.c.i. Locate the `renderPayload` variable declaration in `gatherArtifacts` (approx line 1377). Change the type annotation from `Omit<DialecticRenderJobPayload, 'job_type'>` to simply `DialecticRenderJobPayload`.
        *   `[‚úÖ]` 23.c.ii. In the object literal assigned to `renderPayload`, delete the property `job_type: 'RENDER',`. This ensures the constructed object matches the interface and the database JSON column format.
    *   `[‚úÖ]` 23.d. `[TEST-UNIT]` Rerun `executeModelCallAndSave*.test.ts` and ensure all tests pass.
    *   `[‚úÖ]` 23.e. `[COMMIT]` `fix(be): update executeModelCallAndSave payload construction`

*   `[‚úÖ]` 24. **`[BE]` Fix DialecticExecuteJobPayload Type Definition (EXECUTE)**
    *   `[‚úÖ]` 24.a. `[DEPS]` Similar to RENDER payloads, the `DialecticExecuteJobPayload` definition in `dialectic.interface.ts` (approx line 705) incorrectly includes `job_type: 'execute'`. This causes the same mismatch between the interface and the database schema. The fix is to remove `job_type` from the interface and update the associated type guard. Note that `DialecticExecuteJobPayload` has unique required fields like `prompt_template_id` which allow it to be distinguished from other payload types structurally.
        *   `[‚úÖ]` 24.a.i. Open `supabase/functions/dialectic-service/dialectic.interface.ts`. Find `DialecticExecuteJobPayload` (approx line 705). Delete the line `job_type: 'execute';`.
    *   `[‚úÖ]` 24.b. `[TYPES]` Update Type Guards.
        *   `[‚úÖ]` 24.b.i. `[TEST-UNIT]` Open `type_guards.dialectic.test.ts`. In `isDialecticExecuteJobPayload` tests, remove assertions checking for `job_type`. Ensure validation relies on `prompt_template_id` and `canonicalPathParams`.
        *   `[‚úÖ]` 24.b.ii. `[BE]` Open `type_guards.dialectic.ts`. In `isDialecticExecuteJobPayload` (approx line 869), delete the statement: `if (payload.job_type !== 'execute') throw new Error("Invalid job_type: expected 'execute'");`.
    *   `[‚úÖ]` 24.c. `[COMMIT]` `refactor(be): update DialecticExecuteJobPayload definition and type guards`

*   `[‚úÖ]` 25. **`[BE]` Update `continueJob.ts` (Producer of EXECUTE)**
    *   `[‚úÖ]` 25.a. `[DEPS]` The `continueJob.ts` function constructs payloads for continuation jobs (which are EXECUTE jobs). It currently manually sets `basePayload.job_type = 'execute'` (approx line 118). This assignment is now invalid because the property has been removed from the `DialecticExecuteJobPayload` interface. The fix is to simply remove this assignment line. The `job_type` column is handled separately during the database insert operation.
    *   `[‚úÖ]` 25.b. `[TEST-UNIT]` Open `supabase/functions/dialectic-worker/continueJob.test.ts`.
        *   `[‚úÖ]` 25.b.i. Search for `job_type: 'execute'` in any expected payload assertions (e.g., `expect(payload).toEqual(...)`) or mock setups. Remove this property from the expectations.
    *   `[‚úÖ]` 25.c. `[BE]` Open `supabase/functions/dialectic-worker/continueJob.ts`.
        *   `[‚úÖ]` 25.c.i. Find where `basePayload` is constructed or modified (approx line 118). Delete the line `basePayload.job_type = 'execute';`.
    *   `[‚úÖ]` 25.d. `[TEST-UNIT]` Rerun `continueJob.test.ts` and ensure all tests pass.
    *   `[‚úÖ]` 25.e. `[COMMIT]` `fix(be): update continueJob payload construction`

*   `[‚úÖ]` 26. **`[BE]` Fix DialecticPlanJobPayload Type Definition (PLAN)**
    *   `[‚úÖ]` 26.a. `[DEPS]` `DialecticPlanJobPayload` in `dialectic.interface.ts` (approx line 683) incorrectly includes `job_type: JobType`. PLAN payloads are the "default" complex job type and lack specific unique required fields compared to RENDER/EXECUTE. Removing `job_type` makes `isDialecticPlanJobPayload` potentially too broad (it matches any base payload). Since `isDialecticPlanJobPayload` is used for structural validation of the payload object (not identification of row type), we must ensure it doesn't return false positives for valid objects that are meant to be interpreted as other types if strict discrimination is required. However, the user instruction is to NOT use hacky negative checks. We will remove `job_type` and allow `isDialecticPlanJobPayload` to validate that the object conforms to the *base requirements* of a plan job. Identification of "Is this a PLAN job vs EXECUTE job" in mixed contexts (like `task_isolator`) will be handled by checking for specific features of EXECUTE jobs first.
        *   `[‚úÖ]` 26.a.i. Open `dialectic.interface.ts`. Find `DialecticPlanJobPayload` (approx line 683). Delete the line `job_type: JobType;`.
    *   `[‚úÖ]` 26.b. `[TYPES]` Update Type Guards.
        *   `[‚úÖ]` 26.b.i. `[TEST-UNIT]` Open `type_guards.dialectic.test.ts`. In `isDialecticPlanJobPayload` tests, remove assertions checking for `job_type`.
        *   `[‚úÖ]` 26.b.ii. `[BE]` Open `type_guards.dialectic.ts`. In `isDialecticPlanJobPayload` (approx line 1039), delete the line `if (payload.job_type !== 'PLAN') return false;`. Do NOT add any negative checks or hacks. Rely on structural typing.
    *   `[‚úÖ]` 26.c. `[COMMIT]` `refactor(be): update DialecticPlanJobPayload definition and type guards`

*   `[‚úÖ]` 27. **`[BE]` Update `task_isolator.ts` (Producer of PLAN/EXECUTE)**
    *   `[‚úÖ]` 27.a. `[DEPS]` The `task_isolator.ts` function processes a mixed array of payloads returned by planners to create database rows. It needs to set the `job_type` column for each row. Previously it checked `isDialecticExecuteJobPayload` and `isDialecticPlanJobPayload` which relied on the `job_type` property. With `job_type` removed, we must distinguish them structurally. Since `DialecticExecuteJobPayload` (specific) is a superset of `DialecticPlanJobPayload` (generic), the logic MUST check for the more specific type (`isDialecticExecuteJobPayload`, which checks for `prompt_template_id`) *first*. If it matches, we treat it as EXECUTE. If not, and it matches the generic base (`isDialecticPlanJobPayload`), we treat it as PLAN. This avoids ambiguity without adding hacky negative checks to the type guards themselves.
    *   `[‚úÖ]` 27.b. `[TEST-UNIT]` Open `supabase/functions/dialectic-worker/task_isolator.test.ts`.
        *   `[‚úÖ]` 27.b.i. Remove `job_type` from any mock payloads returned by mock planners in the test setup.
    *   `[‚úÖ]` 27.c. `[BE]` Open `supabase/functions/dialectic-worker/task_isolator.ts`.
        *   `[‚úÖ]` 27.c.i. Locate the payload processing loop (lines 590-600). Replace the existing `if/else if` block with logic that prioritizes the specific type:
            if (isDialecticExecuteJobPayload(payload)) {
                jobType = 'EXECUTE';
                validatedPayload = payload;
            } else if (isDialecticPlanJobPayload(payload)) {
                jobType = 'PLAN';
                validatedPayload = payload;
            } ...
                        This ensures that an EXECUTE payload is correctly identified as such, rather than falling into the PLAN bucket (which it would also technically match structurally).
    *   `[‚úÖ]` 27.d. `[TEST-UNIT]` Rerun `task_isolator.test.ts` and ensure all tests pass.
    *   `[‚úÖ]` 27.e. `[COMMIT]` `fix(be): verify task_isolator payload handling`

*   `[‚úÖ]` 28. **`[BE]` Update Router `processJob.ts`**
    *   `[‚úÖ]` 28.a. `[DEPS]` The `processJob.ts` function is the router for the worker. At line 35, it logs the job type using `job.payload.job_type`. Since `job_type` has been removed from the payload interfaces, this property access is now invalid and will cause a TypeScript error (or runtime undefined). The correct, authoritative source for the job type is the `job.job_type` database column, which is available on the `job` object. The fix is to update the logging statement to use the column value.
    *   `[‚úÖ]` 28.b. `[BE]` Open `supabase/functions/dialectic-worker/processJob.ts`.
        *   `[‚úÖ]` 28.c.i. Find the logging statement at line 35: `deps.logger.info(\`[processJob] Dispatching job ${job.id} of type \${job.payload.job_type}\`);`. Replace `job.payload.job_type` with `job.job_type`. Ensure no ternaries or fallbacks are used, as `job_type` is a required column on the `DialecticJobRow` interface.
    *   `[‚úÖ]` 28.d. `[COMMIT]` `fix(be): update router logging`

*   `[‚úÖ]` 29. **`[TEST-UNIT]` Update Consumer Tests**
    *   `[‚úÖ]` 29.a. `[DEPS]` Unit tests for `processRenderJob`, `processSimpleJob`, and `processComplexJob` create mock `DialecticJobPayload` objects to test the processors. These mock objects currently include the `job_type` property. Since the interfaces have changed, these mocks are now invalid. We must update all test files to remove the `job_type` property from mock payloads, ensuring the tests accurately reflect the new system state.
    *   `[‚úÖ]` 29.b. `[TEST-UNIT]` Update `processRenderJob.test.ts`.
        *   `[‚úÖ]` 29.b.i. Search for the `makeRenderJob` helper function (or any object literal containing `job_type: 'RENDER'`). Delete the `job_type` property line from the mock object.
    *   `[‚úÖ]` 29.c. `[TEST-UNIT]` Update `processSimpleJob.test.ts`.
        *   `[‚úÖ]` 29.c.i. Search for the `makeExecuteJob` helper (or object literals with `job_type: 'execute'`). Delete the `job_type` property line.
    *   `[‚úÖ]` 29.d. `[TEST-UNIT]` Update `processComplexJob.test.ts`.
        *   `[‚úÖ]` 29.d.i. Search for the `makePlanJob` helper (or object literals with `job_type: 'PLAN'`). Delete the `job_type` property line.
    *   `[‚úÖ]` 29.e. `[COMMIT]` `test(be): update consumer tests for all job types`

*   `[‚úÖ]` 30. **`[TEST-INT]` Update Integration Tests**
    *   `[‚úÖ]` 30.a. `[DEPS]` Integration tests in `document_renderer.integration.test.ts` and `executeModelCallAndSave.document.integration.test.ts` verify the behavior of the system by asserting that payloads match expected shapes. These assertions often use `expect.objectContaining({ job_type: ... })`. Since the actual payloads generated by the system will no longer contain `job_type`, these assertions will fail. We must update the integration tests to remove these expectations, verifying only the fields that should actually be present (like `documentKey`, `projectId`, etc.).
    *   `[‚úÖ]` 30.b. `[TEST-INT]` Update `supabase/integration_tests/services/document_renderer.integration.test.ts`.
        *   `[‚úÖ]` 30.b.i. Search the file for `job_type`. Identify any `expect` calls asserting payload structure. Remove the `job_type` property from the expected object.
    *   `[‚úÖ]` 30.c. `[TEST-INT]` Update `supabase/integration_tests/services/executeModelCallAndSave.document.integration.test.ts`.
        *   `[‚úÖ]` 30.c.i. Search the file for `job_type`. Identify any `expect` calls asserting payload structure (e.g., verifying that `executeModelCallAndSave` enqueued a RENDER job). Remove `job_type: 'RENDER'` from the expected payload object in the assertion.
    *   `[‚úÖ]` 30.d. `[COMMIT]` `test(int): update integration tests`
    
*   `[ ]` 31. **`[BE]` Fix Stage Completion Loop in `processComplexJob`**
    *   `[ ]` 31.a. `[DEPS]` The `processComplexJob` function depends on several database tables and other functions to correctly orchestrate a `PLAN` job.
        *   `[ ]` 31.a.i. `supabase/functions/dialectic-worker/processComplexJob.ts`: This is the function to be modified.
        *   `[ ]` 31.a.ii. `dialectic_generation_jobs`: The function reads the status of all child jobs (EXECUTE and RENDER) to determine if a stage is complete.
        *   `[ ]` 31.a.iii. `dialectic_recipe_template_steps` / `dialectic_stage_recipe_steps`: The function reads the recipe definition to know which steps are required for the stage to be considered complete.
    *   `[ ]` 31.b. `[TYPES]` No new types are required for this change.
        *   `[ ]` 31.b.i. The fix will utilize existing types from `dialectic.interface.ts` such as `DialecticJobRow`, `DialecticPlanJobPayload`, `DialecticRecipeTemplateStep`, and `DialecticStageRecipeStep`.
    *   `[ ]` 31.c. `[TEST-UNIT]` Create a failing unit test that proves the infinite loop exists.
        *   `[ ]` 31.c.i. In the appropriate test file for `supabase/functions/dialectic-worker/processComplexJob.ts`.
        *   `[ ]` 31.c.ii. Create a test titled "should mark the parent PLAN job as completed when all recipe steps are successfully completed".
        *   `[ ]` 31.c.iii. The test will mock the database client and set up a state where all child jobs for a given PLAN job are marked as `completed`.
        *   `[ ]` 31.c.iv. Call `processComplexJob` with the parent PLAN job.
        *   `[ ]` 31.c.v. Assert that the function updates the parent PLAN job's status to `completed`. This test will fail because the current logic will incorrectly re-plan instead of completing the job.
    *   `[ ]` 31.d. `[BE]` Implement the fix in `processComplexJob.ts`.
        *   `[ ]` 31.d.i. In `processComplexJob.ts`, refactor the logic at the beginning of the function to perform a definitive check for stage completion before attempting to find the next ready steps.
        *   `[ ]` 31.d.ii. The new logic will fetch all non-skipped recipe steps and all child jobs for the parent PLAN job.
        *   `[ ]` 31.d.iii. It will create a `Set` of all `step_slug`s that have `completed` child jobs.
        *   `[ ]` 31.d.iv. If all required recipe steps are present in the set of completed steps, the function will update the parent PLAN job's status to `completed` and `return`, breaking the loop.
    *   `[ ]` 31.e. `[TEST-UNIT]` Prove the fix works by re-running the unit test.
        *   `[ ]` 31.e.i. Re-run the unit test created in step 31.c. The test will now pass.
    *   `[ ]` 31.f. `[TEST-INT]` Defer integration testing until after the dependent trigger is fixed.
        *   `[ ]` 31.f.i. The integration test for this function will be created in step 32.f, after its dependent (`handle_job_completion`) is also fixed, to prove the entire completion chain works as intended.
    *   `[ ]` 31.g. `[CRITERIA]` The work is complete when the function correctly identifies that all child jobs are complete and updates the parent job, preventing an infinite loop.
        *   `[ ]` 31.g.i. When all child jobs for a PLAN job are complete, `processComplexJob` correctly identifies this state and updates the parent job to `completed`.
        *   `[ ]` 31.g.ii. The system no longer enters an infinite loop of re-planning a completed stage.
    *   `[ ]` 31.h. `[COMMIT]` Provide a commit message for this change.
        *   `[ ]` 31.h.i. `fix(BE): Correct stage completion logic in processComplexJob`

*   `[ ]` 32. **`[DB]` [REFACTOR] Implement a Unified, Recipe-Driven State Management Trigger**
    *   `[ ]` 32.a. [DEPS] The system's state management will be consolidated into a single, unified state management trigger (`on_job_state_propagation`) that executes the function `manage_dialectic_state()`. This new function will manage state based on the stage's recipe, which defines a full Directed Acyclic Graph (DAG) of job dependencies via the `dialectic_stage_recipe_edges` table.
        *   `[ ]` 32.a.i. **Architectural Note:** The existing `on_job_status_change` trigger and its `invoke_worker_on_status_change` function (asynchronous worker invocation) are a separate concern and will be preserved. The `manage_dialectic_state` function handles only synchronous, in-database state changes.
    *   `[ ]` 32.b. [LOGIC] The `manage_dialectic_state()` function must be a comprehensive state machine, executing the following four parts sequentially:
        *   `[ ]` 32.b.i. **Part 1: Immediate Parent Failure Propagation:** If the updated job (`NEW`) has a `parent_job_id` and has entered a terminal failure state (`failed` or `retry_loop_failed`), the function must immediately `UPDATE` the parent job's status to `failed`. This action must take precedence over all other logic.
        *   `[ ]` 32.b.ii. **Part 2: Parent/Child Completion Orchestration:** If the updated job (`NEW`) has a `parent_job_id` and has entered a terminal state, the function must query for all its siblings. If all siblings are now in a terminal state, the function determines the parent's final status: if any sibling has failed, the parent is marked `failed`; if all siblings have `completed`, the parent is marked `completed`.
        *   `[ ]` 32.b.iii. **Part 3: Job-to-Job State Propagation (DAG-Aware):** This part uses the recipe's `dialectic_stage_recipe_edges` table as the source of truth for the stage's DAG.
            *   `[ ]` 32.b.iii.i. **Fan-out Failure Logic:** If any job enters a terminal failure state, the failure propagates through the entire downstream dependency chain. The function must find all jobs that depend on the failed job and mark them as `failed`.
            *   `[ ]` 32.b.iii.ii. **Fan-in Success Logic:** When a job enters `completed` status, the function iterates through its dependent jobs. For each dependent job, it checks if *all* of its prerequisite jobs are `completed`. If so, its status is updated from `waiting_for_prerequisite` to `pending`.
        *   `[ ]` 32.b.iv. **Part 4: Session-Level State Projection:** The function dynamically loads the active recipe for the stage and compares the set of completed jobs against the required steps to correctly update `dialectic_sessions.status` to `generating_[stage_slug]`, `[stage_slug]_generation_failed`, or `[stage_slug]_generation_complete`.
    *   `[ ]` 32.c. [TEST-INT] The test suite `supabase/integration_tests/triggers/manage_dialectic_state.trigger.test.ts` must contain the following test cases to prove all functionality:
        *   `[ ]` 32.c.i. **Test Case (Immediate Parent Failure):**
            *   `[ ]` 32.c.i.i. **Arrange**: Create a `PLAN` job with several child `EXECUTE` jobs.
            *   `[ ]` 32.c.i.ii. **Act**: `UPDATE` one of the child jobs to `failed`.
            *   `[ ]` 32.c.i.iii. **Assert**: The trigger immediately marks the parent `PLAN` job as `failed`.
        *   `[ ]` 32.c.ii. **Test Case (Parent/Child Completion - Success):**
            *   `[ ]` 32.c.ii.i. **Arrange**: Create a `PLAN` job with two child `EXECUTE` jobs.
            *   `[ ]` 32.c.ii.ii. **Act**: `UPDATE` the first child job to `completed`, then `UPDATE` the second child job to `completed`.
            *   `[ ]` 32.c.ii.iii. **Assert**: After the first update, the parent `PLAN` job's status is unchanged. After the second update, the trigger automatically updates the parent `PLAN` job to `completed`.
        *   `[ ]` 32.c.iii. **Test Case (Parent/Child Completion - Failure):**
            *   `[ ]` 32.c.iii.i. **Arrange**: Create a `PLAN` job with two child `EXECUTE` jobs.
            *   `[ ]` 32.c.iii.ii. **Act**: `UPDATE` the first child job to `completed`, then `UPDATE` the second child job to `failed`.
            *   `[ ]` 32.c.iii.iii. **Assert**: After the second update, the trigger automatically updates the parent `PLAN` job to `failed`.
        *   `[ ]` 32.c.iv. **Test Case (Full DAG Failure Propagation):**
            *   `[ ]` 32.c.iv.i. **Arrange**: Set up jobs for a multi-step dependency chain (A -> B -> C).
            *   `[ ]` 32.c.iv.ii. **Act**: `UPDATE` job A to `failed`.
            *   `[ ]` 32.c.iv.iii. **Assert**: The trigger propagates failure, marking both job B and job C as `failed`.
        *   `[ ]` 32.c.v. **Test Case (Handling `retry_loop_failed`):**
            *   `[ ]` 32.c.v.i. **Arrange**: Create a `PLAN` job with a child `EXECUTE` job.
            *   `[ ]` 32.c.v.ii. **Act**: `UPDATE` the child job to `retry_loop_failed`.
            *   `[ ]` 32.c.v.iii. **Assert**: The trigger immediately marks the parent `PLAN` job as `failed`.
        *   `[ ]` 32.c.vi. **Test Case (DAG Fan-in Success - Many-to-One):**
            *   `[ ]` 32.c.vi.i. **Arrange**: Create a job C that depends on two prerequisite jobs (A and B).
            *   `[ ]` 32.c.vi.ii. **Act**: `UPDATE` job A to `completed`.
            *   `[ ]` 32.c.vi.iii. **Assert**: Job C remains `waiting_for_prerequisite`.
            *   `[ ]` 32.c.vi.iv. **Act**: `UPDATE` job B to `completed`.
            *   `[ ]` 32.c.vi.v. **Assert**: The trigger updates job C's status to `pending`.
        *   `[ ]` 32.c.vii. **Test Case (Full Lifecycle and Session Completion):**
            *   `[ ]` 32.c.vii.i. **Arrange**: Set up the full parent `PLAN` and child `EXECUTE` job structure for a stage.
            *   `[ ]` 32.c.vii.ii. **Act**: Sequentially `UPDATE` all child `EXECUTE` jobs to `completed`.
            *   `[ ]` 32.c.vii.iii. **Assert**: After the final child job is completed, the trigger first updates the parent `PLAN` job to `completed`. Subsequently, the trigger recognizes the stage is complete and updates the `dialectic_sessions.status` to `[stage_slug]_generation_complete`.
    *   `[ ]` 32.d. [DB] The migration `<timestamp>_implement_unified_state_trigger.sql` will perform the following actions:
        *   `[ ]` 32.d.i. Create the `manage_dialectic_state()` function containing all logic from 32.b.
        *   `[ ]` 32.d.ii. Create the `on_job_state_propagation` trigger on `dialectic_generation_jobs` that executes the function.
        *   `[ ]` 32.d.iii. `DROP` the obsolete triggers: `trigger_handle_job_completion_on_update` and `trigger_handle_job_completion_on_insert`.
        *   `[ ]` 32.d.iv. `DROP` the obsolete function: `handle_job_completion()`.
    *   `[ ]` 32.e. [CRITERIA] The system is fixed when the trigger correctly manages all parent/child relationships, all job-to-job dependencies according to the recipe's DAG, correctly propagates all terminal statuses, and the `dialectic_sessions.status` is accurately projected.

*   `[‚úÖ]` 33. **`[BE]` Fix Incorrect Database Query in `listStageDocuments`**
    *   `[‚úÖ]` 33.a. `[DEPS]` The `listStageDocuments` function in `supabase/functions/dialectic-service/listStageDocuments.ts` (lines 33-40) constructs a Supabase query that incorrectly includes a filter condition `.eq('project_id', projectId)`. The `dialectic_generation_jobs` table does not have a `project_id` column, causing the database query to fail every time. The query is already correctly scoped by `session_id` and `user_id`, making the `project_id` filter both incorrect and redundant. The fix is to remove this line from the query.
    *   `[‚úÖ]` 33.b. `[TYPES]` No changes to TypeScript types or interfaces are required for this fix.
    *   `[‚úÖ]` 33.c. `[TEST-UNIT]` **RED**: In the existing test file `supabase/functions/dialectic-service/listStageDocuments.test.ts`, modify the existing "Happy Path" test to prove the flaw.
        *   `[‚úÖ]` 33.c.i. Locate the test "listStageDocuments - Happy Path: returns normalized document descriptors and applies all security filters" (line 28).
        *   `[‚úÖ]` 33.c.ii. Within this test, find the assertion that checks for the incorrect filter: `assertEquals(jobsSpies.eq.calls[4].args, ['project_id', PROJECT_ID]);` (line 125).
        *   `[‚úÖ]` 33.c.iii. Change this assertion to reflect the desired GREEN state: assert that only four `eq` calls are made in total, and that none of them are for `project_id`. For example: `assertEquals(jobsSpies.eq.calls.length, 4);` and add a loop to ensure none of the calls match `['project_id', PROJECT_ID]`. This test must fail initially because the code currently makes five `eq` calls, including the incorrect one.
    *   `[‚úÖ]` 33.d. `[BE]` **GREEN**: In `supabase/functions/dialectic-service/listStageDocuments.ts`, correct the database query to align with the database schema.
        *   `[‚úÖ]` 33.d.i. In the Supabase query chain within the `listStageDocuments` function, locate and delete the line `.eq('project_id', projectId);` (line 40).
    *   `[‚úÖ]` 33.e. `[TEST-UNIT]` **GREEN**: Re-run the test from `supabase/functions/dialectic-service/listStageDocuments.test.ts`. The exact same test modified in step 33.c will now pass because the call to `.eq('project_id', projectId)` has been removed, proving the fix.
    *   `[‚úÖ]` 33.f. `[CRITERIA]` The work is complete when: (1) The `.eq('project_id', projectId)` line is removed from the query in `listStageDocuments.ts`, (2) The modified unit test passes, proving the query is constructed correctly, (3) The integration test that previously failed on this error now proceeds past that point.
    *   `[‚úÖ]` 33.g. `[COMMIT]` `fix(BE): Remove invalid project_id filter from listStageDocuments query`

*   `[‚úÖ]` 34. **`[BE]` Fix Type Guard Validation Failure in `mapToStageWithRecipeSteps`**
    *   `[‚úÖ]` 34.a. `[DEPS]` The `mapToStageWithRecipeSteps` function in `supabase/functions/_shared/utils/mappers.ts` fails for the `synthesis` stage with the error: `Failed to map and validate recipe step: synthesis_prepare_pairwise_header. The transformed object did not pass type guard validation.` This occurs at the final validation check (`if (!isDialecticStageRecipeStep(transformedStep))`, lines 124-132), indicating that the `transformedStep` object being created does not conform to the `DialecticStageRecipeStep` interface due to a property mismatch.
    *   `[‚úÖ]` 34.b. `[TYPES]` No changes to the `DialecticStageRecipeStep` type definition itself are required. The bug is in the logic that creates an object intended to match this type.
    *   `[‚úÖ]` 34.c. `[TEST-UNIT]` **RED**: In the existing test file `supabase/functions/_shared/utils/mappers.test.ts`, add a new test case to prove the mapping flaw.
        *   `[‚úÖ]` 34.c.i. Add a new test case titled "should correctly map a synthesis recipe step without failing type guard validation".
        *   `[‚úÖ]` 34.c.ii. Inside the test, construct a mock `DatabaseRecipeSteps` object that accurately represents the data for the `synthesis_prepare_pairwise_header` step, including an `output_type` that is a valid `FileType` but may be transformed incorrectly by the current logic.
        *   `[‚úÖ]` 34.c.iii. Call `mapToStageWithRecipeSteps` with this mock object.
        *   `[‚úÖ]` 34.c.iv. Assert that the function call does **not** throw an error. This test asserts the desired final (GREEN) state. It must fail initially because the current logic *does* throw an error, thus proving the flaw.
    *   `[‚úÖ]` 34.d. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/mappers.ts`, correct the property transformation logic.
        *   `[‚úÖ]` 34.d.i. The property `output_type` is unnecessarily transformed by `toSnakeCase` at line 55. This transformation is the cause of the type mismatch. At line 118, change `output_type: outputType` to `output_type: step.output_type`.
        *   `[‚úÖ]` 34.d.ii. Delete the `const outputType = toSnakeCase(step.output_type);` line (line 55).
        *   `[‚úÖ]` 34.d.iii. At line 68, change `if (!isFileType(outputType))` to `if (!isFileType(step.output_type))`.
    *   `[‚úÖ]` 34.e. `[TEST-UNIT]` **GREEN**: Re-run the new unit test created in step 34.c within `supabase/functions/_shared/utils/mappers.test.ts`. The exact same test will now pass because the function will produce a valid object that does not throw an error.
    *   `[‚úÖ]` 34.f. `[CRITERIA]` The work is complete when: (1) The property mapping in `mapToStageWithRecipeSteps` is corrected, (2) The new unit test passes, proving the mapping logic is correct in isolation, (3) The integration test for the synthesis stage proceeds past the point of failure.
    *   `[‚úÖ]` 34.g. `[COMMIT]` `fix(BE): Correct property mapping in mapToStageWithRecipeSteps to pass type guard validation`

*   `[‚úÖ]` 35. **`[BE]` Fix planPerSourceDocument to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 35.a. `[DEPS]` The `planPerSourceDocument` function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` (line 285) constructs the `document_relationships` object for child jobs as `{ source_group: doc.id }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "thesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[thesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. This has been confirmed by analyzing the logs for job `46d1bab8-e01e-4bd0-8ec6-3dc4a4be4f35`, which was generated by the `generate-success-metrics` recipe step (UUID `11931537-7fcd-4623-9855-0df54bedc6ce`) configured with `granularity_strategy = 'per_source_document'`. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `doc.id`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 35.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 35.b.i. Add a test case "planPerSourceDocument includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'thesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planPerSourceDocument`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('thesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` key.
    *   `[‚úÖ]` 35.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 35.c.i. In the `DialecticExecuteJobPayload` construction block (around line 285), update the `document_relationships` property. Instead of just `{ source_group: doc.id }`, change it to `{ source_group: doc.id, [stageSlug]: doc.id }`.
        *   `[‚úÖ]` 35.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 61).
    *   `[‚úÖ]` 35.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 35.b and ensure it now passes. The `document_relationships` object should now contain both `source_group` and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 35.e. `[TEST-INT]` Prove that `planPerSourceDocument` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 35.e.i. Create a new integration test file `supabase/integration_tests/services/planner_validation.integration.test.ts`.
        *   `[‚úÖ]` 35.e.ii. Add an integration test "planPerSourceDocument produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planPerSourceDocument` directly with a mock parent job (stageSlug='thesis') and source document, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['thesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 35.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 35.g. `[CRITERIA]` All requirements are met: (1) `planPerSourceDocument` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `doc.id`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing `source_group` logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 35.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planPerSourceDocument to enable RENDER job creation`

*   `[ ]` 36. **`[BE]` Fix Try-Catch Swallowing Exceptions in executeModelCallAndSave RENDER Job Enqueueing**
    *   `[ ]` 36.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1328-1425) contains a try-catch block that swallows ALL exceptions during RENDER job enqueueing, preventing error propagation to the caller (`processSimpleJob`). The function has three distinct failure points: (Point A, line 1331) `shouldEnqueueRenderJob()` database queries can fail, (Point B, lines 1336-1396) payload validation throws for missing/invalid fields, and (Point C, lines 1410-1421) database insert can fail for RLS/FK violations. **CRITICAL UNDERSTANDING:** EXECUTE and RENDER jobs share the same recipe step which is already validated before EXECUTE job creation. Therefore, validation at lines 1336-1396 checks that OUR CODE correctly extracted/passed data from the already-valid recipe step (not validating the recipe step itself). The current error handling is inconsistent: validation logic (Point B) throws exceptions for code bugs but the try-catch at lines 1423-1425 catches them all and only logs, lines 1416-1421 check `renderInsertError` but only log without throwing, and the function continues to line 1427+ as if RENDER job succeeded, with no error propagation to `processSimpleJob` which expects exceptions per its try-catch at lines 302-309. The `Promise<void>` return type forces exception-based error handling, but swallowing exceptions breaks this contract‚Äîthe caller assumes success when the promise resolves. This makes debugging impossible: EXECUTE jobs complete successfully while RENDER jobs silently fail to be created, and errors are logged but hidden from job status. The fix requires: (1) removing the try-catch at lines 1423-1425 that swallows all errors, (2) categorizing database insert errors at lines 1416-1421: throw immediately for programmer errors (FK violations, constraint violations), implement retry or throw for transient errors (connection timeouts), (3) keeping validation exactly where it is (lines 1336-1396) since it checks code correctness, not recipe validity, (4) letting validation exceptions propagate naturally‚Äîif validation fails, it's a code bug that should fail EXECUTE to force developer to fix.
    *   `[ ]` 36.b. `[TYPES]` No TypeScript type changes required. The function already returns `Promise<void>` and uses strict typing for `renderPayload` with `isDialecticRenderJobPayload()` and `isJson()` type guards. The fix is purely about restructuring error handling flow, not changing types.
    *   `[ ]` 36.c. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.renderErrors.test.ts` (new file), add tests that prove the try-catch swallows exceptions and prevents error propagation.
        *   `[ ]` 36.c.i. Add a test case "executeModelCallAndSave throws exception when RENDER payload validation fails for missing documentKey" that: (1) creates an EXECUTE job with markdown `output_type`, (2) mocks `shouldEnqueueRenderJob` to return `true` (rendering required), (3) sets up state where `validatedDocumentKey` is undefined (simulating validation failure at lines 1350-1353), (4) calls `executeModelCallAndSave`, (5) asserts that the function THROWS an exception with message "documentKey is required for RENDER job", (6) asserts that the EXECUTE job does NOT complete successfully (error is propagated). This test must initially FAIL because the current try-catch (lines 1423-1425) swallows the validation exception, allowing the function to complete without throwing.
        *   `[ ]` 36.c.ii. Add a test case "executeModelCallAndSave throws exception when RENDER payload validation fails for missing documentIdentity" that: (1) creates an EXECUTE job with markdown `output_type`, (2) mocks `shouldEnqueueRenderJob` to return `true`, (3) sets up state where `documentIdentity` is undefined (simulating validation failure at lines 1354-1357), (4) calls `executeModelCallAndSave`, (5) asserts that the function THROWS an exception with message "documentIdentity is required for RENDER job", (6) asserts the error is propagated to the caller. This test must initially FAIL because the try-catch swallows the validation exception.
        *   `[ ]` 36.c.iii. Add a test case "executeModelCallAndSave throws exception when database insert fails for RENDER job" that: (1) creates an EXECUTE job with markdown `output_type` and valid payload, (2) mocks `shouldEnqueueRenderJob` to return `true`, (3) mocks the database insert (`dbClient.from('dialectic_generation_jobs').insert()`) to return an error (simulating RLS policy rejection or FK constraint violation), (4) calls `executeModelCallAndSave`, (5) asserts that the function THROWS an exception containing the database error details, (6) asserts the error is propagated to the caller. This test must initially FAIL because lines 1416-1421 only log the `renderInsertError` without throwing, and the try-catch swallows any exception that might occur.
        *   `[ ]` 36.c.iv. Add a test case "executeModelCallAndSave throws exception when shouldEnqueueRenderJob query fails" that: (1) creates an EXECUTE job with markdown `output_type`, (2) mocks `shouldEnqueueRenderJob` to throw a database error (simulating connection failure), (3) calls `executeModelCallAndSave`, (4) asserts that the function THROWS an exception containing the query error details, (5) asserts the error is propagated to the caller. This test must initially FAIL because the try-catch at lines 1423-1425 swallows the exception from `shouldEnqueueRenderJob`.
    *   `[ ]` 36.d. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, remove silent failure and categorize database errors.
        *   `[ ]` 36.d.i. **STEP 1 - Remove silent failure try-catch:** Remove the try-catch block at lines 1423-1425 that currently swallows all errors. This allows validation exceptions (code bugs) to propagate naturally to `processSimpleJob` which will mark the EXECUTE job as 'failed', forcing the developer to fix the code issue.
        *   `[ ]` 36.d.ii. **STEP 2 - Categorize database insert errors:** At lines 1416-1421, when `renderInsertError` exists, inspect the error details to determine if it's a programmer error or transient error. If programmer error (FK violation like "foreign key constraint", constraint violation like "unique constraint", RLS policy rejection): throw `RenderJobEnqueueError` immediately with descriptive message including error details. If transient error (connection timeout like "connection refused", "too many connections"): implement local retry with exponential backoff OR throw to trigger job-level retry via existing retry mechanism.
        *   `[ ]` 36.d.iii. **STEP 3 - Keep validation exactly where it is:** The validation logic at lines 1336-1396 stays in place unchanged. It checks that our code correctly extracted `validatedDocumentKey` and `documentIdentity` from the already-validated recipe step. If validation fails, it's a code bug (not a recipe problem), so the exception should propagate to fail the EXECUTE job and alert the developer.
        *   `[ ]` 36.d.iv. **ERROR CLASSES:** Use custom error classes from `supabase/functions/_shared/utils/errors.ts`: `RenderJobValidationError` (thrown by validation at lines 1350-1396 for code bugs extracting data from valid recipe step), `RenderJobEnqueueError` (thrown at lines 1416-1421 for programmer errors during database insert like FK violations). Import these at the top of the file.
        *   `[ ]` 36.d.v. Verify the restructured code follows this flow: (1) Validation logic (lines 1336-1396) remains in place, throws `RenderJobValidationError` for code bugs, (2) No try-catch wrapping validation‚Äîexceptions propagate naturally, (3) Database insert errors at lines 1416-1421 are categorized and thrown (programmer errors throw `RenderJobEnqueueError`, transient errors retry or throw), (4) All exceptions propagate to `processSimpleJob` which marks job as 'failed' with error details.
    *   `[ ]` 36.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 36.c and ensure they now pass. The tests should prove that: (1) Validation exceptions for missing `documentKey` are thrown and propagated (test 36.c.i passes), (2) Validation exceptions for missing `documentIdentity` are thrown and propagated (test 36.c.ii passes), (3) Database insert failures throw exceptions and are propagated (test 36.c.iii passes), (4) Query failures from `shouldEnqueueRenderJob` throw exceptions and are propagated (test 36.c.iv passes). All tests demonstrate that exceptions are no longer swallowed and errors reach the caller.
    *   `[ ]` 36.f. `[TEST-INT]` Prove error propagation works correctly with the caller `processSimpleJob` by adding test cases to `supabase/integration_tests/services/executeModelCallAndSave.integration.test.ts`
        *   `[ ]` 36.f.i. Assert that when `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (test subject) throws an exception due to RENDER payload validation failure, `processSimpleJob` in `supabase/functions/dialectic-worker/processSimpleJob.ts` (consumer) catches the exception in its try-catch block (lines 302-309), marks the EXECUTE job status as 'failed', and updates the job record with error details. Create an integration test that: (1) sets up an EXECUTE job with markdown `output_type` and invalid state (missing `documentKey`), (2) calls `processSimpleJob` with the job, (3) verifies `executeModelCallAndSave` throws a validation exception, (4) verifies `processSimpleJob` catches the exception and updates the job status to 'failed' with appropriate error_details, (5) verifies the job does NOT complete successfully, proving validation errors are now visible at the job level.
        *   `[ ]` 36.f.ii. Assert that when `executeModelCallAndSave` (test subject) throws an exception due to database insert failure for RENDER job, `processSimpleJob` (consumer) catches it and marks the EXECUTE job as 'failed'. Create an integration test that: (1) sets up an EXECUTE job with markdown `output_type` and valid payload, (2) mocks the database insert to fail (simulating RLS rejection), (3) calls `processSimpleJob`, (4) verifies `executeModelCallAndSave` throws an exception containing database error details, (5) verifies `processSimpleJob` catches it and updates the job to 'failed', proving database errors are now propagated and visible.
        *   `[ ]` 36.f.iii. Assert that when `shouldEnqueueRenderJob` throws a query exception, the error propagates through `executeModelCallAndSave` (test subject) to `processSimpleJob` (consumer) and the EXECUTE job is marked as 'failed'. Create an integration test that: (1) sets up an EXECUTE job with markdown `output_type`, (2) mocks `shouldEnqueueRenderJob` to throw a database connection error, (3) calls `processSimpleJob`, (4) verifies the exception propagates and the job is marked 'failed', proving query failures are no longer silently swallowed.
    *   `[ ]` 36.g. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts`, and resolve any warnings or errors.
    *   `[ ]` 36.h. `[CRITERIA]` All requirements are met: (1) The try-catch at lines 1423-1425 that swallowed all errors has been removed, (2) Database insert errors at lines 1416-1421 are now categorized and thrown: programmer errors (FK violations, constraint violations) throw `RenderJobEnqueueError` immediately, transient errors (connection timeouts) retry or throw, (3) Validation logic (lines 1336-1396) remains in place unchanged, checking that code correctly extracted data from the already-validated recipe step, (4) Validation exceptions propagate naturally to `processSimpleJob` (no try-catch swallowing them), (5) The `Promise<void>` error handling contract is restored‚Äîexceptions are thrown on failures, not swallowed, (6) All tests pass, including tests that verify validation exceptions, database insert failures, and query failures are thrown and propagated, (7) Integration tests prove `processSimpleJob` receives exceptions and marks jobs as 'failed', (8) The file is lint-clean, (9) RENDER job failures are now visible: errors are logged AND propagated, EXECUTE jobs fail when RENDER job creation fails (instead of silently completing), errors are actionable (job status reflects failure, error_details contain diagnostic information), (10) The fix correctly understands that EXECUTE and RENDER share the same validated recipe step, so validation checks code correctness, not recipe validity.
    *   `[ ]` 36.i. `[COMMIT]` `fix(be): remove try-catch swallowing RENDER job errors and categorize database insert failures in executeModelCallAndSave`

*   `[ ]` 37. **[BE]** Fix validatedDocumentKey scope mismatch in executeModelCallAndSave
    *   `[‚úÖ]`   37.a. [DEPS] Identify variable scope issue between fileType and output_type
        *   `[‚úÖ]` 37.a.i. Line 1136: `const fileType: ModelContributionFileTypes = output_type;` - fileType initialized to output_type (the document key like 'business_requirements_document')
        *   `[‚úÖ]` 37.a.ii. Line 1219-1226: `validatedDocumentKey` is set IF `isDocumentKey(fileType)` returns true
        *   `[‚úÖ]` 37.a.iii. Line 1230-1232: `storageFileType` is computed - IF `isDocumentKey(fileType)` THEN use `FileType.ModelContributionRawJson` ELSE use original `fileType`
        *   `[‚úÖ]` 37.a.iv. Line 1237+: `uploadContext` uses `storageFileType` for file storage (raw_responses/ folder)
        *   `[‚úÖ]` 37.a.v. Line 1331: `shouldEnqueueRenderJob` checks ORIGINAL `output_type` (not modified fileType or storageFileType)
        *   `[‚úÖ]` 37.a.vi. Line 1350: RENDER job creation expects `validatedDocumentKey` to be defined
        *   `[‚úÖ]` 37.a.vii. **BUG**: `validatedDocumentKey` populated based on `fileType` at line 1220, but `shouldEnqueueRenderJob` uses `output_type` - these may diverge
        *   `[‚úÖ]` 37.a.viii. **BUG**: For document outputs, `isDocumentKey(fileType)` may return false if fileType was modified, causing `validatedDocumentKey` to remain undefined
    *   `[‚úÖ]`   37.b. [TYPES] Define correct variable usage for document key tracking
        *   `[‚úÖ]` 37.b.i. `output_type` (from job.payload): The semantic document key representing WHAT is being generated ('business_case')
        *   `[‚úÖ]` 37.b.ii. `fileType`: Physical file type for storage purposes (may be transformed to 'ModelContributionRawJson' for raw storage)
        *   `[‚úÖ]` 37.b.iii. `storageFileType`: Computed value for FileManagerService determining storage path
        *   `[‚úÖ]` 37.b.iv. `validatedDocumentKey`: Must represent the SEMANTIC document key (output_type), not the storage file type
        *   `[‚úÖ]` 37.b.v. `shouldEnqueueRenderJob({ outputType: output_type })`: Queries recipe using SEMANTIC document key
        *   `[‚úÖ]` 37.b.vi. **RULE**: `validatedDocumentKey` must be populated based on `output_type`, NOT `fileType`, because RENDER job needs the semantic document key
    *   `[‚úÖ]`   37.c. [TEST-UNIT] Write RED test proving validatedDocumentKey is undefined when it should be defined
        *   `[‚úÖ]` 37.c.i. Test: EXECUTE job with `output_type = 'business_case'` (a document key)
        *   `[‚úÖ]` 37.c.ii. Test: `isDocumentKey(output_type)` returns true, `shouldEnqueueRenderJob` returns true (markdown document)
        *   `[‚úÖ]` 37.c.iii. Test: Trace variable values through execution: `fileType = output_type`, then `storageFileType = FileType.ModelContributionRawJson`
        *   `[‚úÖ]` 37.c.iv. Assert: At line 1350, `validatedDocumentKey` should equal `job.payload.document_key`
        *   `[‚úÖ]` 37.c.v. Current behavior: If `isDocumentKey(fileType)` was checked AFTER `storageFileType` assignment, `validatedDocumentKey` may be undefined
        *   `[‚úÖ]` 37.c.vi. Expected: Test FAILS in current implementation (validatedDocumentKey undefined, line 1350 throws, exception silently caught)
    *   `[‚úÖ]`   37.d. [IMPLEMENTATION] Fix validatedDocumentKey to use output_type instead of fileType
        *   `[‚úÖ]` 37.d.i. Change line 1220 from `if (isDocumentKey(fileType))` to `if (isDocumentKey(output_type))`
        *   `[‚úÖ]` 37.d.ii. Rationale: `output_type` is the semantic document key that never changes, while `fileType` may be transformed for storage purposes
        *   `[‚úÖ]` 37.d.iii. Ensure `validatedDocumentKey` is set BEFORE any file type transformations occur
        *   `[‚úÖ]` 37.d.iv. Add assertion: `validatedDocumentKey` must be defined when entering RENDER job creation block (line 1337+)
        *   `[‚úÖ]` 37.d.v. Add logging: Log `output_type`, `fileType`, `storageFileType`, `validatedDocumentKey` before RENDER job creation for debugging
    *   `[‚úÖ]`   37.e. [TEST-UNIT] Rerun tests proving validatedDocumentKey is correctly populated
        *   `[‚úÖ]` 37.e.i. Test: Same scenario as 37.c - markdown document output
        *   `[‚úÖ]` 37.e.ii. Assert: `validatedDocumentKey === job.payload.document_key`
        *   `[‚úÖ]` 37.e.iii. Assert: RENDER job is created successfully with `documentKey: validatedDocumentKey`
        *   `[‚úÖ]` 37.e.iv. Assert: No exception thrown, no silent failure
    *   `[ ]`   37.f. [TEST-INT] Prove document generation ‚Üí RENDER job ‚Üí rendered output flow works end-to-end
        *   `[ ]` 37.f.i. Integration test: Create EXECUTE job for markdown document ‚Üí executeModelCallAndSave ‚Üí RENDER job created ‚Üí processRenderJob ‚Üí document_renderer ‚Üí markdown file in dialectic_project_resources
        *   `[ ]` 37.f.ii. Assert: RENDER job payload contains correct `documentKey` matching original `output_type`
        *   `[ ]` 37.f.iii. Assert: Rendered markdown file exists in storage with correct path
    *   `[ ]`   37.g. [CRITERIA] validatedDocumentKey is always correctly populated for document outputs, RENDER jobs are created with correct documentKey, no scope-related failures occur
    *   `[ ]`   37.h. [COMMIT] Fix validatedDocumentKey scope bug - use output_type instead of fileType

*   `[ ]` 38. **[RESEARCH]** Design unified state management replacing handle_job_completion with manage_dialectic_state
    *   `[ ]`   38.a. [DEPS] Map complete state machine and identify conflicts
        *   `[ ]` 38.a.i. Current Triggers:
            *   `on_job_state_propagation` (from 20251217224555_implement_unified_state_trigger.sql) - Fires on INSERT or UPDATE of status, calls `manage_dialectic_state()`
            *   `on_job_status_change` (from 20251119160820_retrying_trigger.sql) - Fires on UPDATE to pending/retrying/pending_next_step/pending_continuation, calls `invoke_worker_on_status_change()`
            *   `on_job_terminal_state` (dropped by 20251217224555) - Previously fired on terminal states, called `handle_job_completion()` (now dropped)
        *   `[ ]` 38.a.ii. Current Functions:
            *   `manage_dialectic_state()` - Part 1: DAG-based job-to-job propagation, Part 2: Session-level completion detection
            *   `invoke_worker_on_status_change()` - HTTP POST to dialectic-worker when job reaches processable status
            *   `handle_job_completion()` (NOT dropped, still exists!) - Manages parent_job_id and prerequisite_job_id relationships, counts siblings, excludes RENDER jobs
        *   `[ ]` 38.a.iii. Application Functions:
            *   `processComplexJob()` - Lines 183-382: Tracks in-progress jobs, completed source documents, determines ready steps, plans child jobs
            *   `executeModelCallAndSave()` - Lines 1536-1544: Marks EXECUTE job as completed, updates attempt_count
        *   `[ ]` 38.a.iv. **CONFLICT IDENTIFIED**: `manage_dialectic_state()` Part 1 (lines 36-97) handles DAG edges but ignores `parent_job_id` and `prerequisite_job_id`
        *   `[ ]` 38.a.v. **CONFLICT IDENTIFIED**: `handle_job_completion()` lines 388-406 manages parent-child and sibling completion, but `manage_dialectic_state()` doesn't replicate this logic
        *   `[ ]` 38.a.vi. **CONFLICT IDENTIFIED**: `processComplexJob()` lines 282-285 excludes steps with ONLY completed jobs from re-planning, but `manage_dialectic_state()` Part 2 (lines 110-116) counts DISTINCT step_slugs without checking for mixed states
        *   `[ ]` 38.a.vii. **CONFLICT IDENTIFIED**: Step 32 migration drops `on_job_terminal_state` trigger but does NOT drop `handle_job_completion()` function - if trigger still exists from older migration, both triggers will fire
    *   `[ ]`   38.b. [TYPES] Define complete state transition table
        *   `[ ]` 38.b.i. Research Task: Document ALL possible job status values and transitions
        *   `[ ]` 38.b.ii. Research Task: Map which statuses require worker invocation vs. trigger-only state changes
        *   `[ ]` 38.b.iii. Research Task: Define terminal states, in-progress states, waiting states
        *   `[ ]` 38.b.iv. Research Task: Document relationship between job_type (PLAN/EXECUTE/RENDER) and status transitions
        *   `[ ]` 38.b.v. Research Task: Define when parent jobs transition based on child completion
        *   `[ ]` 38.b.vi. Research Task: Define when session status changes based on job completion
    *   `[ ]`   38.c. [TYPES] Define step completion tracking requirements
        *   `[ ]` 38.c.i. Problem: Recipe step may spawn N jobs (M models √ó P source documents √ó Q continuation chunks)
        *   `[ ]` 38.c.ii. Problem: Counting DISTINCT step_slug with status='completed' doesn't account for partial completion (some jobs complete, others pending)
        *   `[ ]` 38.c.iii. Problem: `processComplexJob` filters out steps with in-progress jobs (line 285), but trigger doesn't have this logic
        *   `[ ]` 38.c.iv. Research Task: Should trigger count (completed_jobs_for_step / total_jobs_for_step) per step, or rely on application to mark steps complete?
        *   `[ ]` 38.c.v. Research Task: Should we add `step_completion_status` table that `processComplexJob` updates, and trigger queries?
        *   `[ ]` 38.c.vi. Research Task: How do RENDER jobs (which are side-effects) factor into step completion? Currently excluded by `handle_job_completion` line 362
    *   `[ ]`   38.d. [RESEARCH] Investigate current production failures and looping
        *   `[ ]` 38.d.i. Research Task: Query production logs for `[processComplexJob]` entries showing repeated planning for same step_slug
        *   `[ ]` 38.d.ii. Research Task: Check if `completedStepSlugs.has(slug)` check (processComplexJob.ts:282) is preventing re-planning correctly
        *   `[ ]` 38.d.iii. Research Task: Verify trigger query `SELECT count(DISTINCT step_slug) ... WHERE status = 'completed'` is counting correctly
        *   `[ ]` 38.d.iv. Research Task: Check if session status is being set to `*_generation_complete` prematurely (trigger line 134)
        *   `[ ]` 38.d.v. Research Task: Verify `v_required_steps_count` (line 105-108) matches actual recipe step requirements
        *   `[ ]` 38.d.vi. Research Task: Check if in-progress RENDER jobs are preventing stage completion
    *   `[ ]`   38.e. [RESEARCH] Design single source of truth for state management
        *   `[ ]` 38.e.i. Option A: Triggers handle ALL state transitions, application functions are passive observers
        *   `[ ]` 38.e.ii. Option B: Application functions handle state transitions, triggers only invoke worker
        *   `[ ]` 38.e.iii. Option C: Hybrid - triggers handle prerequisite unblocking, application handles step/session completion
        *   `[ ]` 38.e.iv. Research Task: Evaluate which option aligns with current architecture (job-driven vs. event-driven)
        *   `[ ]` 38.e.v. Research Task: Consider transaction boundaries - can trigger and application update same job without conflicts?
        *   `[ ]` 38.e.vi. Research Task: Consider testing complexity - triggers are harder to test than application functions
    *   `[ ]`   38.f. [RESEARCH] Reconcile DAG edges with parent_job_id and prerequisite_job_id
        *   `[ ]` 38.f.i. Research Task: Are DAG edges (dialectic_stage_recipe_edges) and parent_job_id representing the same relationships?
        *   `[ ]` 38.f.ii. Research Task: When should prerequisite_job_id be used vs. DAG edge dependencies?
        *   `[ ]` 38.f.iii. Research Task: Can we eliminate parent_job_id in favor of DAG edges, or do they serve different purposes?
        *   `[ ]` 38.f.iv. Research Task: RENDER jobs have parent_job_id pointing to EXECUTE job - how does this fit into DAG model?
        *   `[ ]` 38.f.v. Research Task: Continuation jobs have target_contribution_id - is this a prerequisite_job_id relationship?
    *   `[ ]`   38.g. [CRITERIA] Complete state machine documented, conflicts identified, design decision made on single source of truth, integration plan created
    *   `[ ]`   38.h. **[HOLD]** Cannot proceed to implementation until research complete and design approved

*   `[ ]` 39. **[IMPLEMENTATION]** Implement unified state management solution
    *   `[ ]`   39.a. [DEPS] **TO BE DEFINED** - Based on Step 35 research findings
    *   `[ ]`   39.b. [TYPES] **TO BE DEFINED** - Based on Step 35 design decisions
    *   `[ ]`   39.c. [TEST-UNIT] **TO BE DEFINED** - Based on Step 35 requirements
    *   `[ ]`   39.d. [IMPLEMENTATION] **TO BE DEFINED** - Based on Step 35 approved design
    *   `[ ]`   39.e. [TEST-UNIT] **TO BE DEFINED** - Based on Step 35 success criteria
    *   `[ ]`   39.f. [TEST-INT] **TO BE DEFINED** - Based on Step 35 integration requirements
    *   `[ ]`   39.g. [CRITERIA] **TO BE DEFINED** - Based on Step 35 acceptance criteria
    *   `[ ]`   39.h. [COMMIT] **TO BE DEFINED** - Based on Step 35 scope

*   `[‚úÖ]` 40. **`[BE]` Fix planAllToOne to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 40.a. `[DEPS]` The `planAllToOne` function in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` (lines 152 and 278) constructs the `document_relationships` object for child jobs as `{ source_group: anchorDocument.id }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "thesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[thesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `anchorDocument.id`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 40.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 40.b.i. Add a test case "planAllToOne includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'thesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planAllToOne`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('thesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` key.
    *   `[‚úÖ]` 40.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 40.c.i. In the `DialecticExecuteJobPayload` construction block (around lines 152 and 278), update the `document_relationships` property. Instead of just `{ source_group: anchorDocument.id }`, change it to `{ source_group: anchorDocument.id, [stageSlug]: anchorDocument.id }`.
        *   `[‚úÖ]` 40.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 17).
    *   `[‚úÖ]` 40.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 40.b and ensure it now passes. The `document_relationships` object should now contain both `source_group` and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 40.e. `[TEST-INT]` Prove that `planAllToOne` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 40.e.i. In `supabase/integration_tests/services/planner_validation.integration.test.ts`, add an integration test "planAllToOne produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planAllToOne` directly with a mock parent job (stageSlug='thesis') and source document, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['thesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 40.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 40.g. `[CRITERIA]` All requirements are met: (1) `planAllToOne` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `anchorDocument.id`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing `source_group` logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 40.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planAllToOne to enable RENDER job creation`

*   `[‚úÖ]` 41. **`[BE]` Fix planPairwiseByOrigin to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 41.a. `[DEPS]` The `planPairwiseByOrigin` function in `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` (line 295) constructs the `document_relationships` object for child jobs with properties like `{ source_group: anchorDoc.id, [doc.contribution_type]: doc.id }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "synthesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[synthesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `anchorDoc.id`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 41.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 41.b.i. Add a test case "planPairwiseByOrigin includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'synthesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planPairwiseByOrigin`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('synthesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` and contribution type keys.
    *   `[‚úÖ]` 41.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 41.c.i. In the `DialecticExecuteJobPayload` construction block (around line 286), update the `document_relationships` property. Add `[stageSlug]: anchorDoc.id` to the dynamically constructed object.
        *   `[‚úÖ]` 41.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 38).
    *   `[‚úÖ]` 41.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 41.b and ensure it now passes. The `document_relationships` object should now contain `source_group`, contribution keys, and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 41.e. `[TEST-INT]` Prove that `planPairwiseByOrigin` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 41.e.i. In `supabase/integration_tests/services/planner_validation.integration.test.ts`, add an integration test "planPairwiseByOrigin produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planPairwiseByOrigin` directly with a mock parent job (stageSlug='synthesis') and source documents, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['synthesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 41.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 41.g. `[CRITERIA]` All requirements are met: (1) `planPairwiseByOrigin` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `anchorDoc.id`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 41.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planPairwiseByOrigin to enable RENDER job creation`

*   `[‚úÖ]` 42. **`[BE]` Fix planPerModel to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 42.a. `[DEPS]` The `planPerModel` function in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` (line 204) constructs the `document_relationships` object for child jobs as `{ source_group: anchorDoc.id }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "thesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[thesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `anchorDoc.id`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 42.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 42.b.i. Add a test case "planPerModel includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'thesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planPerModel`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('thesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` key.
    *   `[‚úÖ]` 42.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 42.c.i. In the `document_relationships` construction block (around line 204), update the property. Instead of just `{ source_group: anchorDoc.id }`, change it to `{ source_group: anchorDoc.id, [stageSlug]: anchorDoc.id }`.
        *   `[‚úÖ]` 42.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 45).
    *   `[‚úÖ]` 42.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 42.b and ensure it now passes. The `document_relationships` object should now contain both `source_group` and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 42.e. `[TEST-INT]` Prove that `planPerModel` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 42.e.i. In `supabase/integration_tests/services/planner_validation.integration.test.ts`, add an integration test "planPerModel produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planPerModel` directly with a mock parent job (stageSlug='thesis') and source documents, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['thesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 42.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 42.g. `[CRITERIA]` All requirements are met: (1) `planPerModel` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `anchorDoc.id`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing `source_group` logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 42.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planPerModel to enable RENDER job creation`

*   `[‚úÖ]` 43. **`[BE]` Fix planPerSourceDocumentByLineage to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 43.a. `[DEPS]` The `planPerSourceDocumentByLineage` function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` (line 265) constructs the `document_relationships` object for child jobs as `{ source_group: groupId }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "thesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[thesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `groupId`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 43.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 43.b.i. Add a test case "planPerSourceDocumentByLineage includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'thesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planPerSourceDocumentByLineage`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('thesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` key.
    *   `[‚úÖ]` 43.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 43.c.i. In the `document_relationships` construction block (around line 265), update the property. Instead of just `{ source_group: groupId }`, change it to `{ source_group: groupId, [stageSlug]: groupId }`.
        *   `[‚úÖ]` 43.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 25).
    *   `[‚úÖ]` 43.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 43.b and ensure it now passes. The `document_relationships` object should now contain both `source_group` and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 43.e. `[TEST-INT]` Prove that `planPerSourceDocumentByLineage` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 43.e.i. In `supabase/integration_tests/services/planner_validation.integration.test.ts`, add an integration test "planPerSourceDocumentByLineage produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planPerSourceDocumentByLineage` directly with a mock parent job (stageSlug='thesis') and source documents, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['thesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 43.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 43.g. `[CRITERIA]` All requirements are met: (1) `planPerSourceDocumentByLineage` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `groupId`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing `source_group` logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 43.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planPerSourceDocumentByLineage to enable RENDER job creation`

*   `[‚úÖ]` 44. **`[BE]` Fix planPerSourceGroup to Include stageSlug in document_relationships**
    *   `[‚úÖ]` 44.a. `[DEPS]` The `planPerSourceGroup` function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` (line 245) constructs the `document_relationships` object for child jobs as `{ source_group: groupId }`. However, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (lines 1341-1343) strictly validates that `document_relationships` must contain a key matching the current `stageSlug` (e.g., "thesis") to schedule a RENDER job. Because this key is missing, the worker throws an error ("document_relationships[thesis] is required...") and aborts RENDER job creation, even though the EXECUTE job completed successfully. The fix requires: (1) extracting the `stageSlug` from the parent job payload, (2) adding a dynamic key to the `document_relationships` object using this `stageSlug`, setting its value to the `sourceContributionId` (which is `groupId`), (3) ensuring this matches the validation logic in `executeModelCallAndSave`, allowing RENDER jobs to be correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 44.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, add a test case that proves the planner fails to include the `stageSlug` in `document_relationships`.
        *   `[‚úÖ]` 44.b.i. Add a test case "planPerSourceGroup includes stageSlug in document_relationships map to support RENDER job validation" that: (1) creates a parent job with a specific `stageSlug` (e.g., 'thesis'), (2) creates a recipe step that outputs a rendered document (forcing the need for RENDER validation), (3) calls `planPerSourceGroup`, (4) iterates through the returned child jobs, (5) asserts that for every EXECUTE job, `document_relationships` is defined, (6) asserts that `document_relationships` contains a key matching the `stageSlug` ('thesis'), (7) asserts that the value of this key is a string (the document ID). This test must initially fail because the current implementation only adds the `source_group` key.
    *   `[‚úÖ]` 44.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts`, modify the payload construction to include the `stageSlug`.
        *   `[‚úÖ]` 44.c.i. In the `document_relationships` construction block (around line 245), update the property. Instead of just `{ source_group: groupId }`, change it to `{ source_group: groupId, [stageSlug]: groupId }`.
        *   `[‚úÖ]` 44.c.ii. Verify that `stageSlug` is available in the scope (it is already extracted at line 27).
    *   `[‚úÖ]` 44.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 44.b and ensure it now passes. The `document_relationships` object should now contain both `source_group` and the dynamic `stageSlug` key.
    *   `[‚úÖ]` 44.e. `[TEST-INT]` Prove that `planPerSourceGroup` produces payloads that satisfy `executeModelCallAndSave` validation.
        *   `[‚úÖ]` 44.e.i. In `supabase/integration_tests/services/planner_validation.integration.test.ts`, add an integration test "planPerSourceGroup produces valid payloads for RENDER job creation" that: (1) **Producer**: Calls `planPerSourceGroup` directly with a mock parent job (stageSlug='thesis') and source documents, (2) **Subject**: Captures the generated payload, (3) **Consumer**: Calls a helper function that runs the *exact validation logic* extracted from `executeModelCallAndSave` (lines 1341-1343) against the payload's `document_relationships`. The test asserts that the validation passes (i.e., `payload.document_relationships['thesis']` exists and is valid). This proves the chain works without running the full async worker loop, isolating the data contract between planner and worker.
    *   `[‚úÖ]` 44.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 44.g. `[CRITERIA]` All requirements are met: (1) `planPerSourceGroup` now includes the `stageSlug` as a key in `document_relationships`, (2) The value of this key is the `groupId`, (3) This structure satisfies the strict validation in `executeModelCallAndSave`, (4) The RED test proves the missing key was the issue, (5) The GREEN implementation fixes it without breaking existing `source_group` logic, (6) RENDER jobs are now correctly scheduled for documents produced by this planner.
    *   `[‚úÖ]` 44.h. `[COMMIT]` `fix(be): include stageSlug in document_relationships in planPerSourceGroup to enable RENDER job creation`

*   `[‚úÖ]` 45. **`[BE]` Add Comprehensive Logging and Structured Error Reporting to shouldEnqueueRenderJob**
    *   `[‚úÖ]` 45.a. `[DEPS]` The `shouldEnqueueRenderJob` function in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` (191 lines) implements a five-step database query chain to determine if markdown rendering is required for a given `outputType` and `stageSlug`. It queries: (Step 1, lines 110-118) `dialectic_stages` for `active_recipe_instance_id`, (Step 2, lines 121-129) `dialectic_stage_recipe_instances` for instance metadata, (Step 3, lines 134-158) either `dialectic_stage_recipe_steps` (if cloned) or `dialectic_recipe_template_steps` (if not cloned), (Step 4, lines 164-186) parses JSONB `outputs_required` fields, and (Step 5, lines 29-96, 182-189) extracts markdown document keys and checks if `outputType` matches. The function exhibits a SILENT FAILURE PATTERN: it returns `false` on ANY error without logging or throwing exceptions. Specifically: (lines 116-118) if stage query fails OR `!stageData` OR `!active_recipe_instance_id` ‚Üí return `false` silently, (lines 127-129) if instance query fails OR `!instance` ‚Üí return `false` silently, (lines 141-143) if cloned steps query fails OR returns empty ‚Üí return `false` silently, (lines 153-155) if template steps query fails OR returns empty ‚Üí return `false` silently, (lines 173-178) if `JSON.parse(outputs_required)` throws ‚Üí silently skip that step, (line 189) if `outputType` not found ‚Üí return `false` (correct for JSON outputs). The CRITICAL ISSUE is the function cannot distinguish between legitimate scenarios ("this is a JSON output, no rendering needed") vs. transient failures ("database unavailable") vs. configuration errors ("recipe misconfigured") vs. permission errors ("RLS policy blocked query"). This silent failure is called at `executeModelCallAndSave.ts` line 1331 where the result determines RENDER job creation‚Äîif database is temporarily unavailable, the function returns `false`, RENDER job is skipped, and markdown documents are never rendered, all without any logged indication of why. The complex markdown document key extraction (lines 29-96) supports five legacy formats: root-level `{document_key, file_type}`, documents array, template filename detection, assembled JSON array, and files-to-generate array, mirroring frontend selector logic at `packages/store/src/dialecticStore.selectors.ts` lines 933-1000. The fix requires: (1) adding comprehensive logging for each query failure point (stage query, instance query, steps query, parse failures) with details about what failed and why, (2) returning a structured result instead of boolean: `{ shouldRender: boolean, reason: 'is_markdown' | 'is_json' | 'stage_not_found' | 'instance_not_found' | 'steps_not_found' | 'parse_error' | 'query_error' }` to enable callers to distinguish between legitimate false results and error conditions, (3) updating the caller (`executeModelCallAndSave.ts` line 1331) to handle the structured result and log/throw appropriately when `reason` indicates an error condition, (4) considering whether database errors should throw exceptions instead of returning structured errors to propagate transient failures to the job system for retry. This fix has MEDIUM IMPACT (improves debuggability and error visibility) and MEDIUM COMPLEXITY (requires changing return type and updating callers, estimated 4-8 hours).
    *   `[‚úÖ]` 45.b. `[TYPES]` Create a new interface for the structured return type in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.interface.ts`.
        *   `[‚úÖ]` 45.b.i. Add a new type `RenderCheckReason = 'is_markdown' | 'is_json' | 'stage_not_found' | 'instance_not_found' | 'steps_not_found' | 'parse_error' | 'query_error' | 'no_active_recipe'` that enumerates all possible reasons for the render decision.
        *   `[‚úÖ]` 45.b.ii. Add a new interface `ShouldEnqueueRenderJobResult = { shouldRender: boolean; reason: RenderCheckReason; details?: string; }` that provides both the decision (`shouldRender`) and diagnostic information (`reason` and optional `details` for error messages or context).
        *   `[‚úÖ]` 45.b.iii. Update the `shouldEnqueueRenderJob` function signature to return `Promise<ShouldEnqueueRenderJobResult>` instead of `Promise<boolean>`.
    *   `[‚úÖ]` 45.c. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/shouldEnqueueRenderJob.test.ts`, add tests that prove the function returns structured results with reason codes.
        *   `[‚úÖ]` 45.c.i. Add a test case "shouldEnqueueRenderJob returns {shouldRender: true, reason: 'is_markdown'} when outputType is a markdown document" that: (1) sets up database state with a stage, recipe instance, and recipe steps where `outputType` is configured as markdown, (2) calls `shouldEnqueueRenderJob`, (3) asserts the result is `{ shouldRender: true, reason: 'is_markdown' }`, proving markdown documents are correctly identified. This test must initially FAIL because the function currently returns `boolean` not structured result.
        *   `[‚úÖ]` 45.c.ii. Add a test case "shouldEnqueueRenderJob returns {shouldRender: false, reason: 'is_json'} when outputType is a JSON document" that: (1) sets up database state where `outputType` is configured as JSON (not in markdown document keys), (2) calls `shouldEnqueueRenderJob`, (3) asserts the result is `{ shouldRender: false, reason: 'is_json' }`, proving JSON outputs are correctly identified as not requiring rendering. This test must initially FAIL.
        *   `[‚úÖ]` 45.c.iii. Add a test case "shouldEnqueueRenderJob returns {shouldRender: false, reason: 'stage_not_found', details: ...} when stage query fails" that: (1) mocks the `dialectic_stages` query to return an error or empty result, (2) calls `shouldEnqueueRenderJob`, (3) asserts the result is `{ shouldRender: false, reason: 'stage_not_found', details: <error message> }`, proving database errors are distinguished from legitimate false results. This test must initially FAIL because the function currently returns `false` without reason.
        *   `[‚úÖ]` 45.c.iv. Add a test case "shouldEnqueueRenderJob returns {shouldRender: false, reason: 'no_active_recipe'} when stage exists but active_recipe_instance_id is NULL" that: (1) sets up a stage record with `active_recipe_instance_id = NULL`, (2) calls `shouldEnqueueRenderJob`, (3) asserts the result is `{ shouldRender: false, reason: 'no_active_recipe' }`, proving missing recipe configuration is reported distinctly. This test must initially FAIL.
        *   `[‚úÖ]` 45.c.v. Add a test case "shouldEnqueueRenderJob returns {shouldRender: false, reason: 'parse_error', details: ...} when outputs_required contains malformed JSON" that: (1) sets up recipe steps with `outputs_required` containing invalid JSON string, (2) calls `shouldEnqueueRenderJob`, (3) asserts the result includes `reason: 'parse_error'` with details about which step failed to parse, proving parse failures are reported. This test must initially FAIL.
        *   `[‚úÖ]` 45.c.vi. Add a test case "shouldEnqueueRenderJob logs query failures with detailed error information" that: (1) mocks a database query to fail, (2) calls `shouldEnqueueRenderJob`, (3) asserts the function logged the failure with details about which query failed (stage/instance/steps) and the error message, proving all failure points are logged for debugging. This test must initially FAIL because the function currently has no logging for query failures.
    *   `[‚úÖ]` 45.d. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts`, add comprehensive logging and return structured results.
        *   `[‚úÖ]` 45.d.i. Update the function signature to return `Promise<ShouldEnqueueRenderJobResult>` instead of `Promise<boolean>`.
        *   `[‚úÖ]` 45.d.ii. At lines 116-118 (stage query check), replace `return false` with logging and structured return. Change to: `if (stageError || !stageData) { deps.logger.warn('[shouldEnqueueRenderJob] Stage query failed or returned empty', { stageSlug, error: stageError }); return { shouldRender: false, reason: 'stage_not_found', details: stageError?.message }; }`. Add a check: `if (!stageData.active_recipe_instance_id) { deps.logger.warn('[shouldEnqueueRenderJob] Stage has no active recipe', { stageSlug }); return { shouldRender: false, reason: 'no_active_recipe' }; }`.
        *   `[‚úÖ]` 45.d.iii. At lines 127-129 (instance query check), replace `return false` with logging and structured return. Change to: `if (instanceError || !instance) { deps.logger.warn('[shouldEnqueueRenderJob] Instance query failed or returned empty', { instanceId: stageData.active_recipe_instance_id, error: instanceError }); return { shouldRender: false, reason: 'instance_not_found', details: instanceError?.message }; }`.
        *   `[‚úÖ]` 45.d.iv. At lines 141-143 (cloned steps query check), replace `return false` with logging and structured return. Change to: `if (stepsError || !steps || steps.length === 0) { deps.logger.warn('[shouldEnqueueRenderJob] Cloned steps query failed or returned empty', { instanceId: instance.id, error: stepsError }); return { shouldRender: false, reason: 'steps_not_found', details: stepsError?.message }; }`.
        *   `[‚úÖ]` 45.d.v. At lines 153-155 (template steps query check), replace `return false` with logging and structured return. Change to: `if (templateStepsError || !templateSteps || templateSteps.length === 0) { deps.logger.warn('[shouldEnqueueRenderJob] Template steps query failed or returned empty', { templateId: instance.template_id, error: templateStepsError }); return { shouldRender: false, reason: 'steps_not_found', details: templateStepsError?.message }; }`.
        *   `[‚úÖ]` 45.d.vi. At lines 173-178 (JSON parse try-catch), add logging for parse failures. In the catch block, add: `deps.logger.warn('[shouldEnqueueRenderJob] Failed to parse outputs_required', { stepId: step.id, outputsRequired: step.outputs_required, error: e instanceof Error ? e.message : String(e) });`. Consider accumulating parse errors and including them in the final result if all steps fail to parse.
        *   `[‚úÖ]` 45.d.vii. At line 189 (outputType not found), replace `return false` with structured return distinguishing between JSON outputs (legitimate) and missing configuration (error). Change to: `return { shouldRender: false, reason: 'is_json' };` (assuming if outputType is not in markdown keys, it's a JSON output). If you want to distinguish "not found in any recipe step" from "explicitly JSON", add logic to check if ANY recipe steps were processed successfully.
        *   `[‚úÖ]` 45.d.viii. When markdown document is found (success case), return `{ shouldRender: true, reason: 'is_markdown' }` instead of `true`.
        *   `[‚úÖ]` 45.d.ix. Add a logger parameter to the function dependencies if not already present. The function already receives `deps` with `dbClient`, add `logger` to `ShouldEnqueueRenderJobDeps` interface in the interface file.
    *   `[‚úÖ]` 45.e. `[LINT]` Run the linter for `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts`, `supabase/functions/_shared/utils/shouldEnqueueRenderJob.interface.ts`, `supabase/functions/_shared/utils/shouldEnqueueRenderJob.test.ts`, and `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, and resolve any warnings or errors.
    *   `[‚úÖ]` 45.f. `[CRITERIA]` All requirements are met: (1) `shouldEnqueueRenderJob` now returns structured results `ShouldEnqueueRenderJobResult` with `shouldRender`, `reason`, and optional `details` fields, (2) Comprehensive logging has been added for all query failure points (stage query, instance query, steps query, parse errors) with details about what failed and why, (3) The function distinguishes between legitimate false results (`is_json`) and error conditions (`stage_not_found`, `instance_not_found`, `steps_not_found`, `parse_error`, `no_active_recipe`), (4) The caller `executeModelCallAndSave` has been updated to handle structured results and throw exceptions for error reasons while allowing `is_json` to proceed normally, (5) Transient failures (database unavailable, RLS errors) now cause EXECUTE jobs to fail (for retry) instead of silently skipping RENDER job creation, (6) All tests pass, including tests that verify structured results for success, legitimate false, and error scenarios, (7) Integration tests prove the structured results work correctly with the caller and error conditions propagate to job failure, (8) All files are lint-clean, (9) RENDER job skipping is no longer silent: logs indicate why rendering was skipped (JSON output vs. query error vs. config missing), and errors provide actionable diagnostic information for debugging (which query failed, what error occurred, what configuration is missing).
    *   `[‚úÖ]` 45.g. `[COMMIT]` `feat(be): add comprehensive logging and structured error reporting to shouldEnqueueRenderJob to distinguish legitimate false results from transient failures`

*   `[ ]` 46. **`[BE]` Create JobContext Interface and Factory**
    *   `[ ]` 46.a. `[DEPS]` The current `IDialecticJobDeps` interface (defined in `supabase/functions/dialectic-service/dialectic.interface.ts` at line 1093) is used throughout the dialectic-worker family (`executeModelCallAndSave`, `processSimpleJob`, `processComplexJob`, etc.). Currently, adding a new utility (like `shouldEnqueueRenderJob`) to this interface requires updating: (1) the interface definition, (2) the `createDialecticWorkerDeps` factory in `index.ts`, (3) all call sites that construct deps, (4) all test factories that mock deps‚Äîtotaling 15-30 files. The JobContext pattern solves this by creating a single, immutable context object constructed once at the application boundary and passed down the call chain. Future utility additions will only require updating: (1) the JobContext interface, (2) the `createJobContext` factory, (3) the application boundary construction‚Äîtotaling 3 files. The initial JobContext will mirror `IDialecticJobDeps` exactly (1:1 field mapping) to ensure zero behavioral change during migration. This step creates: (1) `JobContext.interface.ts` defining the context type and factory params, (2) `createJobContext.ts` implementing the factory function with its tests.
    *   `[ ]` 46.b. `[TYPES]` In a new file `supabase/functions/dialectic-worker/types/JobContext.interface.ts`, define the JobContext types:
        *   `[ ]` 46.b.i. Create the file and import all types from `IDialecticJobDeps` and `GenerateContributionsDeps`: `SupabaseClient<Database>`, `ILogger`, `IFileManager`, `NotificationServiceType`, `IRagService`, `ITokenWalletService`, `IDocumentRenderer`, `IIndexingService`, `IEmbeddingClient`, `IPromptAssembler`, and all function type signatures from those interfaces.
        *   `[ ]` 46.b.ii. Define `export interface JobContext` with all fields from `IDialecticJobDeps`, using explicit type annotations for each field. Include: `logger: ILogger`, `getSeedPromptForStage: (dbClient: SupabaseClient<Database>, projectId: string, sessionId: string, stageSlug: string, iterationNumber: number, downloadFromStorage: (bucket: string, path: string) => Promise<DownloadStorageResult>) => Promise<SeedPromptData>`, `continueJob`, `retryJob`, `callUnifiedAIModel`, `downloadFromStorage`, `getExtensionFromMimeType`, `randomUUID`, `fileManager`, `deleteFromStorage`, `notificationService`, `executeModelCallAndSave`, `ragService`, `countTokens`, `getAiProviderConfig`, `getGranularityPlanner`, `planComplexStage`, `indexingService`, `embeddingClient`, `promptAssembler`, `getAiProviderAdapter`, `tokenWalletService`, `documentRenderer`. Match the exact types from `IDialecticJobDeps` at lines 1093-1144 in `dialectic.interface.ts`.
        *   `[ ]` 46.b.iii. Define `export interface JobContextParams` with all fields matching `JobContext` fields. This type represents the parameters accepted by the factory function.
        *   `[ ]` 46.b.iv. Add JSDoc comments: `/** Immutable execution context for dialectic-worker operations. Constructed once at application boundary and passed through the execution chain. Enables adding new utilities without replumbing the entire call tree. */` above `JobContext`, and `/** Parameters for constructing JobContext. Each field maps to the corresponding JobContext field. */` above `JobContextParams`.
    *   `[ ]` 46.c. `[TEST-UNIT]` **RED**: In a new file `supabase/functions/dialectic-worker/createJobContext.test.ts`, write failing unit test for the factory function:
        *   `[ ]` 46.c.i. Import `assertEquals` from Deno testing, and import `JobContext`, `JobContextParams` from `./types/JobContext.interface.ts`, and import `createJobContext` from `./createJobContext.ts` (which doesn't exist yet).
        *   `[ ]` 46.c.ii. Create a test case `"createJobContext constructs fully-typed context with all fields from params"` that: (1) constructs mock `JobContextParams` with all required fields (use minimal valid mocks for each function/service), (2) calls `const ctx = createJobContext(params)`, (3) asserts that `ctx.logger === params.logger`, `ctx.fileManager === params.fileManager`, and similarly for all ~24 fields in JobContext, proving that the factory correctly maps all params to context fields, (4) asserts that the return type satisfies `JobContext` (TypeScript compiler validates this).
        *   `[ ]` 46.c.iii. This test must initially FAIL with "Module not found" or similar because `createJobContext.ts` does not exist yet, proving the RED state.
    *   `[ ]` 46.d. `[BE]` **GREEN**: In a new file `supabase/functions/dialectic-worker/createJobContext.ts`, implement the factory function:
        *   `[ ]` 46.d.i. Import `JobContext` and `JobContextParams` from `./types/JobContext.interface.ts`.
        *   `[ ]` 46.d.ii. Implement `export function createJobContext(params: JobContextParams): JobContext` that returns an object literal with all JobContext fields assigned from params: `return { logger: params.logger, getSeedPromptForStage: params.getSeedPromptForStage, continueJob: params.continueJob, retryJob: params.retryJob, callUnifiedAIModel: params.callUnifiedAIModel, downloadFromStorage: params.downloadFromStorage, getExtensionFromMimeType: params.getExtensionFromMimeType, randomUUID: params.randomUUID, fileManager: params.fileManager, deleteFromStorage: params.deleteFromStorage, notificationService: params.notificationService, executeModelCallAndSave: params.executeModelCallAndSave, ragService: params.ragService, countTokens: params.countTokens, getAiProviderConfig: params.getAiProviderConfig, getGranularityPlanner: params.getGranularityPlanner, planComplexStage: params.planComplexStage, indexingService: params.indexingService, embeddingClient: params.embeddingClient, promptAssembler: params.promptAssembler, getAiProviderAdapter: params.getAiProviderAdapter, tokenWalletService: params.tokenWalletService, documentRenderer: params.documentRenderer };`. Ensure the return type is explicitly `JobContext` (TypeScript will enforce all fields are present).
        *   `[ ]` 46.d.iii. Add JSDoc comment: `/** Factory function to construct JobContext at application boundary. All fields are required and must be explicitly provided. */`.
    *   `[ ]` 46.e. `[TEST-UNIT]` **GREEN**: Re-run the test from step 46.c and ensure it passes, proving the factory correctly constructs JobContext with all fields mapped from params.
    *   `[ ]` 46.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/types/JobContext.interface.ts`, `supabase/functions/dialectic-worker/createJobContext.ts`, and `supabase/functions/dialectic-worker/createJobContext.test.ts`, resolving any warnings or errors.
    *   `[ ]` 46.g. `[CRITERIA]` All requirements are met: (1) `JobContext` interface is defined with all fields from `IDialecticJobDeps` using explicit types, (2) `JobContextParams` interface defines factory parameters, (3) `createJobContext` factory function constructs context instances with all fields explicitly set from params, (4) Unit test proves factory correctly maps all params to context fields, (5) All files are lint-clean, (6) Types and factory are ready for use at application boundary.

*   `[ ]` 47. **`[BE]` Update Application Boundary to Construct JobContext**
    *   `[ ]` 47.a. `[DEPS]` The JobContext factory is now available (from step 46). We need to update the application boundary in `supabase/functions/dialectic-worker/index.ts` to use `createJobContext` instead of directly constructing `IDialecticJobDeps`. The `createDialecticWorkerDeps` function (lines 61-146) currently constructs and returns `deps: IDialecticJobDeps` as an object literal (lines 102-144). This step will replace that object literal with a call to `createJobContext(params)`, passing all currently constructed values as params. After this change, the application boundary will construct JobContext once and pass it to `processJob`, which will pass it down the chain. This is a one-time migration that changes how deps are constructed but preserves all existing behavior (1:1 mapping).
    *   `[ ]` 47.b. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/index.ts`, refactor to use JobContext factory:
        *   `[ ]` 47.b.i. Import `createJobContext` from `./createJobContext.ts` and `JobContext` from `./types/JobContext.interface.ts` at the top of the file.
        *   `[ ]` 47.b.ii. Update the return type of `createDialecticWorkerDeps` function signature (line 61) from `Promise<IDialecticJobDeps>` to `Promise<JobContext>`.
        *   `[ ]` 47.b.iii. At lines 102-144 (where `const deps: IDialecticJobDeps = { logger, getSeedPromptForStage, ... }` is constructed), replace the entire object literal construction with a return statement calling the factory: `return createJobContext({ logger, getSeedPromptForStage, continueJob, retryJob, callUnifiedAIModel, downloadFromStorage: (bucket: string, path: string) => downloadFromStorage(adminClient, bucket, path), getExtensionFromMimeType, randomUUID: crypto.randomUUID.bind(crypto), fileManager: fileManager, deleteFromStorage: (bucket: string, paths: string[]) => deleteFromStorage(adminClient, bucket, paths), notificationService, executeModelCallAndSave: (params: ExecuteModelCallAndSaveParams) => executeModelCallAndSave({ ...params, compressionStrategy: getSortedCompressionCandidates }), ragService, countTokens: countTokens, getAiProviderConfig: async (dbClient: SupabaseClient<Database>, modelId: string) => { const { data, error } = await dbClient.from('ai_providers').select('*').eq('id', modelId).single(); if (error || !data) { throw new Error('Failed to fetch AI provider config'); } if (!isAiModelExtendedConfig(data.config)) { throw new Error('Failed to fetch AI provider config'); } return data.config; }, getGranularityPlanner, planComplexStage, indexingService, embeddingClient, promptAssembler, getAiProviderAdapter, tokenWalletService, documentRenderer });`. Remove the `const deps: IDialecticJobDeps = { ... };` and `return deps;` lines (lines 102-146).
        *   `[ ]` 47.b.iv. In the HTTP handler `serve(async (req: Request) => { ... })`, locate where deps is constructed (around line 170: `const deps = await createDialecticWorkerDeps(adminClient);`). Rename the variable from `deps` to `ctx` for clarity: `const ctx = await createDialecticWorkerDeps(adminClient);`.
        *   `[ ]` 47.b.v. Update the `processJob` call (around line 195) to pass `ctx` instead of `deps`: `await processJob(adminClient, typedJob, projectOwnerUserId, processors, ctx, authToken);`.
    *   `[ ]` 47.c. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/index.ts` and resolve any warnings or errors.
    *   `[ ]` 47.d. `[CRITERIA]` All requirements are met: (1) Application boundary uses `createJobContext` factory to construct context, (2) Return type changed from `IDialecticJobDeps` to `JobContext`, (3) All fields are passed to factory matching previous object literal exactly (1:1 mapping), (4) HTTP handler passes `ctx` to `processJob`, (5) File is lint-clean, (6) Ready for `processJob` migration in next step.

*   `[ ]` 48. **`[BE]` Update processJob to Accept JobContext**
    *   `[ ]` 48.a. `[DEPS]` The application boundary now constructs and passes `JobContext` (from step 47). We need to update `processJob` in `supabase/functions/dialectic-worker/processJob.ts` to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`. The `processJob` function orchestrates dispatching to the appropriate processor (`processSimpleJob`, `processComplexJob`, `processRenderJob`) based on job type. It currently receives `deps: IDialecticJobDeps` and passes it to processors. This step changes the parameter type and updates all references within the function to use `ctx` instead of `deps`, then passes `ctx` to the processors (which will be updated in subsequent steps).
    *   `[ ]` 48.b. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/processJob.ts`, update to use JobContext:
        *   `[ ]` 48.b.i. Import `JobContext` from `./types/JobContext.interface.ts` at the top of the file.
        *   `[ ]` 48.b.ii. Update the `processJob` function signature (around line 20-30) to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`: `export async function processJob(dbClient: SupabaseClient<Database>, job: DialecticJobRow, projectOwnerUserId: string, processors: IJobProcessors, ctx: JobContext, authToken: string): Promise<void>`.
        *   `[ ]` 48.b.iii. Find all references to `deps` within the function body and replace with `ctx`. This includes logging calls like `deps.logger.info(...)` ‚Üí `ctx.logger.info(...)` and any other uses of deps.
        *   `[ ]` 48.b.iv. Update all calls to processors to pass `ctx` instead of `deps`: (1) `await processors.processSimpleJob(dbClient, typedJob, projectOwnerUserId, ctx, authToken);`, (2) `await processors.processComplexJob(dbClient, typedJob, projectOwnerUserId, ctx, authToken);`, (3) `await processors.processRenderJob(dbClient, job, projectOwnerUserId, ctx, authToken);`.
    *   `[ ]` 48.c. `[TEST-UNIT]` **GREEN**: In `supabase/functions/dialectic-worker/processJob.test.ts`, update test cases to use JobContext:
        *   `[ ]` 48.c.i. Import `createJobContext` from `./createJobContext.ts` and `JobContext` type from `./types/JobContext.interface.ts`.
        *   `[ ]` 48.c.ii. Locate all test cases that construct mock `deps` objects. Replace those constructions with calls to `createJobContext(mockParams)` where `mockParams` contains the same mocked fields.
        *   `[ ]` 48.c.iii. Update all `processJob(...)` function calls to pass context as the fifth parameter (matching the new signature).
        *   `[ ]` 48.c.iv. Re-run all tests in `processJob.test.ts` and ensure they pass, proving the JobContext migration is behaviorally equivalent.
    *   `[ ]` 48.d. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processJob.ts` and `processJob.test.ts`, resolving any warnings or errors.
    *   `[ ]` 48.e. `[CRITERIA]` All requirements are met: (1) `processJob` function signature accepts `ctx: JobContext`, (2) All references to `deps` within function body updated to `ctx`, (3) All processor calls pass `ctx`, (4) All tests updated to construct and pass JobContext, (5) All tests pass, (6) Files are lint-clean, (7) Ready for processor migration in next steps.

*   `[ ]` 49. **`[BE]` Update processSimpleJob to Accept JobContext**
    *   `[ ]` 49.a. `[DEPS]` The `processJob` function now passes `JobContext` to processors (from step 48). We need to update `processSimpleJob` in `supabase/functions/dialectic-worker/processSimpleJob.ts` to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`. The `processSimpleJob` function executes EXECUTE jobs by calling `executeModelCallAndSave` and handling its results. It currently receives `deps: IDialecticJobDeps` and passes it to `executeModelCallAndSave` via `params.deps`. This step changes the parameter type, updates all references to use `ctx`, and passes `ctx` to `executeModelCallAndSave` (which will be updated in step 50).
    *   `[ ]` 49.b. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/processSimpleJob.ts`, update to use JobContext:
        *   `[ ]` 49.b.i. Import `JobContext` from `./types/JobContext.interface.ts` at the top of the file.
        *   `[ ]` 49.b.ii. Update the `processSimpleJob` function signature (line 22) to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`: `export async function processSimpleJob(dbClient: SupabaseClient<Database>, job: Job & { payload: DialecticJobPayload }, projectOwnerUserId: string, ctx: JobContext, authToken: string)`.
        *   `[ ]` 49.b.iii. Find all references to `deps` within the function body and replace with `ctx`. This includes `deps.logger`, `deps.notificationService`, `deps.promptAssembler`, `deps.executeModelCallAndSave`, etc.
        *   `[ ]` 49.b.iv. Locate the call to `executeModelCallAndSave` (around line 200+) where it's invoked via `deps.executeModelCallAndSave({ dbClient, deps, authToken, ... })` or similar. Update it to pass `ctx`: `const result = await ctx.executeModelCallAndSave({ dbClient, deps: ctx, authToken, job: typedJob, projectOwnerUserId, providerDetails, sessionData, promptConstructionPayload, compressionStrategy: getSortedCompressionCandidates, inputsRelevance });`. Note: The `params.deps` field will be updated to expect `JobContext` in step 50.
    *   `[ ]` 49.c. `[TEST-UNIT]` **GREEN**: In `supabase/functions/dialectic-worker/processSimpleJob.test.ts`, update test cases to use JobContext:
        *   `[ ]` 49.c.i. Import `createJobContext` and `JobContext` type.
        *   `[ ]` 49.c.ii. Replace all mock `deps` constructions with `createJobContext(mockParams)`.
        *   `[ ]` 49.c.iii. Update all `processSimpleJob(...)` calls to pass context as the fourth parameter.
        *   `[ ]` 49.c.iv. Re-run tests and ensure they pass.
    *   `[ ]` 49.d. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processSimpleJob.ts` and `processSimpleJob.test.ts`, resolving any warnings or errors.
    *   `[ ]` 49.e. `[CRITERIA]` All requirements are met: (1) `processSimpleJob` accepts `ctx: JobContext`, (2) All references updated to `ctx`, (3) Calls to `executeModelCallAndSave` pass `ctx`, (4) Tests updated and passing, (5) Files are lint-clean.

*   `[ ]` 50. **`[BE]` Update executeModelCallAndSave and ExecuteModelCallAndSaveParams to Use JobContext**
    *   `[ ]` 50.a. `[DEPS]` The `processSimpleJob` function now passes `JobContext` (from step 49). We need to update `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and its parameter type `ExecuteModelCallAndSaveParams` (defined in `supabase/functions/dialectic-service/dialectic.interface.ts`) to expect `deps: JobContext` instead of `deps: IDialecticJobDeps`. The `executeModelCallAndSave` function is the core worker function where AI model calls are executed, responses are saved, and jobs are orchestrated. It receives a params object containing `deps: IDialecticJobDeps` and uses `deps.logger`, `deps.fileManager`, `deps.ragService`, `deps.tokenWalletService`, etc. throughout. This step changes: (1) the `ExecuteModelCallAndSaveParams` interface to expect `deps: JobContext`, (2) all internal references remain unchanged (since JobContext mirrors IDialecticJobDeps), (3) all test mocks to use JobContext.
    *   `[ ]` 50.b. `[TYPES]` In `supabase/functions/dialectic-service/dialectic.interface.ts`, update the `ExecuteModelCallAndSaveParams` type:
        *   `[ ]` 50.b.i. Import `JobContext` from `../dialectic-worker/types/JobContext.interface.ts` at the top of the file.
        *   `[ ]` 50.b.ii. Locate the `ExecuteModelCallAndSaveParams` interface definition (around line 450-480). Update the `deps` field type from `deps: IDialecticJobDeps` to `deps: JobContext`.
    *   `[ ]` 50.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, verify usage (no code changes needed):
        *   `[ ]` 50.c.i. Import `JobContext` from `./types/JobContext.interface.ts` (if not already imported via the params type).
        *   `[ ]` 50.c.ii. Review the function body. All usages of `params.deps.logger`, `params.deps.fileManager`, etc. should continue to work without changes because `JobContext` has all the same fields as `IDialecticJobDeps`. No code changes are needed in the function body‚Äîonly the type changed.
        *   `[ ]` 50.c.iii. If `deps` is destructured at the top of the function (e.g., `const { deps } = params; const { logger, fileManager } = deps;`), verify it still works with the new type (it will, since the fields are identical).
    *   `[ ]` 50.d. `[TEST-UNIT]` **GREEN**: In ALL `executeModelCallAndSave` test files (14 files: `executeModelCallAndSave.test.ts`, `executeModelCallAndSave.render.test.ts`, `executeModelCallAndSave.renderErrors.test.ts`, `executeModelCallAndSave.continue.test.ts`, `executeModelCallAndSave.tokens.test.ts`, `executeModelCallAndSave.rag.test.ts`, `executeModelCallAndSave.rag2.test.ts`, `executeModelCallAndSave.jsonSanitizer.test.ts`, `executeModelCallAndSave.assembleDocument.test.ts`, `executeModelCallAndSave.continuationCount.test.ts`, `executeModelCallAndSave.pathContext.test.ts`, `executeModelCallAndSave.planValidation.test.ts`, `executeModelCallAndSave.gatherArtifacts.test.ts`, `executeModelCallAndSave.rawJsonOnly.test.ts`), update mock deps to use JobContext:
        *   `[ ]` 50.d.i. In the main test file `executeModelCallAndSave.test.ts`, locate the `getMockDeps()` helper function (or similar) that constructs mock deps. Import `createJobContext` and `JobContext` type. Update `getMockDeps` to return `JobContext` instead of `IDialecticJobDeps`. Use `createJobContext(mockParams)` or construct inline if the mock has specialized fields.
        *   `[ ]` 50.d.ii. In each of the 14 test files, update any local mock deps constructions to use `JobContext` type. If they import `getMockDeps` from the main test file, verify it now returns `JobContext`.
        *   `[ ]` 50.d.iii. Re-run ALL tests across all 14 test files (estimated 80+ tests total) and ensure they all pass, proving the JobContext migration preserves all existing behavior.
    *   `[ ]` 50.e. `[LINT]` Run the linter for `supabase/functions/dialectic-service/dialectic.interface.ts`, `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, and all 14 test files, resolving any warnings or errors.
    *   `[ ]` 50.f. `[CRITERIA]` All requirements are met: (1) `ExecuteModelCallAndSaveParams.deps` type changed to `JobContext`, (2) `executeModelCallAndSave` function body uses JobContext (no code changes needed, type-only), (3) All 14 test files updated to use JobContext, (4) All 80+ tests pass, proving behavior preservation, (5) All files are lint-clean.

*   `[ ]` 51. **`[BE]` Update processComplexJob, planComplexStage, and processRenderJob to Use JobContext**
    *   `[ ]` 51.a. `[DEPS]` The core execution path (processJob ‚Üí processSimpleJob ‚Üí executeModelCallAndSave) now uses JobContext (from steps 48-50). We need to complete the migration by updating the remaining processor functions: `processComplexJob` (in `supabase/functions/dialectic-worker/processComplexJob.ts`), `planComplexStage` (in `supabase/functions/dialectic-worker/task_isolator.ts`), and `processRenderJob` (in `supabase/functions/dialectic-worker/processRenderJob.ts`). These functions handle PLAN and RENDER job types and currently accept `deps: IDialecticJobDeps`. This step updates all three functions and their tests to use JobContext, completing the full migration across the entire dialectic-worker family.
    *   `[ ]` 51.b. `[BE]` **GREEN**: Update all three processor files to use JobContext:
        *   `[ ]` 51.b.i. In `supabase/functions/dialectic-worker/processComplexJob.ts`: (1) Import `JobContext` from `./types/JobContext.interface.ts`, (2) Update the `processComplexJob` function signature to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`, (3) Replace all `deps` references with `ctx`, (4) Update calls to `planComplexStage` to pass `ctx` instead of `deps`.
        *   `[ ]` 51.b.ii. In `supabase/functions/dialectic-worker/task_isolator.ts` (contains `planComplexStage`): (1) Import `JobContext` from `./types/JobContext.interface.ts`, (2) Update the `planComplexStage` function signature to accept `ctx: JobContext` instead of `deps: IDialecticJobDeps`, (3) Replace all `deps` references with `ctx`, (4) Update calls to granularity planner strategies to pass context fields as needed (e.g., `ctx.getGranularityPlanner`).
        *   `[ ]` 51.b.iii. In `supabase/functions/dialectic-worker/processRenderJob.ts`: (1) Import `JobContext` from `./types/JobContext.interface.ts`, (2) Update the function signature to accept `ctx: JobContext` instead of `deps: IRenderJobDeps` (note: this function previously used a different deps type, but will now receive full JobContext and extract only needed fields), (3) At the top of the function, destructure needed fields from ctx: `const { documentRenderer, logger, downloadFromStorage, fileManager, notificationService } = ctx;`, (4) Update all references to use these destructured values.
    *   `[ ]` 51.c. `[TEST-UNIT]` **GREEN**: Update all test files for these processors:
        *   `[ ]` 51.c.i. In `processComplexJob.test.ts`, `processComplexJob.happy.test.ts`, `processComplexJob.errors.test.ts`, `processComplexJob.parallel.test.ts`: Import `createJobContext` and `JobContext`, update all mock deps constructions to use JobContext, update all function calls to pass `ctx`, re-run tests and verify they pass.
        *   `[ ]` 51.c.ii. In `task_isolator.test.ts`, `task_isolator.planComplexStage.test.ts`, `task_isolator.parallel.test.ts`: Update mock deps to JobContext, update function calls, re-run tests and verify they pass.
        *   `[ ]` 51.c.iii. In test files for `processRenderJob` (if any exist): Update to use JobContext and verify tests pass.
    *   `[ ]` 51.d. `[LINT]` Run the linter for `processComplexJob.ts`, `task_isolator.ts`, `processRenderJob.ts`, and all their test files, resolving any warnings or errors.
    *   `[ ]` 51.e. `[CRITERIA]` All requirements are met: (1) All processor functions (`processComplexJob`, `planComplexStage`, `processRenderJob`) accept `ctx: JobContext`, (2) All references to `deps` updated to `ctx`, (3) All tests updated to use JobContext, (4) All tests pass across entire dialectic-worker family, proving complete migration, (5) All files are lint-clean, (6) JobContext pattern is now fully implemented across the worker, (7) Future utility additions (like `shouldEnqueueRenderJob` for step 53) can be added by updating only: JobContext interface, createJobContext factory, and index.ts boundary construction‚Äî3 files instead of 15-30 files.
    *   `[ ]` 51.f. `[COMMIT]` `refactor(be): migrate dialectic-worker family to JobContext pattern for scalable dependency injection`

*   `[ ]` 52. **`[BE]` Update `executeModelCallAndSave` to Handle Structured Results from `shouldEnqueueRenderJob`**
    *   `[ ]` 52.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` calls `shouldEnqueueRenderJob` at line 1326 and expects a `boolean`. The logic at line 1327 `if (!shouldRender)` is based on this boolean assumption. This is now incorrect because `shouldEnqueueRenderJob` returns a `ShouldEnqueueRenderJobResult` object. The existing logic will incorrectly evaluate any returned object (even for errors) as truthy, causing `!shouldRender` to be `false`, which makes the code fall into the `else` block and attempt to enqueue a RENDER job in all cases. This will fail to distinguish between a legitimate non-render (`is_json`) and a critical failure (`stage_not_found`), causing transient errors to be silently ignored instead of retried.
    *   `[ ]` 52.b. `[TEST-UNIT]` **RED**: In a test file for `executeModelCallAndSave.ts`, add unit tests to prove the current implementation fails to correctly handle the structured `ShouldEnqueueRenderJobResult`.
        *   `[ ]` 52.b.i. Add a test "skips RENDER job when shouldEnqueueRenderJob returns { shouldRender: false, reason: 'is_json' }" that: (1) Mocks `shouldEnqueueRenderJob` to return `{ shouldRender: false, reason: 'is_json' }`, (2) Spies on the database insert call for new jobs, (3) Calls `executeModelCallAndSave`, (4) Asserts that the insert spy was **not** called and no error was thrown. This test must initially FAIL because the current logic `if (!shouldRender)` incorrectly evaluates `!{...}` as `false`, causing it to fall into the `else` block and attempt to enqueue a RENDER job.
        *   `[ ]` 52.b.ii. Add a test "throws an error when shouldEnqueueRenderJob returns an error reason like 'stage_not_found'" that: (1) Mocks `shouldEnqueueRenderJob` to return `{ shouldRender: false, reason: 'stage_not_found', details: 'DB error' }`, (2) Calls `executeModelCallAndSave`, (3) Asserts that the function **throws an error**. This test must initially FAIL because the current logic will not throw; it will incorrectly attempt to enqueue a RENDER job, silently ignoring the critical underlying error.
    *   `[ ]` 52.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, update the caller to handle structured results.
        *   `[ ]` 52.c.i. At line 1326, update the call to `shouldEnqueueRenderJob` to destructure the result: `const { shouldRender, reason, details } = await shouldEnqueueRenderJob({ dbClient, logger: deps.logger }, { outputType: output_type, stageSlug });`.
        *   `[ ]` 52.c.ii. Replace the conditional check `if (!shouldRender)` with logic that correctly handles the structured result: `if (shouldRender && reason === 'is_markdown')`.
        *   `[ ]` 52.c.iii. Add logging/error handling for error reasons. After the `shouldEnqueueRenderJob` call, add: `if (!shouldRender && ['stage_not_found', 'instance_not_found', 'steps_not_found', 'parse_error', 'query_error', 'no_active_recipe'].includes(reason)) { deps.logger.error('[executeModelCallAndSave] Failed to determine if RENDER job required due to query/config error', { reason, details, outputType: output_type, stageSlug }); throw new Error(\`Cannot determine render requirement: \${reason}\${details ? \` - \${details}\` : ''}\`); }`. This ensures transient failures (database errors, config issues) cause the EXECUTE job to fail rather than silently skipping rendering.
        *   `[ ]` 52.c.iv. Add logging for successful "is_json" reason to document why rendering was skipped: `if (!shouldRender && reason === 'is_json') { deps.logger.info('[executeModelCallAndSave] Skipping RENDER job for JSON output', { outputType: output_type }); }`.
        *   `[ ]` 52.c.v. Verify the updated caller correctly handles all reason codes: (1) `is_markdown` ‚Üí proceeds with RENDER job creation, (2) `is_json` ‚Üí logs and skips rendering (normal flow), (3) error reasons ‚Üí logs error and throws exception to fail the EXECUTE job.
    *   `[ ]` 52.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 52.b and ensure they now pass, proving the consumer logic correctly handles all cases from the structured result.
    *   `[ ]` 52.e. `[TEST-INT]` Prove the structured results work correctly with the updated caller in `executeModelCallAndSave`.
        *   `[ ]` 52.e.i. Assert that when `shouldEnqueueRenderJob` in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` (test subject) returns `{shouldRender: true, reason: 'is_markdown'}` for a markdown output, `executeModelCallAndSave` in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` (consumer) proceeds to create a RENDER job and logs the decision. Create an integration test that: (1) sets up database state with markdown recipe configuration, (2) calls `executeModelCallAndSave` with markdown `output_type`, (3) verifies `shouldEnqueueRenderJob` returns structured result with `reason: 'is_markdown'`, (4) verifies `executeModelCallAndSave` creates a RENDER job, proving the success path works with structured results.
        *   `[ ]` 52.e.ii. Assert that when `shouldEnqueueRenderJob` (test subject) returns `{shouldRender: false, reason: 'is_json'}` for a JSON output, `executeModelCallAndSave` (consumer) skips RENDER job creation and logs the decision without throwing an error. Create an integration test that: (1) sets up database state with JSON output configuration, (2) calls `executeModelCallAndSave` with JSON `output_type`, (3) verifies `shouldEnqueueRenderJob` returns `reason: 'is_json'`, (4) verifies no RENDER job is created, (5) verifies the EXECUTE job completes successfully (JSON outputs don't require rendering), proving legitimate false results are handled correctly.
        *   `[ ]` 52.e.iii. Assert that when `shouldEnqueueRenderJob` (test subject) returns `{shouldRender: false, reason: 'stage_not_found', details: ...}` due to database query failure, `executeModelCallAndSave` (consumer) throws an exception and the EXECUTE job is marked as 'failed'. Create an integration test that: (1) mocks the stage query to fail, (2) calls `executeModelCallAndSave`, (3) verifies `shouldEnqueueRenderJob` returns structured error with `reason: 'stage_not_found'`, (4) verifies `executeModelCallAndSave` throws an exception containing the reason and details, (5) verifies the error propagates to `processSimpleJob` and the job is marked 'failed', proving query errors are no longer silent and cause job failure for retry.
        *   `[ ]` 52.e.iv. Assert that when `shouldEnqueueRenderJob` (test subject) returns `{shouldRender: false, reason: 'no_active_recipe'}` due to missing recipe configuration, `executeModelCallAndSave` (consumer) throws an exception with diagnostic information. Create an integration test that: (1) sets up a stage with `active_recipe_instance_id = NULL`, (2) calls `executeModelCallAndSave`, (3) verifies the exception contains `reason: 'no_active_recipe'`, (4) verifies the error is logged with context for debugging, proving configuration errors are reported clearly.
    *   `[ ]` 52.f. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and its test file(s) and resolve any warnings or errors.
    *   `[ ]` 52.g. `[CRITERIA]` The `executeModelCallAndSave` function correctly handles all cases from `ShouldEnqueueRenderJobResult`. All unit and integration tests pass, and the file is lint-clean. The system no longer silently fails to render documents when a transient or configuration error occurs, and instead correctly fails the parent job for retry.
    *   `[ ]` 52.h. `[COMMIT]` `fix(be): update executeModelCallAndSave to handle structured results from shouldEnqueueRenderJob`