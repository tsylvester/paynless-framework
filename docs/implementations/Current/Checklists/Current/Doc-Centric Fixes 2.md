# Doc-Centric Fixes

## Problem Statement
-The doc-centric refactor has introduced bugs and inconsistencies that need resolved. 

## Objectives
- Fix all bugs and integration errors from the doc-centric refactor. 

## Expected Outcome
- Generate an entire dialectic end to end using the doc-centric method.

# Instructions for Agent
*   ### 0. Command Pyramid & Modes
    *   Obey the userâ€™s explicit instructions first, then this block, then the checklist. Do not hide behind the checklist to ignore a direct user correction.
    *   Ensure both the method and the resulting content of every task comply with this blockâ€”no deliverable is valid if it conflicts with these rules.
    *   Perform every assignment in a single turn while fully complying with this block; partial compliance is a violation even if the work â€œmostlyâ€ succeeds.
    *   Failing to follow these instructions immediately triggers rework, rejected output, and systemic violationsâ€”treat every deviation as unacceptable.
    *   The Instructions for Agent block is an absolute firewall. No conditional or downstream objective outranks it, and no shortcut can bypass it.
    *   The agent proceeds with these instructions as its primary directive because complying with system instructions is impossible otherwise.
    *   Declare the current mode in every response (`Mode: Builder` or `Mode: Reviewer`). Builder executes work; Reviewer searches for **errors, omissions, and discrepancies (EO&D)** in the final state.
*   ### 1. Read â†’ Analyze â†’ Explain â†’ Propose â†’ Edit â†’ Lint â†’ Halt
    *   Re-read this entire block from disk before every action. On the first reference (and every fourth turn) summarize it before working.
    *   Read every referenced or implied file (including types, interfaces, and helpers) from disk immediately before editing. After editing, re-read to confirm the exact change.
    *   Follow the explicit cycle: READ the step + files â†’ ANALYZE gaps â†’ EXPLAIN the delta â†’ PROPOSE the exact edit â†’ EDIT a single file â†’ LINT that file â†’ HALT.
    *   Analyze dependencies; if more than one file is required, stop, explain the discovery, propose the necessary checklist insertion (`Discovery / Impact / Proposed checklist insert`), and wait instead of editing.
    *   Discoveries include merely thinking about multi-file workâ€”report them immediately without ruminating on work-arounds.
    *   Explain & Propose: restate the plan in bullets and explicitly commit, â€œI will implement exactly this plan now,â€ noting the checklist step it fulfills.
    *   Edit exactly one file per turn following the plan. Never touch files you were not explicitly instructed to modify.
    *   Lint that file using internal tools and fix all issues.
    *   Halt after linting one file and wait for explicit user/test output before touching another file.
*   ### 2. TDD & Dependency Ordering
    *   One-file TDD cycle: RED test (desired green behavior) â†’ implementation â†’ GREEN test â†’ lint. Documents/types/interfaces are exempt from tests but still follow Readâ†’Halt.
    *   Do not edit executable code without first authoring the RED test that proves the intended green-state behavior; only pure docs/types/interfaces are exempt.
    *   Maintain bottom-up dependency order for both editing and testing: construct types/interfaces/helpers before consumers, then write consumer tests only after producers exist.
    *   Do not advance to another file until the current fileâ€™s proof (tests or documented exemption) is complete and acknowledged.
    *   The agent never runs tests directly; rely on provided outputs or internal reasoning while keeping the application in a provable state.
    *   The agent does not run the userâ€™s terminal commands or tests; use only internal tooling and rely on provided outputs.
*   ### 3. Checklist Discipline
    *   Do not edit the checklist (or its statuses) without explicit instruction; when instructed, change only the specified portion using legal-style numbering.
    *   Execute exactly what the active checklist step instructs with no deviation or â€œcreative interpretation.â€
    *   Each numbered checklist step equals one fileâ€™s entire TDD cycle (deps â†’ types â†’ tests â†’ implementation â†’ proof). Preserve existing detail while adding new requirements.
    *   Document every edit within the checklist. If required edits are missing from the plan, explain the discovery, propose the new step, and halt instead of improvising.
    *   Never update the status of any work step (checkboxes or badges) without explicit instruction.
    *   Following a block of related checklist steps that complete a working implementation, include a commit with a proposed commit message. 
*   ### 4. Builder vs Reviewer Modes
    *   **Builder:** follow the Readâ†’â€¦â†’Halt loop precisely. If a deviation, blocker, or new requirement is discoveredâ€”or the current step simply cannot be completed as writtenâ€”explain the problem, propose the required checklist change, and halt immediately.
    *   **Reviewer:** treat prior reasoning as untrusted. Re-read relevant files/tests from scratch and produce a numbered EO&D list referencing files/sections. Ignore checklist status or RED/GREEN history unless it causes a real defect. If no EO&D are found, state â€œNo EO&D detected; residual risks: â€¦â€
*   ### 5. Strict Typing & Object Construction
    *   Use explicit types everywhere. No `any`, `as`, `as const`, inline ad-hoc types, or castsâ€”except for Supabase clients and intentionally malformed objects in error-handling tests (use dedicated helpers and keep typing strict elsewhere). Every object and variable must be typed. 
    *   Always construct full objects that satisfy existing interfaces/tuples from the relevant type file. Compose complex objects from smaller typed components; never rely on defaults, fallbacks, or backfilling to â€œhealâ€ missing data.
    *   Use type guards to prove and narrow types for the compiler when required.
    *   Never import entire libraries with *, never alias imports, never add "type" to type imports. 
    *   A ternary is not a type guard, a ternary is a default value. Default values are prohibited. 
*   ### 6. Plan Fidelity & Shortcut Ban
    *   Once a solution is described, implement exactly that solution and the userâ€™s instruction. Expedient shortcuts are forbidden without explicit approval.
    *   If you realize you deviated, stop, report it, and wait for direction. Repeating corrected violations triggers halt-and-wait immediately.
    *   If your solution to a challenge is "rewrite the entire file", you have made an error. Stop, do not rewrite the file. Explain the problem to the user and await instruction. 
    *   Do not ruminate on how to work around the "only write to one file per turn". If you are even thinking about the need to work around that limit, you have made a discovery. Stop immediately, report the discovery to the user, and await instruction. 
    *   Refactors must preserve all existing functionality unless the user explicitly authorizes removals; log and identifier fidelity is mandatory.
*   ### 7. Dependency Injection & Architecture
    *   Use explicit dependency injection everywhereâ€”pass every dependency with no hidden defaults or optional fallbacks.
    *   Build adapters/interfaces for every function and work bottom-up so dependencies compile before consumers. Preserve existing functionality, identifiers, and logging unless explicitly told otherwise.
    *   When a file exceeds 600 lines, stop and propose a logical refactoring to decompose the file into smaller parts providing clear SOC and DRY. 
*   ### 8. Testing Standards
    *   Tests assert the desired passing state (no RED/GREEN labels) and new tests are added to the end of the file. Each test covers exactly one behavior.
    *   Use real application functions/mocks, strict typing, and Deno std asserts. Tests must call out which production type/helper each mock mirrors so partial objects are not invented.
    *   Integration tests must exercise real code paths; unit tests stay isolated and mock dependencies explicitly. Never change assertions to match broken codeâ€”fix the code instead.
    *   Tests use the same types, objects, structures, and helpers as the real code, never create new fixtures only for tests - a test that relies on imaginary types or fixtures is invalid. 
    *   Prove the functional gap, the implemented fix, and regressions through tests before moving on; never assume success without proof.
*   ### 9. Logging, Defaults, and Error Handling
    *   Do not add or remove logging, defaults, fallbacks, or silent healing unless the user explicitly instructs you to do so.
    *   Adding console logs solely for troubleshooting is exempt from TDD and checklist obligations, but the exemption applies only to the logging statements themselves.
    *   Believe failing tests, linter flags, and user-reported errors literally; fix the stated condition before chasing deeper causes.
    *   If the user flags instruction noncompliance, acknowledge, halt, and wait for explicit directionâ€”do not self-remediate in a way that risks further violations.
*   ### 10. Linting & Proof
    *   After each edit, lint the touched file and resolve every warning/error. Record lint/test evidence in the response (e.g., â€œLint: clean via internal tool; Tests: not run per instructionsâ€).
    *   Evaluate if a linter error can be resolved in-file, or out-of-file. Only resolve in-file linter errors, then report the out-of-file errors and await instruction. 
    *   Testing may produce unresolvable linter errors. Do not silence them with @es flags, create an empty target function, or other work-arounds. The linter error is sometimes itself proof of the RED state of the test. 
    *   Completion proof requires a lint-clean file plus GREEN test evidence (or documented exemption for types/docs).
*   ### 11. Reporting & Traceability
    *   Every response must include: mode declaration, confirmation that this block was re-read, plan bullets (Builder) or EO&D findings (Reviewer), checklist step references, and lint/test evidence.
    *   If tests were not run (per instruction), explicitly state why and list residual risks. If no EO&D are found, state that along with remaining risks.
    *   The agent uses only its own tools and never the userâ€™s terminal.
*   ### 12. Output Constraints
    *   Never output large code blocks (entire files or multi-function dumps) in chat unless the user explicitly requests them.
    *   Never print an entire function and tell the user to paste it in; edit the file directly or provide the minimal diff required.

## Checklist-Specific Editing Rules

*   THE AGENT NEVER TOUCHES THE CHECKLIST UNLESS THEY ARE EXPLICITLY INSTRUCTED TO! 
*   When editing checklists, each numbered step (1, 2, 3, etc.) represents editing ONE FILE with a complete TDD cycle.
*   Sub-steps within each numbered step use legal-style numbering (1.a, 1.b, 1.a.i, 1.a.ii, etc.) for the complete TDD cycle for that file.
*   All changes to a single file are described and performed within that file's numbered step.
*   Types files (interfaces, enums) are exempt from RED/GREEN testing requirements.
*   Each file edit includes: RED test â†’ implementation â†’ GREEN test â†’ optional refactor.
*   Steps are ordered by dependency (lowest dependencies first).
*   Preserve all existing detail and work while adding new requirements.
*   Use proper legal-style nesting for sub-steps within each file edit.
*   NEVER create multiple top-level steps for the same file edit operation.
*   Adding console logs is not required to be detailed in checklist work. 

### Example Checklist

*   `[ ]`   1. **Title** Objective
    *   `[ ]`   1.a. [DEPS] A list explaining dependencies of the function, its signature, and its return shape
        *   `[ ]` 1.a.i. eg. `function(something)` in `file.ts` provides this or that
    *   `[ ]`   1.b. [TYPES] A list strictly typing all the objects used in the function
    *   `[ ]`   1.c. [TEST-UNIT] A list explaining the test cases
        *   `[ ]` 1.c.i. Assert `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [SPACE] A list explaining the implementation requirements
        *   `[ ]` 1.d.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [TEST-UNIT] Rerun and expand test proving the function
        *   `[ ]` 1.d.i. Implement `function(something)` in `file.ts` acts a certain way 
    *   `[ ]`   1.d. [TEST-INT] If there is a chain of functions that work together, prove it
        *   `[ ]` 1.d.i. For every cross-function interaction, assert `thisFunction(something)` in `this_file.ts` acts a certain way towards `thatFunction(other)` in `that_file.ts`
    *   `[ ]`   1.d. [CRITERIA] A list explaining the acceptence criteria to consider the work complete and correct. 
    *   `[ ]`   1.e. [COMMIT] A commit that explains the function and its proofs

*   `[ ]`   2. **Title** Objective
    *   `[ ]`   2.a. [DEPS] Low level providers are always build before high level consumers (DI/DIP)
    *   `[ ]`   2.b. [TYPES] DI/DIP and strict typing ensures unit tests can always run 
    *   `[ ]`   2.c. [TEST-UNIT] All functions matching defined external objects and acting as asserted helps ensure integration tests pass

## Legend - You must use this EXACT format. Do not modify it, adapt it, or "improve" it. The bullets, square braces, ticks, nesting, and numbering are ABSOLUTELY MANDATORY and UNALTERABLE. 

*   `[ ]` 1. Unstarted work step. Each work step will be uniquely named for easy reference. We begin with 1.
    *   `[ ]` 1.a. Work steps will be nested as shown. Substeps use characters, as is typical with legal documents.
        *   `[ ]` 1. a. i. Nesting can be as deep as logically required, using roman numerals, according to standard legal document numbering processes.
*   `[âœ…]` Represents a completed step or nested set.
*   `[ðŸš§]` Represents an incomplete or partially completed step or nested set.
*   `[â¸ï¸]` Represents a paused step where a discovery has been made that requires backtracking or further clarification.
*   `[â“]` Represents an uncertainty that must be resolved before continuing.
*   `[ðŸš«]` Represents a blocked, halted, or stopped step or has an unresolved problem or prior dependency to resolve before continuing.

## Component Types and Labels

*   `[DB]` Database Schema Change (Migration)
*   `[RLS]` Row-Level Security Policy
*   `[BE]` Backend Logic (Edge Function / RLS / Helpers / Seed Data)
*   `[API]` API Client Library (`@paynless/api` - includes interface definition in `interface.ts`, implementation in `adapter.ts`, and mocks in `mocks.ts`)
*   `[STORE]` State Management (`@paynless/store` - includes interface definition, actions, reducers/slices, selectors, and mocks)
*   `[UI]` Frontend Component (e.g., in `apps/web`, following component structure rules)
*   `[CLI]` Command Line Interface component/feature
*   `[IDE]` IDE Plugin component/feature
*   `[TEST-UNIT]` Unit Test Implementation/Update
*   `[TEST-INT]` Integration Test Implementation/Update (API-Backend, Store-Component, RLS)
*   `[TEST-E2E]` End-to-End Test Implementation/Update
*   `[DOCS]` Documentation Update (READMEs, API docs, user guides)
*   `[REFACTOR]` Code Refactoring Step
*   `[PROMPT]` System Prompt Engineering/Management
*   `[CONFIG]` Configuration changes (e.g., environment variables, service configurations)
*   `[COMMIT]` Checkpoint for Git Commit (aligns with "feat:", "test:", "fix:", "docs:", "refactor:" conventions)
*   `[DEPLOY]` Checkpoint for Deployment consideration after a major phase or feature set is complete and tested.

# Work Breakdown Structure

*   `[âœ…]` 1. **`[DB]` Fix Job Queue Continuation When First Step Completes**
    *   `[âœ…]` 1.a. `[DEPS]` After reviewing the recipe stage migration files (`*_stage.sql`) and the codebase, the complete list of job statuses that require worker invocation is: `pending` (new jobs, handled by `on_new_job_created` trigger on INSERT), `pending_next_step` (PLAN jobs ready for next recipe step after children complete, MISSING trigger), `pending_continuation` (continuation jobs created by `continueJob` function, MISSING trigger), and `retrying` (jobs being retried, handled by `on_job_retrying` trigger). Statuses that do NOT need worker invocation are: `processing` (worker already owns the job), `waiting_for_children` (internal state set by `processComplexJob`), `waiting_for_prerequisite` (internal state set by prerequisite logic), and terminal states (`completed`, `failed`, `retry_loop_failed`). The database trigger `handle_job_completion()` in `supabase/migrations/20251119160820_retrying_trigger.sql` correctly sets PLAN job status to `pending_next_step` when all child EXECUTE jobs complete (lines 234-236), and also sets jobs to `pending` when prerequisites complete (line 181). The `continueJob` function in `supabase/functions/dialectic-worker/continueJob.ts` sets jobs to `pending_continuation` (line 198). However, there is no trigger configured to invoke the `dialectic-worker` Edge Function when a job's status changes to `pending_next_step` or `pending_continuation`. The worker endpoint (`supabase/functions/dialectic-worker/index.ts`) is invoked via POST request with a job record in the request body (it does not fetch jobs itself), so it must be triggered by a database webhook when the status changes. Currently, there is a generic `invoke_dialectic_worker()` function (defined in `supabase/migrations/20250922165259_document_centric_generation.sql`, lines 59-143) that handles HTTP invocation logic, and it is used by the `on_new_job_created` trigger (fires on INSERT). There is also a separate `handle_job_retrying()` function and `on_job_retrying` trigger (lines 5-137 in `20251119160820_retrying_trigger.sql`) that specifically handle `retrying` status. Rather than creating separate triggers for each status, a better architectural approach is to create a single, generic trigger that fires on status changes and invokes the worker for any status that requires processing. The `processComplexJob` function in `supabase/functions/dialectic-worker/processComplexJob.ts` already handles `pending_next_step` status correctly (it processes ready steps when called), but the worker is never invoked when the status changes to `pending_next_step`, leaving PLAN jobs stuck in that state.
    *   `[âœ…]` 1.b. `[DOCS]` **TRIGGER AUDIT**: Document all current triggers on `dialectic_generation_jobs` table and identify competing/deprecated triggers. Current active triggers: (1) `on_new_job_created` (from `20250722033928_fix_job_trigger.sql`) - fires `AFTER INSERT`, calls `invoke_dialectic_worker()`, handles new jobs; (2) `on_job_retrying` (from `20251119160820_retrying_trigger.sql`) - fires `AFTER UPDATE OF status` when `status = 'retrying'`, calls `handle_job_retrying()`, handles retry logic and retry limit checking before invoking worker; (3) `trigger_handle_job_completion_on_update` (from `20250725182218_prerequisite_job_id.sql`) - fires `AFTER UPDATE OF status`, calls `handle_job_completion()`, handles parent/child dependencies and prerequisite logic; (4) `trigger_handle_job_completion_on_insert` (from `20250725182218_prerequisite_job_id.sql`) - fires `AFTER INSERT`, calls `handle_job_completion()`, handles parent/child dependencies on insert. Potentially deprecated: `on_job_terminal_state` (from `20250711205050_create_job_completion_trigger.sql`, line 150 drops it, replaced by `trigger_handle_job_completion_on_update`). The new generic trigger will replace `on_job_retrying` and may overlap with `on_new_job_created` for UPDATE operations (though `on_new_job_created` only fires on INSERT, so no conflict). Document that `handle_job_retrying()` contains special retry limit checking logic (checks if `attempt_count >= (max_retries + 1)` and marks job as `retry_loop_failed`) that must be preserved in the generic trigger or moved to a separate function.
    *   `[âœ…]` 1.c. `[TEST-INT]` **RED**: In `supabase/integration_tests/triggers/invoke_worker_on_status_change.trigger.test.ts`, write integration tests that prove the generic trigger preserves all existing `handle_job_retrying()` logic and provides new logic for missing state transitions.
        *   `[âœ…]` 1.c.i. Create a test file following the pattern of `handle_job_completion.trigger.test.ts` that sets up test database state (admin client, test user, project, session, stage).
        *   `[âœ…]` 1.c.ii. Write a test case that proves `pending_next_step` status DOES invoke worker (status updates to `pending_next_step` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no trigger for `pending_next_step`.
        *   `[âœ…]` 1.c.iii. Write a test case that proves `pending_continuation` status DOES invoke worker (status updates to `pending_continuation` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no trigger for `pending_continuation`.
        *   `[âœ…]` 1.c.iv. Write a test case that proves `pending` status set via UPDATE (when prerequisites complete) DOES invoke worker (status updates from `waiting_for_prerequisite` to `pending` and creates log entry in `dialectic_trigger_logs` indicating HTTP invocation attempt). This test must fail because there is currently no UPDATE trigger for `pending` status.
        *   `[âœ…]` 1.c.v. Write a test case that proves `retrying` status with `attempt_count >= (max_retries + 1)` marks job as `retry_loop_failed` and does NOT invoke worker. This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.vi. Write a test case that proves `retrying` status with `attempt_count < (max_retries + 1)` invokes worker (creates log entry in `dialectic_trigger_logs`). This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.vii. Write a test case that proves test jobs (`is_test_job = true`) with `retrying` status do NOT invoke worker (creates log entry indicating test job skip, but no HTTP invocation attempt). This test should pass initially (existing `on_job_retrying` trigger handles this), but must continue to pass after the generic trigger replaces it.
        *   `[âœ…]` 1.c.viii. Write a test case that proves status changes that do NOT require worker invocation (`processing`, `waiting_for_children`, `waiting_for_prerequisite`, `completed`, `failed`, `retry_loop_failed`) do NOT invoke worker. This test should pass initially and must continue to pass after the generic trigger is implemented.
        *   `[âœ…]` 1.c.ix. Write a test case that proves status updates where `OLD.status = NEW.status` do NOT invoke worker (no status change). This test must pass after the generic trigger is implemented.
    *   `[âœ…]` 1.d. `[DB]` **GREEN**: In `supabase/migrations/20251119160820_retrying_trigger.sql`, create a generic trigger that invokes the worker when job status changes to any status that requires processing.
        *   `[âœ…]` 1.d.i. Create or update a generic trigger function `invoke_worker_on_status_change()` that: (1) checks if the new status is one that requires worker invocation (`pending`, `pending_next_step`, `pending_continuation`, `retrying`), (2) only processes when status actually changes (not on every update: `OLD.status IS NULL OR OLD.status != NEW.status`), (3) skips test jobs (`COALESCE(NEW.is_test_job, false)`), (4) handles special retry limit checking for `retrying` status (if `attempt_count >= (max_retries + 1)`, mark job as `retry_loop_failed` and return early), (5) reuses the existing `invoke_dialectic_worker()` function's logic for determining worker URL (from vault secrets or local dev URL), extracting `user_jwt` from payload, building request body with job record, and invoking via `net.http_post()` using `pg_net` extension, (6) logs the invocation attempt to `dialectic_trigger_logs` table. Alternatively, update the existing `invoke_dialectic_worker()` function to accept both INSERT and UPDATE operations, and add status-checking logic to filter statuses that require invocation.
        *   `[âœ…]` 1.d.ii. Create a new trigger `on_job_status_change` that: (1) fires `AFTER UPDATE OF status`, (2) executes for each row where `NEW.status IN ('pending', 'pending_next_step', 'pending_continuation', 'retrying') AND (OLD.status IS NULL OR OLD.status != NEW.status)`, (3) calls the generic trigger function. This replaces the need for separate triggers for each status.
        *   `[âœ…]` 1.d.iii. **DEPRECATE**: Drop the `on_job_retrying` trigger and deprecate the `handle_job_retrying()` function (or keep it as a helper if retry limit checking is extracted). Add comments explaining that this generic trigger invokes the worker for any status change that requires processing, including PLAN jobs reaching `pending_next_step` status after all child jobs complete (set by `handle_job_completion` trigger), continuation jobs reaching `pending_continuation` status (set by `continueJob` function), jobs being retried (set by `retryJob` function), and jobs set to `pending` when prerequisites complete (set by `handle_job_completion` trigger). Note that `on_new_job_created` trigger remains active for handling INSERT operations, as it fires on a different event type and does not conflict with the UPDATE-based generic trigger.
    *   `[âœ…]` 1.e. `[TEST-INT]` **GREEN**: Re-run all tests from step 1.c and ensure they now pass. The tests from 1.c.ii, 1.c.iii, and 1.c.iv should now pass because the generic trigger handles these status transitions. The tests from 1.c.v, 1.c.vi, and 1.c.vii should continue to pass, proving that all existing `handle_job_retrying()` logic is preserved.

*   `[âœ…]` 2. **`[STORE]` Fix StageTabCard to Use Valid Document Artifacts Only for Completion Check**
    *   `[âœ…]` 2.a. `[DEPS]` The `selectStageProgressSummary` selector in `packages/store/src/dialecticStore.selectors.ts` (lines 661-733) counts ALL entries in `progress.documents`, including non-document artifacts like `header_context` (which has `artifact_class: 'header_context'` and `file_type: 'json'`). This causes `StageTabCard` to display incorrect counts like "Completed 1/1 documents" when only a `header_context` artifact exists, while `StageRunChecklist` correctly shows "0 of 4 documents" because it uses `selectValidMarkdownDocumentKeys` (lines 1014-1045) which filters out non-markdown artifacts. The `selectValidMarkdownDocumentKeys` selector uses `extractMarkdownDocumentKeysFromRule` (lines 933-1000) to identify valid markdown document keys from recipe steps' `outputs_required` fields, filtering based on `file_type === 'markdown'` or markdown template filenames, and excludes `header_context` artifacts. `StageTabCard` only needs `isComplete` to determine if the SubmitResponses button should be active (it doesn't need to display counts - that's handled by `StageRunChecklist`). The fix requires: (1) modifying `selectStageProgressSummary` to filter document keys using `selectValidMarkdownDocumentKeys` before counting, (2) removing the document count display from `StageTabCard` (lines 116-136 in `apps/web/src/components/dialectic/StageTabCard.tsx`), keeping only the `isComplete` check (line 121) for button activation.
    *   `[âœ…]` 2.b. `[TEST-UNIT]` **RED**: In `packages/store/src/dialecticStore.selectors.test.ts`, write a new test for `selectStageProgressSummary` that verifies it excludes non-document artifacts from counts.
        *   `[âœ…]` 2.b.i. Create a test case that sets up `stageRunProgress` state with both valid markdown document keys (e.g., `'draft_document_markdown'`) and non-document artifacts (e.g., `'HeaderContext'` with `artifact_class: 'header_context'`).
        *   `[âœ…]` 2.b.ii. Mock `selectValidMarkdownDocumentKeys` to return a Set containing only the valid markdown document keys (excluding `header_context`).
        *   `[âœ…]` 2.b.iii. Call `selectStageProgressSummary` with the test state and assert that `totalDocuments` and `completedDocuments` only count valid markdown documents, not `header_context` artifacts.
        *   `[âœ…]` 2.b.iv. Assert that `isComplete` is `false` when only `header_context` is completed but markdown documents are not, and `true` only when all valid markdown documents are completed. This test must fail because `selectStageProgressSummary` currently counts all entries in `progress.documents`.
    *   `[âœ…]` 2.c. `[STORE]` **GREEN**: In `packages/store/src/dialecticStore.selectors.ts`, modify `selectStageProgressSummary` (lines 661-733) to filter document keys using `selectValidMarkdownDocumentKeys` before counting.
        *   `[âœ…]` 2.c.i. Add `selectValidMarkdownDocumentKeys` as a dependency parameter to the selector (or use it within the selector function). The selector function signature is `(state: DialecticStateValues, sessionId: string, stageSlug: string, iterationNumber: number, modelId?: string)`, so we need to pass `stageSlug` to `selectValidMarkdownDocumentKeys(state, stageSlug)`.
        *   `[âœ…]` 2.c.ii. After getting `documentEntries` from `progress.documents` (line 689) and filtering by `modelId` (lines 691-695), filter the resulting `documentKeys` array to only include keys that exist in the Set returned by `selectValidMarkdownDocumentKeys(state, stageSlug)`.
        *   `[âœ…]` 2.c.iii. Use the filtered `documentKeys` array for all subsequent counting logic (lines 697-716) instead of the unfiltered array.
        *   `[âœ…]` 2.c.iv. Ensure the return type and values remain unchanged (the filtering only affects which documents are counted).
    *   `[âœ…]` 2.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 2.b and ensure it now passes. Also verify that existing tests for `selectStageProgressSummary` still pass.
    *   `[âœ…]` 2.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/StageTabCard.test.tsx`, write a test that verifies `StageTabCard` does not display document counts.
        *   `[âœ…]` 2.e.i. Render `StageTabCard` with mock store state that includes stage progress with valid documents.
        *   `[âœ…]` 2.e.ii. Assert that the document count display (the element with `data-testid={`stage-progress-count-${stage.slug}`}`) is NOT rendered (or does not contain count text).
        *   `[âœ…]` 2.e.iii. Assert that `isComplete` check (the element with `data-testid={`stage-progress-label-${stage.slug}`}`) is still rendered when `isComplete` is `true`. This test must fail because `StageTabCard` currently displays counts.
    *   `[âœ…]` 2.f. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/StageTabCard.tsx`, remove the document count display while keeping the completion check for button activation.
        *   `[âœ…]` 2.f.i. Remove the document count display div (lines 116-136) that shows `${progress.completedDocuments} / ${progress.totalDocuments} documents` (line 133).
        *   `[âœ…]` 2.f.ii. Keep the `isComplete` check (lines 121-128) that displays "Completed" label when `progress.isComplete` is `true`, as this is needed for visual feedback and may be used for the SubmitResponses button activation logic.
        *   `[âœ…]` 2.f.iii. Update the `hasDocuments` check (line 80) if needed - it currently checks `progress.totalDocuments > 0`, which should still work correctly after the selector fix (it will now reflect valid documents only). Alternatively, check `progress.isComplete` directly if that's the only requirement.
        *   `[âœ…]` 2.f.iv. Update any TypeScript types or interfaces if the `StageProgressSnapshotSummary` interface (lines 19-23) needs modification (it may not need changes if the selector still returns the same structure, just with filtered counts).
    *   `[âœ…]` 2.g. `[TEST-UNIT]` **GREEN**: Re-run the test from step 2.e and ensure it now passes. Also verify that existing `StageTabCard` tests still pass.
    *   `[âœ…]` 2.h. `[LINT]` Run the linter for all modified files (`packages/store/src/dialecticStore.selectors.ts`, `apps/web/src/components/dialectic/StageTabCard.tsx`) and resolve any warnings or errors.

*   `[âœ…]` 3. **`[UI]` Remove Document Progress Display from SessionContributionsDisplayCard**
    *   `[âœ…]` 3.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` is a container component whose responsibility is to contain child components that display document information, not to display document progress itself. Currently, it displays progress summary "Completed X of Y documents" (lines 474-484) using `selectStageProgressSummary`, which is incorrect because: (1) Displaying document progress is not the container's job - that responsibility belongs to child components like `StageRunChecklist` which already correctly displays progress counts; (2) The progress summary display includes non-document artifacts like `header_context` (which will be fixed by item 2's changes to `selectStageProgressSummary`, but the container shouldn't be displaying progress at all); (3) The container should only use `stageProgressSummary?.isComplete` (line 231) for button activation logic (`canSubmitStageResponses`), not for displaying progress counts. The container correctly uses `isComplete` for button activation (line 390) and `hasFailed` for error display (lines 282-293, 516-540), which are valid container responsibilities (managing UI state and error handling). The fix requires: (1) removing the progress summary display (lines 474-484) that shows "Completed X of Y documents", (2) keeping the `isComplete` check (line 231) for button activation logic, (3) keeping the `hasFailed` check (lines 282-293) for error display if needed, or verifying if error display should also be delegated to child components.
    *   `[âœ…]` 3.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx` (or create if it doesn't exist), write a test that verifies the component does not display document progress summary.
        *   `[âœ…]` 3.b.i. Render `SessionContributionsDisplayCard` with mock store state that includes `stageProgressSummary` with `completedDocuments`, `totalDocuments`, and `outstandingDocuments` properties.
        *   `[âœ…]` 3.b.ii. Assert that the progress summary display (the element containing "Completed X of Y documents" text, or the div at lines 474-484) is NOT rendered (or does not contain progress count text).
        *   `[âœ…]` 3.b.iii. Assert that the `canSubmitStageResponses` logic still works correctly (the submit button is enabled when `isComplete` is `true` and disabled when `isComplete` is `false`). This test must fail because `SessionContributionsDisplayCard` currently displays progress counts.
    *   `[âœ…]` 3.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, remove the document progress summary display while keeping the completion check for button activation.
        *   `[âœ…]` 3.c.i. Remove the progress summary display div (lines 474-484) that shows "Completed {stageProgressSummary.completedDocuments} of {stageProgressSummary.totalDocuments} documents" and "Outstanding: ..." text.
        *   `[âœ…]` 3.c.ii. Keep the `stageProgressSummary` selector usage (lines 218-229) because it's still needed for `isComplete` check (line 231) and `hasFailed` check (lines 282-283) for button activation and error handling logic.
        *   `[âœ…]` 3.c.iii. Ensure the `canSubmitStageResponses` logic (line 231) still works correctly - it uses `stageProgressSummary?.isComplete === true` which is valid container responsibility (button activation logic).
        *   `[âœ…]` 3.c.iv. Verify that error display logic (lines 282-293, 516-540) using `stageProgressSummary?.hasFailed` and `failedDocumentKeys` still works if needed, or consider if error display should also be delegated to child components. For now, keep it since error handling is a valid container responsibility.
    *   `[âœ…]` 3.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 3.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify button activation logic using `isComplete`).
    *   `[âœ…]` 3.e. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 4. **`[UI]` Fix GeneratedContributionCard to Exclude Non-Document Artifacts from Status Display**
    *   `[âœ…]` 4.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` correctly filters out non-document artifacts when rendering document content (lines 136-141 use `selectValidMarkdownDocumentKeys` to check `isValidMarkdownDocument`, and lines 345-468 only render document content when `focusedDocument && isValidMarkdownDocument` is true). However, it still displays the status badge for ALL focused documents, including non-document artifacts like `header_context` (lines 330-334). The `documentDescriptor` is retrieved from `stageRunProgress.documents?.[focusedDocument.documentKey]` (lines 193-196), which includes ALL entries in `progress.documents`, including `header_context` artifacts. The status badge should only be shown for valid markdown documents. Additionally, `StageRunChecklist` (used at line 339) should already filter out non-markdown documents after item 3's fix to `selectStageDocumentChecklist`, but we need to ensure `GeneratedContributionCard` doesn't show status for non-document artifacts even if they somehow become focused. The fix requires: (1) modifying the status badge display (lines 330-334) to only show when `isValidMarkdownDocument` is true, ensuring non-document artifacts like `header_context` don't show completion status even if they become focused.
    *   `[âœ…]` 4.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write a test that verifies the component does not display status badge for non-document artifacts.
        *   `[âœ…]` 4.b.i. Render `GeneratedContributionCard` with mock store state where `focusedDocument` has a `documentKey` that is NOT in `validMarkdownDocumentKeys` (e.g., `'HeaderContext'` with `artifact_class: 'header_context'`).
        *   `[âœ…]` 4.b.ii. Mock `stageRunProgress` to include a `header_context` entry with status `'completed'` in `documents` map.
        *   `[âœ…]` 4.b.iii. Assert that the status badge (the element with `Badge` variant="secondary" containing status text, or the element at lines 330-334) is NOT rendered when `isValidMarkdownDocument` is `false`. This test must fail because `GeneratedContributionCard` currently shows status badge for all focused documents regardless of whether they are valid markdown documents.
    *   `[âœ…]` 4.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/GeneratedContributionCard.tsx`, modify the status badge display to only show for valid markdown documents.
        *   `[âœ…]` 4.c.i. Update the status badge condition (line 330) from `{documentDescriptor && (` to `{documentDescriptor && isValidMarkdownDocument && (` to ensure the badge is only shown when the focused document is a valid markdown document.
        *   `[âœ…]` 4.c.ii. Ensure the `isValidMarkdownDocument` check (lines 136-141) still works correctly - it uses `selectValidMarkdownDocumentKeys(state, stageSlug)` (lines 129-134) to filter out non-markdown artifacts like `header_context`.
        *   `[âœ…]` 4.c.iii. Verify that document content rendering logic (lines 345-468) already correctly filters using `isValidMarkdownDocument` - no changes needed here as it already has the correct check.
        *   `[âœ…]` 4.c.iv. Ensure that `StageRunChecklist` component (line 339) will correctly filter non-markdown documents after item 3's fix to `selectStageDocumentChecklist`, but this fix ensures `GeneratedContributionCard` doesn't show status even if a non-document artifact somehow becomes focused.
    *   `[âœ…]` 4.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 4.b and ensure it now passes. Also verify that existing `GeneratedContributionCard` tests still pass (especially tests that verify status badge displays correctly for valid markdown documents).
    *   `[âœ…]` 4.e. `[LINT]` Run the linter for `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 5. **`[UI]` Change Submit Responses Button to Detect Last Stage and Disable Itself with a "Project Complete" Notice**
    *   `[âœ…]` 5.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` uses a Submit Responses button (`renderSubmitButton()` at lines 387-401) that enables when `canSubmitStageResponses` is `true` (based on `stageProgressSummary?.isComplete` at line 231). The button currently always allows submission and advancement to the next stage when enabled, but should be disabled with a "Project Complete" notice when the user is in the last stage of the dialectic process. The component already has access to `sortedStages` via `selectSortedStages` (line 110) and `processTemplate` via the store (lines 107-109). The `activeStage` is computed from `processTemplate` (lines 117-119). To detect the last stage, we can check if `activeStage.slug` matches the last stage in `sortedStages` array (similar to how `SessionInfoCard` detects `isFinalStageInProcess` at lines 83-87, which checks if there are no transitions where the current stage is the source). Alternatively, we can check if `activeStage.slug === sortedStages[sortedStages.length - 1]?.slug`. The fix requires: (1) adding a `useMemo` hook to compute `isLastStage` based on `sortedStages` and `activeStage`, (2) modifying the button to be disabled when `isLastStage` is `true`, (3) displaying a "Project Complete" notice when `isLastStage` is `true` and `canSubmitStageResponses` is `true` (indicating all documents are complete), (4) ensuring the button text or notice clearly indicates that the project is complete and no further stages are available.
    *   `[âœ…]` 5.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the Submit Responses button is disabled when in the last stage.
        *   `[âœ…]` 5.b.i. Create a test case that mocks store state with `sortedStages` containing multiple stages (e.g., `['thesis', 'antithesis', 'synthesis']`) and sets `activeStageSlug` to the last stage (`'synthesis'`).
        *   `[âœ…]` 5.b.ii. Mock `stageProgressSummary` to have `isComplete: true` (so `canSubmitStageResponses` would normally be `true`).
        *   `[âœ…]` 5.b.iii. Render `SessionContributionsDisplayCard` and assert that the Submit Responses button (the element with text "Submit Responses & Advance Stage" or `data-testid` if available) is disabled when in the last stage, even if `canSubmitStageResponses` is `true`.
        *   `[âœ…]` 5.b.iv. Assert that a "Project Complete" notice is displayed when in the last stage and all documents are complete. This test must fail because the button currently doesn't check for last stage.
    *   `[âœ…]` 5.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, add last stage detection and modify the button to be disabled with a notice when in the last stage.
        *   `[âœ…]` 5.c.i. Add a `useMemo` hook (after line 231) to compute `isLastStage` by checking if `activeStage?.slug === sortedStages[sortedStages.length - 1]?.slug`. Handle the case where `sortedStages` is empty or `activeStage` is null (return `false`).
        *   `[âœ…]` 5.c.ii. Modify `renderSubmitButton()` (lines 387-401) to disable the button when `isLastStage` is `true` by adding `|| isLastStage` to the `disabled` condition (line 390), or create a new computed value `isButtonDisabled = isSubmitting || !canSubmitStageResponses || isLastStage`.
        *   `[âœ…]` 5.c.iii. Add a conditional render after `renderSubmitButton()` (or modify the button section) that displays a "Project Complete" notice when `isLastStage && canSubmitStageResponses` is `true`. The notice should be a non-interactive element (e.g., a `Badge`, `Alert`, or styled `div`) with text like "Project Complete - All stages finished" or similar, styled appropriately to indicate completion.
        *   `[âœ…]` 5.c.iv. Ensure the button text remains "Submit Responses & Advance Stage" (line 398) - the button should just be disabled in the last stage, not change its text. The notice should be separate from the button.
        *   `[âœ…]` 5.c.v. Handle edge cases: if `sortedStages` is empty, `isLastStage` should be `false`. If `activeStage` is null, `isLastStage` should be `false`. If there's only one stage, it should be considered the last stage.
    *   `[âœ…]` 5.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 5.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify button enabling/disabling based on `canSubmitStageResponses`).
    *   `[âœ…]` 5.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the button still works correctly for non-last stages.
        *   `[âœ…]` 5.e.i. Create a test case that mocks store state with `sortedStages` containing multiple stages and sets `activeStageSlug` to a non-last stage (e.g., `'thesis'` when stages are `['thesis', 'antithesis', 'synthesis']`).
        *   `[âœ…]` 5.e.ii. Mock `stageProgressSummary` to have `isComplete: true` and assert that the Submit Responses button is enabled (not disabled) when not in the last stage.
        *   `[âœ…]` 5.e.iii. Assert that the "Project Complete" notice is NOT displayed when not in the last stage. This test should pass immediately after step 5.c if implemented correctly.
    *   `[âœ…]` 5.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 5.e and ensure it passes. This verifies that the fix doesn't break normal button behavior for non-last stages.
    *   `[âœ…]` 5.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 6. **`[UI]` Remove Conditional "Export Final" Button from SessionInfoCard**
    *   `[âœ…]` 6.a. `[DEPS]` The `SessionInfoCard` component in `apps/web/src/components/dialectic/SessionInfoCard.tsx` displays a conditional "Export Final" button (lines 287-297) that appears when `isFinalStageInProcess` is `true`. However, there is now an always-visible "Export" button (lines 247-257) that provides the same functionality, making the conditional "Export Final" button redundant and deprecated. The `isFinalStageInProcess` logic (lines 83-87) is computed based on process template transitions and is currently used in two places: (1) to hide the Submit button in the final stage (line 261: `!isFinalStageInProcess`), and (2) to show the conditional "Export Final" button (line 288: `isFinalStageInProcess && project &&`). The fix requires: (1) removing the conditional "Export Final" button (lines 287-297), (2) keeping the `isFinalStageInProcess` logic since it's still needed to hide the Submit button in the final stage (line 261), (3) removing the comment "Final stage export button" (line 287) as it's no longer relevant. The always-visible "Export" button (lines 247-257) provides the export functionality for all stages, eliminating the need for the conditional button.
    *   `[âœ…]` 6.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionInfoCard.test.tsx`, write a test that verifies the "Export Final" button is never displayed.
        *   `[âœ…]` 6.b.i. Create a test case that mocks store state with `isFinalStageInProcess` set to `true` (by setting up a project with process template transitions where the current stage has no outgoing transitions).
        *   `[âœ…]` 6.b.ii. Mock `project` to be non-null and render `SessionInfoCard`.
        *   `[âœ…]` 6.b.iii. Assert that the "Export Final" button (the element with text "Export Final" or `data-testid` if available) is NOT rendered, even when `isFinalStageInProcess` is `true`.
        *   `[âœ…]` 6.b.iv. Assert that the always-visible "Export" button (lines 247-257) is still rendered. This test must fail because the conditional "Export Final" button currently displays when `isFinalStageInProcess` is `true`.
    *   `[âœ…]` 6.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionInfoCard.tsx`, remove the conditional "Export Final" button while keeping the Submit button hiding logic.
        *   `[âœ…]` 6.c.i. Remove the conditional "Export Final" button block (lines 287-297), including the comment "Final stage export button" (line 287), the conditional check `{isFinalStageInProcess && project && (`, the `ExportProjectButton` component with "Export Final" text, and the closing `)}`.
        *   `[âœ…]` 6.c.ii. Keep the `isFinalStageInProcess` computation (lines 83-87) since it's still needed to hide the Submit button in the final stage (line 261: `!isFinalStageInProcess`). Do not remove this logic.
        *   `[âœ…]` 6.c.iii. Verify that the always-visible "Export" button (lines 247-257) remains unchanged and continues to provide export functionality for all stages.
        *   `[âœ…]` 6.c.iv. Ensure the Submit button hiding logic (lines 259-285) still works correctly - it should hide the Submit button when `isFinalStageInProcess` is `true`, which is the correct behavior since users shouldn't submit to advance when already in the final stage.
    *   `[âœ…]` 6.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 6.b and ensure it now passes. Also verify that existing `SessionInfoCard` tests still pass (especially tests that verify the always-visible "Export" button and the Submit button hiding logic).
    *   `[âœ…]` 6.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionInfoCard.test.tsx`, write a test that verifies the always-visible "Export" button is still rendered in all stages.
        *   `[âœ…]` 6.e.i. Create test cases for both final and non-final stages (by setting up projects with different process template transition configurations).
        *   `[âœ…]` 6.e.ii. Assert that the always-visible "Export" button (lines 247-257) is rendered regardless of whether `isFinalStageInProcess` is `true` or `false`. This test should pass immediately after step 6.c if the always-visible button was not modified.
    *   `[âœ…]` 6.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 6.e and ensure it passes. This verifies that the always-visible "Export" button continues to work correctly for all stages.
    *   `[âœ…]` 6.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionInfoCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 7. **`[UI]` Remove Conditional "Export Final" Button from SessionInfoCard**
    *   `[âœ…]` 7.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` displays a "Generating documents" loader (lines 510-524) when `isGenerating` is `true`. The `isGenerating` logic (lines 308-311) checks `contributionGenerationStatus === 'generating'`, which is a global state that can be `'generating'` even when: (1) no models are selected for the current session (`selectedModelIds` might be empty), (2) the Generate button hasn't been clicked (the status might persist from a previous session or stage), (3) a different session is generating (the global status applies to all sessions). The component already computes rich document state via `documentsByModel` (lines 169-216) which uses `selectStageDocumentChecklist` and includes document statuses from `stageRunProgress`, and `stageProgressSummary` (lines 218-229) which summarizes document states. Documents in `documentsByModel` have `status` fields that can be `'generating'`, `'completed'`, `'failed'`, or `'not_started'` (as defined in `packages/types/src/dialectic.types.ts`). The component already uses this pattern for `failedDocumentKeys` (lines 294-306) which filters documents by `status === 'failed'`. The fix requires: (1) replacing the global `contributionGenerationStatus === 'generating'` check in `isGenerating` (line 309) with a check that examines document statuses from `documentGroups` (derived from `documentsByModel`) to see if any documents have `status === 'generating'`, following the same pattern as `failedDocumentKeys`, (2) ensuring the check only looks at documents for the current session/stage/iteration (which is already scoped by `documentsByModel`), (3) ensuring the loader display logic (lines 510-524) correctly reflects the session-specific generation state. This approach is more consistent with existing component patterns and uses data already computed rather than requiring a new hook or selector.
    *   `[âœ…]` 7.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the loader does not display when no documents in the current session are generating.
        *   `[âœ…]` 7.b.i. Create a test case that mocks store state where `contributionGenerationStatus` is `'generating'` but all documents in `stageRunProgress` for the current session/stage/iteration have status `'completed'` or `'not_started'` (simulating a different session generating or generation completed).
        *   `[âœ…]` 7.b.ii. Mock store state where `contributionGenerationStatus` is `'generating'` but `selectedModelIds` is empty and no documents exist in `stageRunProgress` for the current session.
        *   `[âœ…]` 7.b.iii. Render `SessionContributionsDisplayCard` with the test state and assert that the "Generating documents" loader (the element with text "Generating documents" or `data-testid` if available) is NOT displayed when no documents have `status === 'generating'` in the current session's `documentsByModel`.
        *   `[âœ…]` 7.b.iv. Assert that the loader is NOT displayed when `contributionGenerationStatus` is `'generating'` but documents are not in generating state. This test must fail because the component currently uses the global `contributionGenerationStatus` which can be `'generating'` even when the current session's documents aren't generating.
    *   `[âœ…]` 7.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, replace the global generation status check with a document status check using existing computed data.
        *   `[âœ…]` 7.c.i. Add a new `useMemo` hook (after line 306, after `failedDocumentKeys` definition) to compute `hasGeneratingDocuments` by checking if any documents in `documentGroups` have `status === 'generating'`. The logic should follow the same pattern as `failedDocumentKeys`: `const hasGeneratingDocuments = useMemo(() => { return documentGroups.some(([, documents]) => documents.some((document) => document.status === 'generating')); }, [documentGroups]);`. This uses the same data source (`documentGroups`) and pattern as the existing `failedDocumentKeys` check.
        *   `[âœ…]` 7.c.ii. Modify `isGenerating` (lines 308-311) to check `hasGeneratingDocuments` instead of `contributionGenerationStatus === 'generating'`. The new logic should be: `const isGenerating = hasGeneratingDocuments && failedDocumentKeys.length === 0 && !generationError;`. This ensures the loader only shows when documents in the current session/stage/iteration are actually generating, using data already computed and scoped correctly.
        *   `[âœ…]` 7.c.iii. Remove the `contributionGenerationStatus` selector usage (lines 141-143) since it's no longer needed - the component will derive generation state from document statuses instead of the global status. Also remove `selectContributionGenerationStatus` from imports if it's not used elsewhere.
        *   `[âœ…]` 7.c.iv. Ensure the loader display logic (lines 510-524) correctly uses the updated `isGenerating` value - no changes needed here as it already uses `isGenerating`.
    *   `[âœ…]` 7.d. `[TEST-UNIT]` **GREEN**: Re-run the test from step 7.b and ensure it now passes. Also verify that existing `SessionContributionsDisplayCard` tests still pass (especially tests that verify the loader displays correctly when generation is active).
    *   `[âœ…]` 7.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write a test that verifies the loader displays correctly when documents in the current session are generating.
        *   `[âœ…]` 7.e.i. Create a test case that mocks store state where `stageRunProgress` for the current session/stage/iteration contains documents with `status: 'generating'` (e.g., set `draft_document_outline` document descriptor status to `'generating'`).
        *   `[âœ…]` 7.e.ii. Ensure `failedDocumentKeys` will be empty (no documents with `status === 'failed'`) and `generationError` is null.
        *   `[âœ…]` 7.e.iii. Render `SessionContributionsDisplayCard` and assert that the "Generating documents" loader IS displayed when documents in the current session have `status === 'generating'`. This test should pass immediately after step 7.c if implemented correctly.
    *   `[âœ…]` 7.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 7.e and ensure it passes. This verifies that the loader still works correctly when documents are actually generating in the current session.
    *   `[âœ…]` 7.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and resolve any warnings or errors.

*   `[âœ…]` 8. **`[BE]` Fix Recipe Step Output Type to Use Model Contribution File Type**
    *   `[âœ…]` 8.a. `[DEPS]` Recipe steps in database migrations have `output_type: 'rendered_document'` (the final product), but planners expect `output_type` to be a `ModelContributionFileType` (what the model produces). The actual document key (e.g., `business_case`, `feature_spec`, `technical_approach`, `success_metrics`) is found in `outputs_required.documents[0].document_key`. The `getStageRecipe.ts` function in `supabase/functions/dialectic-service/getStageRecipe.ts` (lines 89-99) validates `output_type` against only 3 values (`HeaderContext`, `AssembledDocumentJson`, `RenderedDocument`), which is architecturally incorrect: `HeaderContext` and `AssembledDocumentJson` are backend-only and should never be sent to the frontend. The `OutputType` type in `supabase/functions/dialectic-service/dialectic.interface.ts` (lines 627-631) incorrectly includes these backend-only types. All planners (`planPerSourceDocument`, `planAllToOne`, `planPerSourceGroup`, `planPairwiseByOrigin`, `planPerSourceDocumentByLineage`, `planPerModel`) validate that `recipeStep.output_type` must be a `ModelContributionFileType` (using `isModelContributionFileType` check), and they use it directly to set `jobPayload.output_type` for EXECUTE jobs. The `shouldEnqueueRenderJob` function in `supabase/functions/_shared/utils/shouldEnqueueRenderJob.ts` checks if `outputType` matches any `document_key` in `outputs_required` that has markdown `file_type`, so `output_type` should match `document_key` for proper rendering logic. Every `document_key` is a `FileType`, and `output_type` should match the `document_key` to create semantic alignment. The fix requires: (1) defining `OutputType` as a subset of `ModelContributionFileTypes` that become rendered documents (excludes `HeaderContext`, `AssembledDocumentJson`, and other backend-only types), (2) updating all stage migration files (`20251006194531_thesis_stage.sql`, `20251006194542_antithesis_stage.sql`, `20251006194558_parenthesis_stage.sql`, `20251006194549_synthesis_stage.sql`, `20251006194605_paralysis_stage.sql`) to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'` for all EXECUTE job steps, (3) updating `getStageRecipe.ts` to validate that `output_type` is a `ModelContributionFileType` and is in the `OutputType` subset (renderable types), and pass it through as-is to the DTO (no mapping needed since `output_type` will match `document_key`), (4) filtering out PLAN job steps with `header_context` output from the DTO sent to frontend (or handling them separately) since they are backend-only.
    *   `[âœ…]` 8.b. `[TYPES]` In `supabase/functions/dialectic-service/dialectic.interface.ts`, define `OutputType` as a subset of `ModelContributionFileTypes` that become rendered documents. This excludes backend-only types like `HeaderContext`, `AssembledDocumentJson`, `ModelContributionRawJson`, `PairwiseSynthesisChunk`, `ReducedSynthesis`, `Synthesis`, `header_context_pairwise`, `SynthesisHeaderContext`, and other intermediate types that don't become user-facing rendered documents. The `OutputType` should include all `ModelContributionFileTypes` that have markdown outputs in `outputs_required` (e.g., `business_case`, `feature_spec`, `technical_approach`, `success_metrics`, `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `non_functional_requirements`, `dependency_map`, `comparison_vector`, `product_requirements`, `system_architecture`, `tech_stack`, `technical_requirements`, `master_plan`, `milestone_schema`, `updated_master_plan`, `actionable_checklist`, `advisor_recommendations`). Import `ModelContributionFileTypes` from `../_shared/types/file_manager.types.ts` and create a manually curated union type `OutputType` that includes only the renderable `ModelContributionFileTypes`. This provides type safety and semantic clarity: `OutputType` represents "FileTypes that become RenderedDocument". Replace the current `OutputType` definition (lines 627-631) with this new subset type.
    *   `[âœ…]` 8.b.i. `[TYPES]` In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, create a type guard function `isOutputType(value: ModelContributionFileTypes): value is OutputType` that validates if a `ModelContributionFileType` is in the `OutputType` subset (renderable types). This function should check against a runtime map similar to `MODEL_CONTRIBUTION_FILE_TYPES_MAP`, but only include the renderable types. Import `OutputType` from `../../dialectic-service/dialectic.interface.ts` and create `OUTPUT_TYPES_MAP: { [K in OutputType]: true }` with all renderable types, then implement `isOutputType` to check if the value exists in this map.
    *   `[âœ…]` 8.c. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-service/getStageRecipe.test.ts` (or create if it doesn't exist), write tests that verify `getStageRecipe` accepts renderable `ModelContributionFileTypes` (those in `OutputType`) for EXECUTE job steps and filters out backend-only types.
        *   `[âœ…]` 8.c.i. Create a test case that mocks a recipe step with `output_type: 'business_case'` (a renderable `ModelContributionFileType` in `OutputType`) and `job_type: 'EXECUTE'`, and assert that `getStageRecipe` returns successfully with `output_type: 'business_case'` in the response DTO.
        *   `[âœ…]` 8.c.ii. Create a test case that mocks a recipe step with `output_type: 'feature_spec'` (a renderable `ModelContributionFileType` in `OutputType`) and `job_type: 'EXECUTE'`, and assert that `getStageRecipe` returns successfully with `output_type: 'feature_spec'` in the response DTO.
        *   `[âœ…]` 8.c.iii. Create a test case that mocks a recipe step with `output_type: 'header_context'` (a backend-only `ModelContributionFileType` NOT in `OutputType`) and `job_type: 'PLAN'`, and assert that `getStageRecipe` either: (a) excludes this step from the DTO response (filters it out), or (b) returns an error indicating backend-only types cannot be sent to frontend. PLAN jobs with `header_context` are backend-only and should not appear in the frontend DTO.
        *   `[âœ…]` 8.c.iv. Create a test case that mocks a recipe step with `output_type: 'rendered_document'` (which is NOT a `ModelContributionFileType`), and assert that `getStageRecipe` returns an error. The database should never have `output_type: 'rendered_document'` after migrations are updated.
        *   `[âœ…]` 8.c.v. These tests must fail initially because `getStageRecipe.ts` validation (lines 89-99) only accepts 3 values (`HeaderContext`, `AssembledDocumentJson`, `RenderedDocument`), not the renderable `ModelContributionFileTypes`.
    *   `[âœ…]` 8.d. `[BE]` **GREEN**: In `supabase/functions/dialectic-service/getStageRecipe.ts`, update the validation logic (lines 89-99) to validate that `output_type` is a `ModelContributionFileType` and is in the `OutputType` subset (renderable types), and filter out backend-only steps from the DTO.
        *   `[âœ…]` 8.d.i. Import `ModelContributionFileTypes`, `isModelContributionFileType`, and `OutputType` from `../_shared/types/file_manager.types.ts`, `../_shared/utils/type-guards/type_guards.file_manager.ts`, and `./dialectic.interface.ts` respectively (if not already imported).
        *   `[âœ…]` 8.d.ii. Replace the restrictive validation (lines 92-99) that only accepts 3 values with: (1) a check that uses `isModelContributionFileType(rawType)` to validate that `output_type` is a valid `ModelContributionFileType`, (2) a check that uses `isOutputType(rawType)` (from step 8.b.i) to verify `rawType` is in the `OutputType` subset (renderable types). If `output_type` is a `ModelContributionFileType` but not in `OutputType`, either filter out the step (for backend-only types like `header_context`) or return an error.
        *   `[âœ…]` 8.d.iii. Set `mappedOutputType` to `rawType as OutputType` after validating it's in the `OutputType` subset. The `output_type` passes through as-is (no mapping needed) since it will match `document_key` in `outputs_required`.
        *   `[âœ…]` 8.d.iv. For PLAN job steps with `output_type: 'header_context'` (or other backend-only types), either: (a) filter them out before adding to the `normalized` array (skip adding the DTO), or (b) return an error. Option (a) is preferred - backend-only steps should not be sent to the frontend.
        *   `[âœ…]` 8.d.v. Remove the error logging that references only the 3 specific values, and update it to log any invalid `output_type` that fails the `isModelContributionFileType` check or is not in the `OutputType` subset.
    *   `[âœ…]` 8.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 8.c and ensure they now pass. Verify that: (1) renderable `ModelContributionFileTypes` pass validation and appear in the DTO, (2) backend-only types like `header_context` are filtered out from the DTO, (3) invalid types return errors. Also verify that existing `getStageRecipe` tests still pass, but note that PLAN job steps with `header_context` should no longer appear in the DTO response (they are filtered out as backend-only).
    *   `[âœ…]` 8.f. `[DB]` In `supabase/migrations/20251006194531_thesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.g.i. For template step `'thesis_generate_business_case'` (line 323): change `output_type` from `'rendered_document'` (line 329) to `'business_case'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 336).
        *   `[âœ…]` 8.g.ii. For template step `'thesis_generate_feature_spec'` (line 485): change `output_type` from `'rendered_document'` (line 491) to `'feature_spec'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 498).
        *   `[âœ…]` 8.g.iii. For template step `'thesis_generate_technical_approach'` (line 643): change `output_type` from `'rendered_document'` (line 649) to `'technical_approach'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 656).
        *   `[âœ…]` 8.g.iv. For template step `'thesis_generate_success_metrics'` (line 800): change `output_type` from `'rendered_document'` (line 806) to `'success_metrics'` (which is the `document_key` in `outputs_required.documents[0].document_key` at line 813).
        *   `[âœ…]` 8.g.v. For instance step `'thesis_generate_business_case'` (line 1060): change `output_type` from `'rendered_document'` (line 1065) to `'business_case'`.
        *   `[âœ…]` 8.g.vi. For instance step `'thesis_generate_feature_spec'` (line 1132): change `output_type` from `'rendered_document'` (line 1137) to `'feature_spec'`.
        *   `[âœ…]` 8.g.vii. For instance step `'thesis_generate_technical_approach'` (line 1200): change `output_type` from `'rendered_document'` (line 1205) to `'technical_approach'`.
        *   `[âœ…]` 8.g.viii. For instance step `'thesis_generate_success_metrics'` (line 1267): change `output_type` from `'rendered_document'` (line 1272) to `'success_metrics'`.
        *   `[âœ…]` 8.g.ix. Verify that PLAN job steps (e.g., `'thesis_build_stage_header'`) keep their `output_type: 'header_context'` unchanged, as these are correct.
    *   `[âœ…]` 8.h. `[DB]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.h.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for antithesis stage include: `'business_case_critique'`, `'technical_feasibility_assessment'`, `'risk_register'`, `'non_functional_requirements'`, `'dependency_map'`, `'comparison_vector'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.i. `[DB]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.i.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for parenthesis stage include: `'technical_requirements'`, `'master_plan'`, `'milestone_schema'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.j. `[DB]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.j.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for synthesis stage include: `'product_requirements'`, `'system_architecture'`, `'tech_stack'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.k. `[DB]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, update all EXECUTE job recipe steps to set `output_type` to the actual `ModelContributionFileType` from `outputs_required.documents[0].document_key` instead of `'rendered_document'`.
        *   `[âœ…]` 8.k.i. For each EXECUTE job step in the migration file, identify the `document_key` in `outputs_required.documents[0].document_key` and update `output_type` from `'rendered_document'` to that `document_key` value. Common document keys for paralysis stage include: `'updated_master_plan'`, `'actionable_checklist'`, `'advisor_recommendations'`. Ensure both template steps and instance steps are updated.
    *   `[âœ…]` 8.l. `[TEST-INT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file, write a test that verifies planners can successfully create EXECUTE jobs from recipe steps with `output_type` set to actual `ModelContributionFileTypes` (e.g., `'business_case'`) when using real recipe steps from the updated database.
        *   `[âœ…]` 8.l.i. Create a test case that sets up a PLAN job and fetches real recipe steps from the database (after migrations are updated in steps 8.f-8.k).
        *   `[âœ…]` 8.l.ii. Find an EXECUTE job recipe step (e.g., `'thesis_generate_business_case'`) and verify it has `output_type: 'business_case'` (not `'rendered_document'`).
        *   `[âœ…]` 8.l.iii. Call the planner function (e.g., `planPerSourceDocument`) with the recipe step and source documents, and assert that it successfully creates child EXECUTE job payloads with `output_type: 'business_case'`.
        *   `[âœ…]` 8.l.iv. Assert that the `isModelContributionFileType` validation in the planner passes (no error is thrown).
        *   `[âœ…]` 8.l.v. Verify that `output_type` in the recipe step matches `document_key` in `outputs_required.documents[0].document_key` (e.g., both are `'business_case'`), ensuring semantic alignment. This test verifies the end-to-end flow: database has correct `output_type` matching `document_key`, `getStageRecipe` returns it correctly, and planners can use it to create EXECUTE jobs.
    *   `[âœ…]` 8.m. `[TEST-INT]` **GREEN**: Re-run the test from step 8.l and ensure it passes. This verifies that the complete end-to-end flow works correctly after all changes (types updated, source code updated, database migrations applied).
    *   `[âœ…]` 8.n. `[LINT]` Run the linter for all modified files (`supabase/functions/dialectic-service/dialectic.interface.ts`, `supabase/functions/dialectic-service/getStageRecipe.ts`) and resolve any warnings or errors.

*   `[âœ…]` 9. **`[BE]` Fix assembleTurnPrompt to Query Header Context by Contribution ID Instead of Storage Path**
    *   `[âœ…]` 9.a. `[DEPS]` The `assembleTurnPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` (lines 47-51, 78-82) requires `payload.header_context_resource_id` (a storage path string) and hardcodes bucket `"SB_CONTENT_STORAGE_BUCKET"` to download header context. However, planners (e.g., `planPerSourceDocument` in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, line 88) set `inputs[`${doc.contribution_type}_id`] = doc.id`, which for `header_context` creates `inputs.header_context_id` with the contribution ID. Header context is stored as a contribution in `dialectic_contributions` with `contribution_type = 'header_context'`. The `assemblePlannerPrompt` and `assembleSeedPrompt` functions correctly use `gatherInputsForStage` which queries contributions by metadata (stage, iteration, etc.) and gets storage details from the database record, avoiding duplication of storage paths in payloads. The fix requires: (1) updating `assembleTurnPrompt` to read `inputs.header_context_id` from `job.payload.inputs.header_context_id`, query `dialectic_contributions` by that ID to get `storage_bucket`, `storage_path`, and `file_name`, construct the full path, and download using the record's bucket (matching the pattern in `gatherInputsForStage` lines 104-185), (2) removing the `header_context_resource_id` requirement from the payload precondition check (line 47-51), (3) removing the hardcoded bucket usage, ensuring the function uses the database as the single source of truth for storage details.
    *   `[âœ…]` 9.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, write tests that verify `assembleTurnPrompt` queries header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 9.b.i. Create a test case that mocks a job payload with `inputs.header_context_id` set to a contribution ID (e.g., `"contrib-123"`), and mocks a database query to `dialectic_contributions` that returns a contribution record with `storage_bucket: "dialectic_contributions"`, `storage_path: "path/to/header"`, `file_name: "header_context.json"`, and `contribution_type: "header_context"`.
        *   `[âœ…]` 9.b.ii. Mock `downloadFromStorage` to be called with the contribution's `storage_bucket` and the constructed path (`${storage_path}/${file_name}`) instead of hardcoded `"SB_CONTENT_STORAGE_BUCKET"` and `payload.header_context_resource_id`.
        *   `[âœ…]` 9.b.iii. Assert that `assembleTurnPrompt` successfully queries the contribution by ID, constructs the storage path from the record, and downloads using the record's bucket. This test must fail because `assembleTurnPrompt` currently requires `header_context_resource_id` in payload and uses hardcoded bucket.
        *   `[âœ…]` 9.b.iv. Create a test case that verifies `assembleTurnPrompt` throws an error when `inputs.header_context_id` is missing or invalid (contribution not found in database). This test must fail because the function currently checks for `header_context_resource_id` instead.
        *   `[âœ…]` 9.b.v. Create a test case that verifies `assembleTurnPrompt` throws an error when the contribution record is missing `storage_bucket`, `storage_path`, or `file_name`. This test must fail initially but should pass after implementation.
    *   `[âœ…]` 9.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts`, update the function to query header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 9.c.i. Remove the precondition check for `payload.header_context_resource_id` (lines 47-51) that throws an error when missing.
        *   `[âœ…]` 9.c.ii. Add a precondition check that verifies `job.payload.inputs` exists and is a record, and that `job.payload.inputs.header_context_id` is a string (the contribution ID).
        *   `[âœ…]` 9.c.iii. Query `dialectic_contributions` by `inputs.header_context_id` to get the contribution record, selecting `storage_bucket`, `storage_path`, `file_name`, and `contribution_type` fields. Verify the contribution exists and has `contribution_type = 'header_context'`.
        *   `[âœ…]` 9.c.iv. Validate that the contribution record has `storage_bucket`, `storage_path`, and `file_name` (all non-null strings). If any are missing, throw an error indicating the contribution is missing storage details.
        *   `[âœ…]` 9.c.v. Construct the full storage path as `${contrib.storage_path}/${contrib.file_name}` (or just `contrib.storage_path` if `file_name` is empty, matching the pattern in `gatherInputsForStage` line 144-146).
        *   `[âœ…]` 9.c.vi. Replace the `downloadFromStorage` call (lines 78-82) to use `contrib.storage_bucket` instead of hardcoded `"SB_CONTENT_STORAGE_BUCKET"`, and use the constructed path instead of `job.payload.header_context_resource_id`.
        *   `[âœ…]` 9.c.vii. Ensure all error messages are updated to reference the contribution ID lookup instead of `header_context_resource_id`.
    *   `[âœ…]` 9.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 9.b and ensure they now pass. Also verify that existing `assembleTurnPrompt` tests still pass (update any tests that mock `header_context_resource_id` to instead mock `inputs.header_context_id` and the database query).
    *   `[âœ…]` 9.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` and resolve any warnings or errors.

*   `[âœ…]` 10. **`[BE]` Fix assembleContinuationPrompt to Query Header Context by Contribution ID Instead of Storage Path**
    *   `[âœ…]` 10.a. `[DEPS]` The `assembleContinuationPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts` (lines 82-89) optionally uses `payload.header_context_resource_id` (storage path) and hardcodes bucket `"dialectic_project_resources"`. However, planners (e.g., `planPerSourceDocument` in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, line 88) set `inputs[`${doc.contribution_type}_id`] = doc.id`, which for `header_context` creates `inputs.header_context_id` with the contribution ID. Header context is stored as a contribution in `dialectic_contributions` with `contribution_type = 'header_context'`. The `assemblePlannerPrompt` and `assembleSeedPrompt` functions correctly use `gatherInputsForStage` which queries contributions by metadata (stage, iteration, etc.) and gets storage details from the database record, avoiding duplication of storage paths in payloads. The fix requires: (1) updating `assembleContinuationPrompt` to optionally read `inputs.header_context_id` and query the contribution if present, (2) removing the hardcoded bucket usage and storage path requirement, ensuring the function uses the database as the single source of truth for storage details, (3) keeping the optional behavior: if `inputs.header_context_id` is missing, the function should continue without header context (no error thrown).
    *   `[âœ…]` 10.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.test.ts`, write tests that verify `assembleContinuationPrompt` optionally queries header context by contribution ID from `inputs` instead of requiring `header_context_resource_id` in payload.
        *   `[âœ…]` 10.b.i. Create a test case that mocks a job payload with `inputs.header_context_id` set to a contribution ID, and mocks a database query that returns a contribution record with storage details, and asserts that `assembleContinuationPrompt` successfully queries and downloads the header context using the contribution's bucket and path.
        *   `[âœ…]` 10.b.ii. Create a test case that verifies `assembleContinuationPrompt` works correctly when `inputs.header_context_id` is missing (header context is optional for continuation prompts), and no header context is included in the final prompt. This test should pass initially since the function already handles optional header context (line 82-106).
        *   `[âœ…]` 10.b.iii. Create a test case that verifies `assembleContinuationPrompt` throws an error when `inputs.header_context_id` is provided but the contribution is not found in the database. This test must fail because the function currently uses `header_context_resource_id` directly without querying.
        *   `[âœ…]` 10.b.iv. Create a test case that verifies `assembleContinuationPrompt` uses the contribution's `storage_bucket` instead of hardcoded `"dialectic_project_resources"` when downloading header context. This test must fail because the function currently hardcodes the bucket (line 87).
    *   `[âœ…]` 10.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts`, update the function to optionally query header context by contribution ID from `inputs` instead of using `header_context_resource_id` in payload.
        *   `[âœ…]` 10.c.i. Update the header context fetching logic (lines 82-106) to check for `job.payload.inputs?.header_context_id` instead of `job.payload?.header_context_resource_id`.
        *   `[âœ…]` 10.c.ii. If `inputs.header_context_id` exists and is a string, query `dialectic_contributions` by that ID to get the contribution record with `storage_bucket`, `storage_path`, `file_name`, and `contribution_type` fields. Verify the contribution exists and has `contribution_type = 'header_context'`.
        *   `[âœ…]` 10.c.iii. Validate that the contribution record has `storage_bucket`, `storage_path`, and `file_name` (all non-null strings). If any are missing, throw an error indicating the contribution is missing storage details.
        *   `[âœ…]` 10.c.iv. Construct the full storage path as `${contrib.storage_path}/${contrib.file_name}` (or just `contrib.storage_path` if `file_name` is empty).
        *   `[âœ…]` 10.c.v. Replace the `downloadFromStorage` call (lines 85-89) to use `contrib.storage_bucket` instead of hardcoded `"dialectic_project_resources"`, and use the constructed path instead of `headerResourceId`.
        *   `[âœ…]` 10.c.vi. Ensure all error messages are updated to reference the contribution ID lookup instead of `header_context_resource_id`.
        *   `[âœ…]` 10.c.vii. Keep the optional behavior: if `inputs.header_context_id` is missing or undefined, the function should continue without header context (no error thrown), matching the current optional behavior.
    *   `[âœ…]` 10.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 10.b and ensure they now pass. Also verify that existing `assembleContinuationPrompt` tests still pass (update any tests that mock `header_context_resource_id` to instead mock `inputs.header_context_id` and the database query).
    *   `[âœ…]` 10.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleContinuationPrompt.ts` and resolve any warnings or errors.

*   `[âœ…]` 11. **`[BE]` Fix planAllToOne to Extract and Validate document_key**
    *   `[âœ…]` 11.a. `[DEPS]` The `planAllToOne` planner function in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 11.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, write tests that verify `planAllToOne` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 11.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'business_case'` and asserts that the created payload has `document_key: 'business_case'`.
        *   `[âœ…]` 11.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property (e.g., only has `header_context_artifact`), and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planAllToOne` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 11.b.v. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key` set to `null` or empty string, and asserts that `planAllToOne` throws an error (step outputs documents but `document_key` is invalid).
        *   `[âœ…]` 11.b.vi. Create a test case that mocks a recipe step with `outputs_required` missing or undefined, and asserts that `planAllToOne` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 11.b.vii. These tests must fail because `planAllToOne` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 11.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 11.c.i. Before creating `newPayload` (line 44), check if the step outputs documents: verify that `recipeStep.outputs_required` exists, is an object, has a `documents` property that is an array, and the array has at least one item.
        *   `[âœ…]` 11.c.ii. If the step outputs documents (condition from 11.c.i is true), extract `document_key` from `recipeStep.outputs_required.documents[0].document_key`. If `documents[0]` is missing, not an object, or missing `document_key` property, throw an error: `"planAllToOne requires recipeStep.outputs_required.documents[0].document_key but it is missing"`.
        *   `[âœ…]` 11.c.iii. If the step outputs documents, validate that `document_key` is a non-empty string. If it's null, undefined, empty string, or not a string type, throw an error: `"planAllToOne requires recipeStep.outputs_required.documents[0].document_key to be a non-empty string, but received: ${typeof document_key === 'string' ? `'${document_key}'` : String(document_key)}"`.
        *   `[âœ…]` 11.c.iv. If the step outputs documents, store the validated `document_key` in a variable. If the step does not output documents, leave `document_key` as `undefined`.
        *   `[âœ…]` 11.c.v. Add `document_key` to the `newPayload` object (after line 68, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 11.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 11.b and ensure they now pass. Also verify that existing `planAllToOne` tests still pass.
    *   `[âœ…]` 11.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 12. **`[BE]` Fix planPairwiseByOrigin to Extract and Validate document_key**
    *   `[âœ…]` 12.a. `[DEPS]` The `planPairwiseByOrigin` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 12.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, write tests that verify `planPairwiseByOrigin` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 12.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'pairwise_synthesis_chunk'` and asserts that the created payload has `document_key: 'pairwise_synthesis_chunk'`.
        *   `[âœ…]` 12.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPairwiseByOrigin` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 12.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPairwiseByOrigin` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 12.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPairwiseByOrigin` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 12.b.v. These tests must fail because `planPairwiseByOrigin` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 12.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 12.c.i. Before creating `newPayload` (line 91), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPairwiseByOrigin"` instead of `"planAllToOne"`.
        *   `[âœ…]` 12.c.ii. Add `document_key` to the `newPayload` object (after line 116, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 12.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 12.b and ensure they now pass. Also verify that existing `planPairwiseByOrigin` tests still pass.
    *   `[âœ…]` 12.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 13. **`[BE]` Fix planPerModel to Extract and Validate document_key**
    *   `[âœ…]` 13.a. `[DEPS]` The `planPerModel` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 13.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, write tests that verify `planPerModel` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 13.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'synthesis'` and asserts that the created payload has `document_key: 'synthesis'`.
        *   `[âœ…]` 13.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerModel` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 13.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerModel` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 13.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerModel` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 13.b.v. These tests must fail because `planPerModel` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 13.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 13.c.i. Before creating `newPayload` (line 75), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerModel"` instead of `"planAllToOne"`.
        *   `[âœ…]` 13.c.ii. Add `document_key` to the `newPayload` object (after line 98, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 13.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 13.b and ensure they now pass. Also verify that existing `planPerModel` tests still pass.
    *   `[âœ…]` 13.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 14. **`[BE]` Fix planPerSourceDocument to Extract and Validate document_key**
    *   `[âœ…]` 14.a. `[DEPS]` The `planPerSourceDocument` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 14.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, write tests that verify `planPerSourceDocument` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 14.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'business_case_critique'` and asserts that the created payload has `document_key: 'business_case_critique'`.
        *   `[âœ…]` 14.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceDocument` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 14.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceDocument` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 14.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceDocument` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 14.b.v. These tests must fail because `planPerSourceDocument` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 14.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 14.c.i. Before creating `newPayload` (line 94), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceDocument"` instead of `"planAllToOne"`.
        *   `[âœ…]` 14.c.ii. Add `document_key` to the `newPayload` object (after line 116, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 14.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 14.b and ensure they now pass. Also verify that existing `planPerSourceDocument` tests still pass.
    *   `[âœ…]` 14.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 15. **`[BE]` Fix planPerSourceDocumentByLineage to Extract and Validate document_key**
    *   `[âœ…]` 15.a. `[DEPS]` The `planPerSourceDocumentByLineage` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 15.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, write tests that verify `planPerSourceDocumentByLineage` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 15.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'technical_requirements'` and asserts that the created payload has `document_key: 'technical_requirements'`.
        *   `[âœ…]` 15.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceDocumentByLineage` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 15.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceDocumentByLineage` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 15.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceDocumentByLineage` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 15.b.v. These tests must fail because `planPerSourceDocumentByLineage` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 15.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 15.c.i. Before creating `newPayload` (line 70), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceDocumentByLineage"` instead of `"planAllToOne"`.
        *   `[âœ…]` 15.c.ii. Add `document_key` to the `newPayload` object (after line 99, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 15.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 15.b and ensure they now pass. Also verify that existing `planPerSourceDocumentByLineage` tests still pass.
    *   `[âœ…]` 15.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 16. **`[BE]` Fix planPerSourceGroup to Extract and Validate document_key**
    *   `[âœ…]` 16.a. `[DEPS]` The `planPerSourceGroup` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` creates `DialecticExecuteJobPayload` objects but does not set `document_key`. The `assembleTurnPrompt` function requires `job.payload.document_key` (string) to find document info in `headerContext.files_to_generate`, but only for steps that output documents. The `document_key` must be extracted from `recipeStep.outputs_required.documents[0].document_key` ONLY IF the step outputs documents (i.e., if `outputs_required.documents` exists and has at least one item). If the step outputs documents (has `outputs_required.documents` array with items), then `document_key` must be a non-empty string and must be extracted and validated. If the step does not output documents (missing `outputs_required.documents` or empty array), then `document_key` should not be set in the payload. If the step outputs documents but `outputs_required.documents[0].document_key` is missing, undefined, null, or not a string, the planner must throw an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing.
    *   `[âœ…]` 16.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, write tests that verify `planPerSourceGroup` sets `document_key` in the payload when the step outputs documents and does not require it when the step does not output documents.
        *   `[âœ…]` 16.b.i. Create a test case that mocks a recipe step with `outputs_required.documents[0].document_key: 'synthesis'` and asserts that the created payload has `document_key: 'synthesis'`.
        *   `[âœ…]` 16.b.ii. Create a test case that mocks a recipe step with `outputs_required.documents` array empty, and asserts that `planPerSourceGroup` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 16.b.iii. Create a test case that mocks a recipe step with `outputs_required` missing `documents` property, and asserts that `planPerSourceGroup` does NOT set `document_key` in the payload (step does not output documents, so `document_key` is not required).
        *   `[âœ…]` 16.b.iv. Create a test case that mocks a recipe step with `outputs_required.documents[0]` missing `document_key` property, and asserts that `planPerSourceGroup` throws an error (step outputs documents but `document_key` is missing).
        *   `[âœ…]` 16.b.v. These tests must fail because `planPerSourceGroup` currently does not set `document_key` in the payload and does not validate its presence conditionally.
    *   `[âœ…]` 16.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts`, conditionally extract `document_key` from `recipeStep.outputs_required` only if the step outputs documents, and set it in the payload, throwing errors when missing for document-outputting steps.
        *   `[âœ…]` 16.c.i. Before creating `newPayload` (line 54), use the same conditional logic as step 11.c.i through 11.c.v, but with error messages prefixed with `"planPerSourceGroup"` instead of `"planAllToOne"`.
        *   `[âœ…]` 16.c.ii. Add `document_key` to the `newPayload` object (after line 80, before the closing brace) only if it was extracted: `...(documentKey ? { document_key: documentKey } : {})`. Do not use any fallback or default value.
    *   `[âœ…]` 16.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 16.b and ensure they now pass. Also verify that existing `planPerSourceGroup` tests still pass.
    *   `[âœ…]` 16.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 17. **`[TEST-INT]` Fix Integration Test to Verify End-to-End document_key Flow**
    *   `[âœ…]` 17.a. `[DEPS]` After all planners are updated (steps 11-16), write an integration test that verifies the complete end-to-end flow: planners create EXECUTE job payloads with `document_key`, and `assembleTurnPrompt` can successfully process them. The test file `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file should verify this flow.
    *   `[âœ…]` 17.b. `[TEST-INT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.integration.test.ts` or similar integration test file, write a test that verifies EXECUTE jobs created by planners include `document_key` and can be processed by `assembleTurnPrompt`.
        *   `[âœ…]` 17.b.i. Create a test case that sets up a PLAN job, calls a planner function (e.g., `planPerSourceDocument`) with a recipe step that has `outputs_required.documents[0].document_key`, and creates child EXECUTE jobs.
        *   `[âœ…]` 17.b.ii. Assert that the created EXECUTE job payloads have `document_key` set to the expected value from `outputs_required.documents[0].document_key`.
        *   `[âœ…]` 17.b.iii. Mock `assembleTurnPrompt` and verify it receives a payload with `document_key` set correctly, and that it can successfully find the document info in `headerContext.files_to_generate` using that `document_key`.
        *   `[âœ…]` 17.b.iv. This test must fail initially if any planner is missing `document_key`, but should pass after all planners are updated (steps 11-16).
    *   `[âœ…]` 17.c. `[TEST-INT]` **GREEN**: Re-run the test from step 17.b and ensure it now passes. This verifies that the complete end-to-end flow works correctly: planners create payloads with `document_key`, and `assembleTurnPrompt` can process them successfully.
    *   `[âœ…]` 17.d. `[LINT]` Run the linter for the integration test file and resolve any warnings or errors.
    *   `[âœ…]` 17.e. `[CRITERIA]` All six planners (`planAllToOne`, `planPairwiseByOrigin`, `planPerModel`, `planPerSourceDocument`, `planPerSourceDocumentByLineage`, `planPerSourceGroup`) now extract `document_key` from `recipeStep.outputs_required.documents[0].document_key` and set it in the `DialecticExecuteJobPayload`. If `document_key` is missing, undefined, null, empty string, or not a string, each planner throws an error immediately (fail loud and hard) - no fallbacks, defaults, or silent healing. All unit tests pass, integration tests verify the end-to-end flow works, and `assembleTurnPrompt` can successfully process EXECUTE jobs created by any planner. Error messages clearly identify which planner failed and why `document_key` is missing.
    *   `[âœ…]` 17.f. `[COMMIT]` Commit message: "fix: add document_key validation to all planner EXECUTE job payloads - Extract document_key from recipeStep.outputs_required.documents[0].document_key - Throw errors immediately if document_key is missing, null, empty, or invalid (fail loud and hard, no fallbacks) - Update all six planners (planAllToOne, planPairwiseByOrigin, planPerModel, planPerSourceDocument, planPerSourceDocumentByLineage, planPerSourceGroup) - Add unit tests for each planner verifying document_key extraction and error handling - Add integration test verifying end-to-end flow with assembleTurnPrompt - Resolves missing document_key error in assembleTurnPrompt"

*   `[âœ…]` 18. **`[TYPES]` Define ContentToInclude Type, Update ContextForDocument, and Define HeaderContext Interface in dialectic.interface.ts**
    *   `[âœ…]` 18.a. `[DEPS]` The `ContextForDocument` interface in `supabase/functions/dialectic-service/dialectic.interface.ts` (line 1240) currently defines `content_to_include` as `Record<string, unknown> | Record<string, unknown>[]`, which is too generic and doesn't enforce schema consistency. The `assembleTurnPrompt.ts` file (line 9) and `assembleContinuationPrompt.ts` file (line 8) both import `HeaderContext` from `dialectic.interface.ts`, but this type is NOT exported from that file, causing a missing type definition that will cause TypeScript errors. Based on complete analysis of all recipe migrations in Recipe-Planner-Gap-Analysis.md (lines 862-900), all `content_to_include` structures are objects (conceptually `Record<string, ...>`) where values can be: string (empty string "" for placeholders, or filled strings), string[] (array of strings), boolean (for flags), number (for scores, counts), nested ContentToInclude objects (recursive), or ContentToInclude[] (array of nested objects for repeated sections). **IMPORTANT**: Because the type is recursive, TypeScript requires index signature syntax `{[key: string]: ...}` rather than `Record<string, ...>` which cannot express recursion. The only exception is Antithesis `non_functional_requirements` which uses array of strings at top level - this will be fixed in migration step 20. The unified type must support all observed structures from all 5 stage migrations, and any structure from the migration that is not aligned to other structures in other migrations must be aligned to the common, unified pattern. The `HeaderContext` interface must be defined to match the actual structure of the header_context artifact generated by PLAN jobs: `system_materials: SystemMaterials`, `header_context_artifact: HeaderContextArtifact`, and `context_for_documents: ContextForDocument[]`. It must NOT have `files_to_generate` (that's in recipe step, not header context). All three type definitions must be added to the same file in a single step to ensure type consistency and avoid import errors.
    *   `[âœ…]` 18.b. `[TYPES]` In `supabase/functions/dialectic-service/dialectic.interface.ts`, define all three types/interfaces in the correct order: `ContentToInclude` type (before `ContextForDocument` interface, around line 1230), update `ContextForDocument` interface (line 1239-1242), and define `HeaderContext` interface (after `ContextForDocument`, around line 1242).
        *   `[âœ…]` 18.b.i. Add the `ContentToInclude` type definition before the `ContextForDocument` interface definition (around line 1230, before line 1239 where `ContextForDocument` is defined). The type definition must include the complete JSDoc comment explaining all observed structures and examples from recipes (as specified in Recipe-Planner-Gap-Analysis.md Fix 2, lines 1110-1128). **IMPORTANT**: TypeScript does not permit recursive types with `Record<string, ...>` syntax. Use index signature syntax instead. The type must be exported with this exact multi-line format: `export type ContentToInclude = {` on the first line, `[key: string]:` on the second line, then each union member (`| string`, `| string[]`, `| boolean`, `| number`, `| ContentToInclude`, `| ContentToInclude[]`) on separate lines with comments, then closing with `};` on the final line. This index signature syntax is the only way TypeScript supports recursive type definitions. Do NOT use `Record<string, ...>` as it cannot express recursion.
        *   `[âœ…]` 18.b.ii. Update the `ContextForDocument` interface (lines 1239-1242) to use `ContentToInclude` instead of the generic type. Change line 1240 from `content_to_include: Record<string, unknown> | Record<string, unknown>[];` to `content_to_include: ContentToInclude;`. Ensure `ContentToInclude` is accessible (it should be in the same file after step 18.b.i).
        *   `[âœ…]` 18.b.iii. Verify that `SystemMaterials` and `HeaderContextArtifact` types/interfaces are already defined in the file. If they are missing, this step must halt and report the missing types as a blocking issue. If they exist, proceed to define `HeaderContext`.
        *   `[âœ…]` 18.b.iv. Add the `HeaderContext` interface definition exactly as specified in Recipe-Planner-Gap-Analysis.md Fix 2 (lines 1146-1157) after the `ContextForDocument` interface definition (after `ContextForDocument`). The interface definition must include the complete JSDoc comment explaining that `files_to_generate` is NOT in header_context, that it's defined in the EXECUTE recipe step's outputs_required, and that `context_for_documents` is filled by PLAN job agent with alignment details. Note that `review_metadata` is stage-specific (only Antithesis) and is not part of the base `HeaderContext` interface - it should be handled separately in stage-specific logic if needed. Ensure the interface is exported: `export interface HeaderContext { system_materials: SystemMaterials; header_context_artifact: HeaderContextArtifact; context_for_documents: ContextForDocument[]; }`
        *   `[âœ…]` 18.b.v. Add `context_for_documents?: ContextForDocument[]` as an optional field to the `DialecticPlanJobPayload` interface (around line 682-684) so planners can pass it in PLAN job payloads. This ensures `assemblePlannerPrompt` can access `context_for_documents` from the job payload. The interface currently only extends `DialecticBaseJobPayload` and has `job_type: JobType`. Update it to: `export interface DialecticPlanJobPayload extends DialecticBaseJobPayload { job_type: JobType; context_for_documents?: ContextForDocument[]; }`
    *   `[âœ…]` 18.c. `[LINT]` Run the linter for `supabase/functions/dialectic-service/dialectic.interface.ts` and resolve any warnings or errors. Verify that all three types are properly exported and can be imported by other files.

*   `[âœ…]` 19. **`[DB]` Fix Thesis Stage Migration - Verify and Fix Structure Matching Between PLAN and EXECUTE Steps**
    *   `[âœ…]` 19.a. `[DEPS]` The Thesis stage migration file `supabase/migrations/20251006194531_thesis_stage.sql` has PLAN steps with `context_for_documents` and EXECUTE steps with `files_to_generate`. According to Recipe-Planner-Gap-Analysis.md Gap 9 (lines 793-850) and Gap 10 (lines 852-963), ALL recipe migrations must ensure: (1) `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the EXECUTE steps, (2) `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. The Thesis stage has 4 EXECUTE steps: `thesis_generate_business_case`, `thesis_generate_feature_spec`, `thesis_generate_technical_approach`, and `thesis_generate_success_metrics`. Each EXECUTE step must have its `files_to_generate[].from_document_key` match the corresponding PLAN step's `context_for_documents[].document_key`, and the `content_to_include` structures must match exactly. The migration file must be verified and any mismatches must be fixed to ensure proper PLAN â†” EXECUTE structure mapping.
    *   `[âœ…]` 19.b. `[DB]` In `supabase/migrations/20251006194531_thesis_stage.sql`, verify and fix structure matching between PLAN and EXECUTE steps.
        *   `[âœ…]` 19.b.i. For the PLAN step (lines 171-251), identify all `context_for_documents` entries and their `document_key` values: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`. Document the `content_to_include` structure for each entry.
        *   `[âœ…]` 19.b.ii. For each EXECUTE step (`thesis_generate_business_case` around line 323, `thesis_generate_feature_spec` around line 485, `thesis_generate_technical_approach` around line 643, `thesis_generate_success_metrics` around line 800), verify that `files_to_generate[].from_document_key` matches the PLAN step's `context_for_documents[].document_key` for the same document. For example, `thesis_generate_business_case` EXECUTE step should have `files_to_generate` with `from_document_key: "business_case"` matching the PLAN step's `context_for_documents` entry with `document_key: "business_case"`.
        *   `[âœ…]` 19.b.iii. For each EXECUTE step, verify that `documents[].content_to_include` structure exactly matches the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. For example, if PLAN step has `{"field1": "", "field2": []}`, the EXECUTE step must have the same structure `{"field1": "", "field2": []}` (values can differ, but structure must match).
        *   `[âœ…]` 19.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after changes.
        *   `[âœ…]` 19.b.v. **Fix 7: Standardize PLAN step `content_to_include` structures**: For the PLAN step's `context_for_documents` entry for `feature_spec` (line 219), change `content_to_include` from an array `[{...}]` to an object structure that conforms to the `ContentToInclude` type. The structure should be: `{"features": [{"feature_name": "", "user_stories": []}]}` (wrapping the array in an object with a `features` key). This ensures all `content_to_include` structures are objects (not arrays at top level) as required by the `ContentToInclude` type.
        *   `[âœ…]` 19.b.vi. **Fix 7: Update corresponding EXECUTE step**: For the EXECUTE step `thesis_generate_feature_spec` (around line 485), update the `documents[].content_to_include` structure to match the standardized PLAN step structure: `{"features": [{"feature_name": "", "user_stories": []}]}`. Ensure the structure exactly matches the PLAN step.
        *   `[âœ…]` 19.b.vii. **Fix 8: Standardize all EXECUTE step `content_to_include` structures**: For all 4 EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) Structures must exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
        *   `[âœ…]` 19.b.viii. Verify that all 4 EXECUTE steps have been checked and fixed. Document any changes made in comments if needed.
    *   `[âœ…]` 19.c. `[LINT]` Run the linter for `supabase/migrations/20251006194531_thesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 20. **`[DB]` Fix Antithesis Stage Migration - Fix non_functional_requirements Structure and Verify Structure Matching**
    *   `[âœ…]` 20.a. `[DEPS]` The Antithesis stage migration file `supabase/migrations/20251006194542_antithesis_stage.sql` has a PLAN step with `context_for_documents` entry for `non_functional_requirements` that uses an array of strings at the top level (e.g., `["security", "performance", ...]`) instead of an object structure. According to Recipe-Planner-Gap-Analysis.md Gap 7 (lines 736-763) and Gap 10 (lines 852-963), all `content_to_include` structures must be objects (Record<string, ...>), not arrays at the top level. This must be changed to `{"categories": ["security", "performance", ...]}` to match the unified object pattern defined by the `ContentToInclude` type. The corresponding EXECUTE step's `documents[].content_to_include` must also be updated to match this structure. Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps: `context_for_documents[].document_key` must match `files_to_generate[].from_document_key`, and `content_to_include` structures must match exactly.
    *   `[âœ…]` 20.b. `[DB]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, fix the `non_functional_requirements` structure and verify structure matching.
        *   `[âœ…]` 20.b.i. Find the PLAN step's `context_for_documents` entry for `non_functional_requirements` (should be in the PLAN step around lines 448-577). Change the `content_to_include` from an array of strings (e.g., `["security", "performance", ...]`) to an object with a `categories` property: `{"categories": ["security", "performance", ...]}`. Preserve the actual category values, only change the structure from array to object. Ensure JSON structure remains valid.
        *   `[âœ…]` 20.b.ii. Find the EXECUTE step for `non_functional_requirements` (should be an EXECUTE step in the same file). Update the `documents[].content_to_include` to match the PLAN step structure: `{"categories": ["security", "performance", ...]}`. Ensure the structure exactly matches the PLAN step (same keys, same array structure within the object).
        *   `[âœ…]` 20.b.iii. For all other documents in the Antithesis stage (business_case_critique, technical_feasibility_assessment, risk_register, dependency_map, comparison_vector), verify that PLAN step's `context_for_documents[].document_key` matches EXECUTE step's `files_to_generate[].from_document_key` for each document. Verify that `content_to_include` structures match exactly between PLAN and EXECUTE steps for each `document_key`.
        *   `[âœ…]` 20.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after all changes.
        *   `[âœ…]` 20.b.v. **Fix 7: Standardize all PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level - `non_functional_requirements` is already fixed in 20.b.i), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 20.b.vi. **Fix 8: Standardize all EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 20.c. `[LINT]` Run the linter for `supabase/migrations/20251006194542_antithesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 21. **`[DB]` Fix Synthesis Stage Migration - Remove files_to_generate from PLAN Steps, Add to EXECUTE Steps, and Verify Structure Matching**
    *   `[âœ…]` 21.a. `[DEPS]` The Synthesis stage migration file `supabase/migrations/20251006194549_synthesis_stage.sql` has multiple issues identified in Recipe-Planner-Gap-Analysis.md: (1) Synthesis Pairwise PLAN step (lines 247-368, specifically lines 350-367) incorrectly includes `files_to_generate`, (2) Synthesis Final Header PLAN step (lines 779-938, specifically lines 924-937) incorrectly includes `files_to_generate`, (3) Synthesis Pairwise EXECUTE step (lines 390-420) is missing `files_to_generate` - JSON ends at line 419 with just `]` closing the documents array, (4) Synthesis Final Deliverables EXECUTE steps are missing `files_to_generate`: `product_requirements` (around line 1018), `system_architecture` (around line 1070), and `tech_stack` (around line 1124). According to the conceptual model (Gap 3, Gap 6), PLAN steps should only have `context_for_documents` (which defines the document models with empty `content_to_include` objects for the agent to fill), while `files_to_generate` belongs only in EXECUTE steps (which define execution instructions). Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps.
    *   `[âœ…]` 21.b. `[DB]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, remove `files_to_generate` from PLAN steps, add to EXECUTE steps, and verify structure matching.
        *   `[âœ…]` 21.b.i. For Synthesis Pairwise PLAN step (lines 247-368): Remove the `files_to_generate` array (lines 350-367) that contains entries for `synthesis_pairwise_business_case`, `synthesis_pairwise_feature_spec`, `synthesis_pairwise_technical_approach`, and `synthesis_pairwise_success_metrics`. Remove the comma before `files_to_generate` if present. Ensure the JSON structure remains valid after removal. Verify that `context_for_documents` array remains intact with proper structure.
        *   `[âœ…]` 21.b.ii. For Synthesis Final Header PLAN step (lines 779-938): Remove the `files_to_generate` array (lines 924-937) that contains entries for `product_requirements`, `system_architecture`, and `tech_stack`. Remove the comma before `files_to_generate` if present. Ensure the JSON structure remains valid after removal. Verify that `context_for_documents` array remains intact with proper structure.
        *   `[âœ…]` 21.b.iii. For Synthesis Pairwise EXECUTE step (lines 390-420): After the `documents` array closes (line 419), add a comma and then add `files_to_generate` array with entries matching the document keys in the `documents` array. Each entry must have `from_document_key` (matching the `document_key` from the documents array) and `template_filename` (matching the `template_filename` from the documents array). The structure should be: `"files_to_generate": [{"template_filename": "synthesis_pairwise_business_case.json", "from_document_key": "synthesis_pairwise_business_case"}, {"template_filename": "synthesis_pairwise_feature_spec.json", "from_document_key": "synthesis_pairwise_feature_spec"}, {"template_filename": "synthesis_pairwise_technical_approach.json", "from_document_key": "synthesis_pairwise_technical_approach"}, {"template_filename": "synthesis_pairwise_success_metrics.json", "from_document_key": "synthesis_pairwise_success_metrics"}]`. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.iv. For Synthesis Final Deliverables EXECUTE step `product_requirements` (around line 1018): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_product_requirements_document.md", "from_document_key": "product_requirements"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.v. For Synthesis Final Deliverables EXECUTE step `system_architecture` (around line 1070): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_system_architecture.md", "from_document_key": "system_architecture"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.vi. For Synthesis Final Deliverables EXECUTE step `tech_stack` (around line 1124): After the `documents` array closes, add a comma and then add `files_to_generate` array with entry: `{"template_filename": "synthesis_tech_stack.md", "from_document_key": "tech_stack"}`. Verify the `template_filename` matches what's in the `documents` array. Ensure JSON structure is valid.
        *   `[âœ…]` 21.b.vii. For all Synthesis PLAN and EXECUTE steps, verify that `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the corresponding EXECUTE steps. For example, Synthesis Pairwise PLAN step has `context_for_documents` with `document_key: "synthesis_pairwise_business_case"`, and the Synthesis Pairwise EXECUTE step must have `files_to_generate` with `from_document_key: "synthesis_pairwise_business_case"`. Verify this mapping for all documents: synthesis_pairwise_business_case, synthesis_pairwise_feature_spec, synthesis_pairwise_technical_approach, synthesis_pairwise_success_metrics, product_requirements, system_architecture, tech_stack.
        *   `[âœ…]` 21.b.viii. For all Synthesis PLAN and EXECUTE steps, verify that `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. Fix any mismatches by updating the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure.
        *   `[âœ…]` 21.b.ix. **Fix 7: Standardize all Synthesis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries (Pairwise and Final Header), ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 21.b.x. **Fix 8: Standardize all Synthesis EXECUTE step `content_to_include` structures**: For all EXECUTE steps (Pairwise, Document-level, and Final Deliverables), ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 21.c. `[LINT]` Run the linter for `supabase/migrations/20251006194549_synthesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid for all steps.

*   `[âœ…]` 22. **`[DB]` Fix Parenthesis Stage Migration - Verify and Fix Structure Matching Between PLAN and EXECUTE Steps**
    *   `[âœ…]` 22.a. `[DEPS]` The Parenthesis stage migration file `supabase/migrations/20251006194558_parenthesis_stage.sql` has PLAN steps with `context_for_documents` and EXECUTE steps with `files_to_generate`. According to Recipe-Planner-Gap-Analysis.md Gap 9 (lines 793-850) and Gap 10 (lines 852-963), ALL recipe migrations must ensure: (1) `context_for_documents[].document_key` values exactly match `files_to_generate[].from_document_key` values for the EXECUTE steps, (2) `context_for_documents[].content_to_include` schema exactly matches `documents[].content_to_include` schema for the same `document_key`. The Parenthesis stage has 3 EXECUTE steps: `parenthesis_generate_technical_requirements`, `parenthesis_generate_master_plan`, and `parenthesis_generate_milestone_schema`. Each EXECUTE step must have its `files_to_generate[].from_document_key` match the corresponding PLAN step's `context_for_documents[].document_key`, and the `content_to_include` structures must match exactly.
    *   `[âœ…]` 22.b. `[DB]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, verify and fix structure matching between PLAN and EXECUTE steps.
        *   `[âœ…]` 22.b.i. For the PLAN step (lines 271-393), identify all `context_for_documents` entries and their `document_key` values: `technical_requirements`, `master_plan`, `milestone_schema`. Document the `content_to_include` structure for each entry.
        *   `[âœ…]` 22.b.ii. For each EXECUTE step (`parenthesis_generate_technical_requirements` around line 647, `parenthesis_generate_master_plan` around line 800, `parenthesis_generate_milestone_schema` around line 950), verify that `files_to_generate[].from_document_key` matches the PLAN step's `context_for_documents[].document_key` for the same document. For example, `parenthesis_generate_technical_requirements` EXECUTE step should have `files_to_generate` with `from_document_key: "technical_requirements"` matching the PLAN step's `context_for_documents` entry with `document_key: "technical_requirements"`.
        *   `[âœ…]` 22.b.iii. For each EXECUTE step, verify that `documents[].content_to_include` structure exactly matches the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Compare the structure (not values) - they must have the same keys, same nested structure, same array positions, etc. For example, if PLAN step has `{"array_field": [], "string_field": "", "nested_object": {}}`, the EXECUTE step must have the same structure (values can differ, but structure must match).
        *   `[âœ…]` 22.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after changes.
        *   `[âœ…]` 22.b.v. **Fix 7: Standardize all Parenthesis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 22.b.vi. **Fix 8: Standardize all Parenthesis EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
        *   `[âœ…]` 22.b.vii. Verify that all 3 EXECUTE steps have been checked and fixed. Document any changes made in comments if needed.
    *   `[âœ…]` 22.c. `[LINT]` Run the linter for `supabase/migrations/20251006194558_parenthesis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 23. **`[DB]` Fix Paralysis Stage Migration - Add content_to_include to EXECUTE Step and Verify Structure Matching**
    *   `[âœ…]` 23.a. `[DEPS]` The Paralysis stage migration file `supabase/migrations/20251006194605_paralysis_stage.sql` has an EXECUTE step for `actionable_checklist` (lines 458-486) where the `documents` array entry is missing the `content_to_include` field. According to Recipe-Planner-Gap-Analysis.md Gap 8 (lines 764-791), EXECUTE step `documents[].content_to_include` must exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key` to enable proper mapping. The PLAN step (lines 293-315) has `context_for_documents` with `actionable_checklist` entry that includes `content_to_include: {"action_items": ["..."]}`. The EXECUTE step must have the same structure. Additionally, according to Gap 9 and Gap 10, ALL recipe migrations must ensure structure matching between PLAN and EXECUTE steps for all documents: `context_for_documents[].document_key` must match `files_to_generate[].from_document_key`, and `content_to_include` structures must match exactly.
    *   `[âœ…]` 23.b. `[DB]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, add `content_to_include` to the EXECUTE step and verify structure matching.
        *   `[âœ…]` 23.b.i. Find the EXECUTE step for `actionable_checklist` (lines 458-486). Locate the `documents` array entry (should be around lines 461-464 based on the analysis).
        *   `[âœ…]` 23.b.ii. Add `content_to_include` field to the document entry. The structure must exactly match the PLAN step's `context_for_documents` entry for `actionable_checklist`. Based on the PLAN step (lines 293-315), it should be: `"content_to_include": {"action_items": ["..."]}`. Use the same structure as the PLAN step (empty array or placeholder values are acceptable for the migration - the actual values will be filled by the agent during execution). Ensure proper comma placement and JSON structure validity.
        *   `[âœ…]` 23.b.iii. For all documents in the Paralysis stage (actionable_checklist, updated_master_plan, advisor_recommendations), verify that PLAN step's `context_for_documents[].document_key` matches EXECUTE step's `files_to_generate[].from_document_key` for each document. Verify that `content_to_include` structures match exactly between PLAN and EXECUTE steps for each `document_key`.
        *   `[âœ…]` 23.b.iv. Fix any mismatches found: (1) If `from_document_key` doesn't match `document_key`, update `from_document_key` to match, (2) If `content_to_include` structures don't match, update the EXECUTE step's `documents[].content_to_include` structure to exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Ensure JSON structure remains valid after all changes.
        *   `[âœ…]` 23.b.v. **Fix 7: Standardize all Paralysis PLAN step `content_to_include` structures**: For all PLAN step `context_for_documents` entries, ensure that `content_to_include` structures conform to the `ContentToInclude` type: (1) Must be objects (not arrays at top level), (2) All nested structures must use allowed types (string, string[], boolean, number, ContentToInclude, ContentToInclude[]), (3) All structures must be consistent across documents of the same type. Fix any structures that don't conform.
        *   `[âœ…]` 23.b.vi. **Fix 8: Standardize all Paralysis EXECUTE step `content_to_include` structures**: For all EXECUTE steps, ensure that `documents[].content_to_include` structures conform to the `ContentToInclude` type and exactly match the PLAN step's `context_for_documents[].content_to_include` structure for the same `document_key`. Fix any structures that don't conform.
    *   `[âœ…]` 23.c. `[LINT]` Run the linter for `supabase/migrations/20251006194605_paralysis_stage.sql` and resolve any warnings or errors. Verify JSON structure is valid.

*   `[âœ…]` 24. **`[BE]` Fix isHeaderContext Type Guard and Tests in type_guards.dialectic.ts, and Define isContentToInclude Helper Function**
    *   `[âœ…]` 24.a. `[DEPS]` The type guard `isHeaderContext` in `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts` (lines 182-205) currently validates `files_to_generate` as optional (lines 197-202), but according to the new `HeaderContext` interface defined in step 18, `files_to_generate` should NOT be in `HeaderContext` at all (it's in recipe step, not header context). The type guard must be updated to match the new interface structure. The function signature uses `ReturnType<typeof JSON.parse>` which is not type-safe. The type guard should import `HeaderContext` from `../../dialectic-service/dialectic.interface.ts` and use it in the return type annotation. Additionally, steps 25, 27-32 require a helper function `isContentToInclude(value: unknown): value is ContentToInclude` to validate that `content_to_include` structures conform to the `ContentToInclude` type. This helper must be defined in the same file (`type_guards.dialectic.ts`) and exported so it can be imported by `assembleTurnPrompt` and all planner functions. The test file `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts` must be updated to test both the new `isHeaderContext` structure and the new `isContentToInclude` helper function. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 24.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts`, write tests for `isHeaderContext` that verify it matches the new `HeaderContext` interface structure.
        *   `[âœ…]` 24.b.i. Create a test case that asserts `isHeaderContext` returns `true` for a valid `HeaderContext` object with: `system_materials` (valid `SystemMaterials`), `header_context_artifact` (valid `HeaderContextArtifact`), and `context_for_documents` (array of valid `ContextForDocument` objects with `ContentToInclude` structures). This test must pass after the type guard is updated.
        *   `[âœ…]` 24.b.ii. Create a test case that asserts `isHeaderContext` returns `false` for an object that has `files_to_generate` property (since `HeaderContext` interface does not include `files_to_generate`). This test must fail initially because the current type guard allows `files_to_generate` as optional, but should pass after the type guard is updated to reject it.
        *   `[âœ…]` 24.b.iii. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `system_materials`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.iv. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `header_context_artifact`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.v. Create a test case that asserts `isHeaderContext` returns `false` for an object missing `context_for_documents`. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.vi. Create a test case that asserts `isHeaderContext` returns `false` for an object where `context_for_documents` is not an array. This test should pass with the current type guard.
        *   `[âœ…]` 24.b.vii. Create a test case that asserts `isHeaderContext` returns `false` for an object where `context_for_documents` contains invalid entries (missing `document_key` or invalid `content_to_include` structure). This test should pass with the current type guard if it validates structure.
        *   `[âœ…]` 24.b.viii. `[TEST-UNIT]` **RED**: Write tests for `isContentToInclude` helper function that will be defined in step 24.c.vi. Create test cases that verify: (1) `isContentToInclude` returns `true` for valid `ContentToInclude` objects (simple objects with string values like `{"field1": "", "field2": ""}`, objects with string arrays like `{"field1": [], "field2": ["value1", "value2"]}`, objects with nested objects like `{"dimensions": {"feasibility": {"score": 0, "rationale": ""}}}`, objects with arrays of objects like `{"features": [{"name": "", "stories": []}]}`, mixed structures combining all types), (2) `isContentToInclude` returns `false` for arrays at top level (must be objects, not arrays - e.g., `["string1", "string2"]` should fail), (3) `isContentToInclude` returns `false` for invalid value types (functions, null, undefined as top-level value in object), (4) `isContentToInclude` correctly validates nested structures recursively (deeply nested objects and arrays of objects). These tests must fail initially because the function doesn't exist yet.
    *   `[âœ…]` 24.c. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts`, update the `isHeaderContext` function to match the new `HeaderContext` interface and define the `isContentToInclude` helper function.
        *   `[âœ…]` 24.c.i. Import `HeaderContext` from `../../../dialectic-service/dialectic.interface.ts` at the top of the file (the file is at `supabase/functions/_shared/utils/type-guards/`, so it needs three levels up to reach `dialectic-service`).
        *   `[âœ…]` 24.c.ii. Update the function signature (line 182) from `export function isHeaderContext(value: unknown): value is ReturnType<typeof JSON.parse>` to `export function isHeaderContext(value: unknown): value is HeaderContext`.
        *   `[âœ…]` 24.c.iii. Remove the `files_to_generate` validation block (lines 197-202) entirely, since `HeaderContext` interface does not include this property.
        *   `[âœ…]` 24.c.iv. Ensure the validation for `system_materials`, `header_context_artifact`, and `context_for_documents` remains intact (lines 185-195). These validations should continue to work correctly.
        *   `[âœ…]` 24.c.v. Verify the function still returns `true` only when all required fields are present and valid.
        *   `[âœ…]` 24.c.vi. Import `ContentToInclude` from `../../../dialectic-service/dialectic.interface.ts` at the top of the file (same import path as `HeaderContext`).
        *   `[âœ…]` 24.c.vii. Define and export the `isContentToInclude` helper function: `export function isContentToInclude(value: unknown): value is ContentToInclude`. The function must: (1) Check that value is an object using `isRecord(value)` helper (import from `../type_guards.ts` if needed), (2) Check that value is NOT an array at top level using `!Array.isArray(value)` - if it's an array, return `false`, (3) Recursively validate that all values in the object are allowed types according to the `ContentToInclude` type definition: `string`, `string[]`, `boolean`, `number`, or recursively `ContentToInclude`/`ContentToInclude[]`. For each key-value pair in the object: (a) If value is `string`, `boolean`, or `number`, it's valid, (b) If value is an array, check if it's `string[]` (all elements are strings) or `ContentToInclude[]` (all elements pass `isContentToInclude` recursively), (c) If value is an object, recursively call `isContentToInclude(value)` to validate nested structures, (d) If value is any other type (function, null, undefined, etc.), return `false`. The function must handle all cases from the `ContentToInclude` type definition: `Record<string, string | string[] | boolean | number | ContentToInclude | ContentToInclude[]>`. Export the function so it can be imported by `assembleTurnPrompt` (step 25) and all planner functions (steps 27-32).
    *   `[âœ…]` 24.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 24.b (including 24.b.viii for `isContentToInclude`) and ensure they now pass. The test from 24.b.ii should now pass because the type guard rejects `files_to_generate`. Also verify that existing `isHeaderContext` tests still pass. Verify that all `isContentToInclude` tests from 24.b.viii pass.
    *   `[âœ…]` 24.e. `[LINT]` Run the linter for `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.ts` and `supabase/functions/_shared/utils/type-guards/type_guards.dialectic.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 25. **`[BE]` Fix assembleTurnPrompt to Read files_to_generate from Recipe Step and Use header_context for Alignment, and Tests**
    *   `[âœ…]` 25.a. `[DEPS]` The `assembleTurnPrompt` function in `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` currently has incorrect logic (lines 152-159) that looks for `files_to_generate` in `headerContext` and uses `document_key` instead of `from_document_key`. According to Recipe-Planner-Gap-Analysis.md Fix 1 (lines 1026-1082) and Gap 1 (lines 615-632) and Gap 2 (lines 634-651), the function must: (1) read `files_to_generate` from `stage.recipe_step.outputs_required.files_to_generate` (execution instructions), (2) use `header_context.context_for_documents` for cross-document alignment details (completed by PLAN job), (3) match `files_to_generate[].from_document_key` to `header_context.context_for_documents[].document_key`, (4) use the matched entry's `content_to_include` for document generation, (5) validate that `content_to_include` is filled (not empty). The function also imports `HeaderContext` from `dialectic.interface.ts` (line 9), which should now work correctly after step 18 defines the interface. The test file `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts` must be updated to test the new behavior. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 25.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts`, write tests that verify `assembleTurnPrompt` reads `files_to_generate` from recipe step and uses `header_context.context_for_documents` for alignment.
        *   `[âœ…]` 25.b.i. Create a test case that mocks `stage.recipe_step.outputs_required.files_to_generate` with an array containing `{from_document_key: "business_case", template_filename: "thesis_business_case.md"}`, mocks `headerContext.context_for_documents` with an array containing `{document_key: "business_case", content_to_include: {"field1": "value1"}}`, and asserts that `assembleTurnPrompt` successfully finds the document info using `from_document_key` matching `document_key`, and uses the `content_to_include` from `context_for_documents` for alignment. This test must fail because the function currently looks for `files_to_generate` in `headerContext` and uses `document_key` instead of `from_document_key`.
        *   `[âœ…]` 25.b.ii. Create a test case that mocks `stage.recipe_step.outputs_required` missing `files_to_generate`, and asserts that `assembleTurnPrompt` throws an error with message indicating `files_to_generate` is missing from recipe step. This test must fail because the function currently doesn't check for `files_to_generate` in recipe step.
        *   `[âœ…]` 25.b.iii. Create a test case that mocks `headerContext` missing `context_for_documents`, and asserts that `assembleTurnPrompt` throws an error indicating `context_for_documents` is missing from header context. This test must fail because the function currently doesn't validate `context_for_documents` exists.
        *   `[âœ…]` 25.b.iv. Create a test case that mocks `files_to_generate` with `from_document_key: "business_case"` but `headerContext.context_for_documents` has no entry with `document_key: "business_case"`, and asserts that `assembleTurnPrompt` throws an error indicating no matching `context_for_documents` entry found. This test must fail because the function currently doesn't perform this matching.
        *   `[âœ…]` 25.b.v. Create a test case that mocks `context_for_documents` entry with empty `content_to_include` object (`{}`), and asserts that `assembleTurnPrompt` throws an error indicating `content_to_include` is not filled in. This test must fail because the function currently doesn't validate that `content_to_include` is filled.
        *   `[âœ…]` 25.b.vi. Create a test case that verifies `assembleTurnPrompt` uses `docInfo.template_filename` from the matched `files_to_generate` entry (not from `headerContext`). This test must fail because the function currently gets template filename from the wrong location.
        *   `[âœ…]` 25.b.vii. Create a test case that mocks `files_to_generate` with `from_document_key: "business_case"` but the matched `context_for_documents` entry has a `content_to_include` structure that doesn't match the `ContentToInclude` type schema (e.g., has invalid nested structure), and asserts that `assembleTurnPrompt` throws an error indicating structure mismatch. This test must fail because the function currently doesn't validate structure matching.
        *   `[âœ…]` 25.b.viii. Create a test case that verifies `contextForDoc.content_to_include` is merged into the `renderContext` object passed to `renderPrompt`, ensuring alignment details are available in the template context. This test must fail because the function currently doesn't merge alignment details into render context.
    *   `[âœ…]` 25.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts`, rewrite the logic to read `files_to_generate` from recipe step and use `header_context.context_for_documents` for alignment.
        *   `[âœ…]` 25.c.i. Implement the logic described in Recipe-Planner-Gap-Analysis.md Fix 1 (lines 1050-1082) as a specification: Get `files_to_generate` from `stage.recipe_step.outputs_required?.files_to_generate`, validate it exists and is an array, find the document info using `from_document_key` matching `documentKey`, get alignment details from `headerContext.context_for_documents` by matching `document_key`, validate that `content_to_include` is filled (not empty), and use `docInfo.template_filename` for template file and `contextForDoc.content_to_include` for alignment details in prompt. Do not copy-paste pseudocode - implement the actual TypeScript logic following the specification.
        *   `[âœ…]` 25.c.ii. **Fix 9: Add runtime validation that verifies PLAN â†” EXECUTE structure mapping**: (1) Validate that `files_to_generate[].from_document_key` matches `headerContext.context_for_documents[].document_key` for the matched entry. (2) **Validate `content_to_include` structure conforms to `ContentToInclude` type**: Import the `isContentToInclude` helper function from `../../utils/type-guards/type_guards.dialectic.ts` (defined in step 24.c.vii) and use it to validate that `contextForDoc.content_to_include` conforms to the `ContentToInclude` type. (3) **Validate structure matching**: Compare the structure (not values) of `contextForDoc.content_to_include` with the expected structure from the recipe step's `documents[].content_to_include` for the same `document_key` - they must have the same keys, same nested structure, same array positions. Use the `isHeaderContext` type guard (from Step 24) to validate the header context structure at runtime. If structure mismatch is detected, throw an error indicating the mapping violation: `assembleTurnPrompt requires content_to_include structure for document_key '${documentKey}' to match the recipe step's expected structure`. **Note**: Per Fix 9 (line 1394), structure matching validation is optional and may be better performed here where `header_context` is already loaded, rather than in planners.
        *   `[âœ…]` 25.c.iii. Update any subsequent code that uses `docInfo` to ensure it uses `docInfo.template_filename` (which now comes from recipe step, not header context).
        *   `[âœ…]` 25.c.iv. Merge `contextForDoc.content_to_include` into the prompt context by adding it to the `renderContext` object that is passed to `renderPrompt`. The `content_to_include` object should be merged into `renderContext` so that template variables can access the alignment details. Ensure the merge happens after `system_materials` and `user_domain_overlay_values` but before `document_specific_data`, so alignment details can be overridden by document-specific data if needed.
        *   `[âœ…]` 25.c.v. Verify the `HeaderContext` import now resolves correctly after step 18 defines the interface.
    *   `[âœ…]` 25.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 25.b and ensure they now pass. Also verify that existing `assembleTurnPrompt` tests still pass (update any tests that mock `headerContext.files_to_generate` to instead mock `stage.recipe_step.outputs_required.files_to_generate`).
    *   `[âœ…]` 25.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.ts` and `supabase/functions/_shared/prompt-assembler/assembleTurnPrompt.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 26. **`[BE]` Fix assemblePlannerPrompt to Include context_for_documents in PLAN Prompts, and Tests**
    *   `[âœ…]` 26.a. `[DEPS]` The `assemblePlannerPrompt` function in `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts` must include `context_for_documents` in PLAN prompts so the agent knows what alignment details to produce. According to Recipe-Planner-Gap-Analysis.md Fix 6 (lines 1261-1271), the function must: (1) extract `context_for_documents` from `recipe_step.outputs_required`, (2) include the empty `content_to_include` object models in the prompt, (3) instruct the agent to fill in these models with specific alignment values, (4) structure the prompt to ensure the agent produces `header_context` with completed `content_to_include` objects. The test file `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts` must be updated to test the new behavior. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 26.b. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts`, write tests that verify `assemblePlannerPrompt` includes `context_for_documents` in PLAN prompts.
        *   `[âœ…]` 26.b.i. Create a test case that mocks a PLAN job with `recipe_step.outputs_required.context_for_documents` containing entries with empty `content_to_include` object models, and asserts that `assemblePlannerPrompt` includes these `context_for_documents` in the generated prompt. This test must fail if the function doesn't currently include `context_for_documents`.
        *   `[âœ…]` 26.b.ii. Create a test case that asserts the prompt includes instructions telling the agent to fill in the `content_to_include` objects with specific alignment values. This test must fail if the function doesn't currently include such instructions.
        *   `[âœ…]` 26.b.iii. Create a test case that mocks `recipe_step.outputs_required` missing `context_for_documents`, and asserts that `assemblePlannerPrompt` throws an error indicating `context_for_documents` is required for PLAN jobs. This test must fail if the function doesn't currently validate this.
    *   `[âœ…]` 26.c. `[BE]` **GREEN**: In `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts`, update the function to include `context_for_documents` in PLAN prompts.
        *   `[âœ…]` 26.c.i. Read the current implementation to understand how it assembles PLAN prompts.
        *   `[âœ…]` 26.c.ii. Extract `context_for_documents` from `recipe_step.outputs_required.context_for_documents` (validate it exists and is an array).
        *   `[âœ…]` 26.c.iii. Include the `context_for_documents` structure (with empty `content_to_include` object models) in the prompt that is sent to the agent.
        *   `[âœ…]` 26.c.iv. Add instructions in the prompt telling the agent to: (1) fill in the `content_to_include` objects with specific alignment values (shared terminology, consistent values, coordinated decisions), (2) produce a `header_context` artifact with completed `content_to_include` objects, (3) ensure all documents in the step group will use these alignment details.
        *   `[âœ…]` 26.c.v. Ensure the prompt structure clearly separates the empty models (what the agent should fill) from other prompt content.
    *   `[âœ…]` 26.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 26.b and ensure they now pass. Also verify that existing `assemblePlannerPrompt` tests still pass.
    *   `[âœ…]` 26.e. `[LINT]` Run the linter for `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.ts` and `supabase/functions/_shared/prompt-assembler/assemblePlannerPrompt.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 27. **`[BE]` Fix planAllToOne to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 27.a. `[DEPS]` The `planAllToOne` planner function in `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` currently does not validate or pass `context_for_documents` for PLAN jobs, and does not validate `files_to_generate` for EXECUTE jobs. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** According to Recipe-Planner-Gap-Analysis.md Fix 3 (lines 1179-1210) and Fix 4 (lines 1213-1244), when `job_type === 'PLAN'`, the planner MUST ALWAYS: (1) validate that `recipe_step.outputs_required.context_for_documents` exists and is an array with entries (this is REQUIRED, not optional), (2) validate that each entry has a `document_key` and an empty `content_to_include` object model (not an array at top level), (3) pass the entire `context_for_documents` structure into the job payload so the prompt assembler can include it. When `job_type === 'EXECUTE'`, the planner MUST ALWAYS: (1) validate that `recipe_step.outputs_required.files_to_generate` exists and is an array with entries (this is REQUIRED, not optional), (2) validate that `recipe_step.outputs_required.documents` exists and is an array with entries (this is REQUIRED, not optional), (3) validate structure (each `files_to_generate` entry has `from_document_key` and `template_filename`). The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 44-74) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with the planner function name (e.g., 'planAllToOne requires...') to aid debugging. The test file `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 27.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts`, write tests that verify `planAllToOne` validates and passes `context_for_documents` for PLAN jobs and validates `files_to_generate` for EXECUTE jobs.
        *   `[âœ…]` 27.b.i. Create a test case that mocks a PLAN job with `recipe_step.outputs_required.context_for_documents` containing valid entries, and asserts that `planAllToOne` successfully creates a job payload that includes `context_for_documents`. This test must fail because the function currently doesn't handle `context_for_documents`.
        *   `[âœ…]` 27.b.ii. Create a test case that mocks a PLAN job with `recipe_step.outputs_required` missing `context_for_documents`, and asserts that `planAllToOne` throws an error indicating `context_for_documents` is required for PLAN jobs. This test must fail because the function currently doesn't validate this.
        *   `[âœ…]` 27.b.iii. Create a test case that mocks a PLAN job with `context_for_documents` entry missing `document_key`, and asserts that `planAllToOne` throws an error indicating `document_key` is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.iv. Create a test case that mocks a PLAN job with `context_for_documents` entry missing `content_to_include`, and asserts that `planAllToOne` throws an error indicating `content_to_include` object model is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.v. Create a test case that mocks an EXECUTE job with `recipe_step.outputs_required.files_to_generate` containing valid entries, and asserts that `planAllToOne` successfully creates a job payload. This test should pass if the function doesn't currently validate (it just doesn't check), but will need to pass after validation is added.
        *   `[âœ…]` 27.b.vi. Create a test case that mocks an EXECUTE job with `recipe_step.outputs_required` missing `files_to_generate`, and asserts that `planAllToOne` throws an error indicating `files_to_generate` is required for EXECUTE jobs. This test must fail because the function currently doesn't validate this.
        *   `[âœ…]` 27.b.vii. Create a test case that mocks an EXECUTE job with `files_to_generate` entry missing `from_document_key`, and asserts that `planAllToOne` throws an error indicating `from_document_key` is missing. This test must fail because the function currently doesn't validate structure.
        *   `[âœ…]` 27.b.viii. Create a test case that mocks an EXECUTE job with `files_to_generate` entry missing `template_filename`, and asserts that `planAllToOne` throws an error indicating `template_filename` is missing. This test must fail because the function currently doesn't validate structure.
    *   `[âœ…]` 27.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts`, add validation and passing of `context_for_documents` for PLAN jobs and validation of `files_to_generate` for EXECUTE jobs.
        *   `[âœ…]` 27.c.i. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file.
        *   `[âœ…]` 27.c.ii. Before creating `newPayload` (line 44), validate that `recipeStep.job_type` is either `'PLAN'` or `'EXECUTE'`. If it's neither, throw an error: `planAllToOne requires job_type to be 'PLAN' or 'EXECUTE', received: ${recipeStep.job_type}`.
        *   `[âœ…]` 27.c.iii. Add a check for `recipeStep.job_type === 'PLAN'`. If true, implement the validation pattern from Recipe-Planner-Gap-Analysis.md Fix 3 (lines 1192-1206): validate that `context_for_documents` exists, is an array, and has entries (`contextForDocuments.length > 0`); validate that each entry has `document_key` (non-empty string) and `content_to_include` object model (must be an object, not an array at top level - use `Array.isArray()` check and throw error if array); **validate `content_to_include` structure conforms to `ContentToInclude` type**: Import the `isContentToInclude` helper function from `../../_shared/utils/type-guards/type_guards.dialectic.ts` (defined in step 24.c.vii) and use it to validate that each `content_to_include` entry conforms to the type (object, not array at top level, all values are allowed types). Include `context_for_documents` in the job payload structure so `assemblePlannerPrompt` can access it. All error messages must be prefixed with 'planAllToOne requires...'. Ensure the validation happens before creating the job payload, so invalid PLAN jobs fail early.
        *   `[âœ…]` 27.c.iv. After the PLAN job validation, add a check for `recipeStep.job_type === 'EXECUTE'`. If true, implement the validation pattern from Recipe-Planner-Gap-Analysis.md Fix 4 (lines 1226-1240): validate that `files_to_generate` exists, is an array, and has entries (`filesToGenerate.length > 0`); validate that each entry has `from_document_key` (non-empty string) and `template_filename` (non-empty string). All error messages must be prefixed with 'planAllToOne requires...'. Ensure the validation happens before creating the job payload, so invalid EXECUTE jobs fail early.
        *   `[âœ…]` 27.c.v. Ensure the existing `document_key` extraction logic (lines 44-74) still works correctly and is not affected by the new validation logic.
        *   `[âœ…]` 27.c.vi. The `DialecticPlanJobPayload` interface was already updated in step 18.b.v to include `context_for_documents?: ContextForDocument[]`. Use this field when creating PLAN job payloads by adding `context_for_documents` to the payload object when `job_type === 'PLAN'`.
        *   `[âœ…]` 27.c.vii. **Fix 9: Validate PLAN â†” EXECUTE structure mapping**: When creating EXECUTE jobs, validate that each `files_to_generate[].from_document_key` matches a `document_key` in the PLAN step's `context_for_documents`. If the planner has access to the PLAN step's `context_for_documents` (via recipe step lookup or parent job context), add validation: for each `file` in `filesToGenerate`, check if `file.from_document_key` exists in the PLAN step's `context_for_documents[].document_key` array. If no match is found, throw an error: `planAllToOne requires files_to_generate[].from_document_key '${file.from_document_key}' to match a document_key in the PLAN step's context_for_documents`. **Note**: This validation requires access to the PLAN step's `context_for_documents`. If the planner doesn't have direct access (e.g., needs to fetch from database), this validation is deferred to `assembleTurnPrompt` where `header_context` is already loaded (as noted in Fix 9 line 1394). The validation in `assembleTurnPrompt` (step 25.c.ii) will catch any mismatches at runtime.
    *   `[âœ…]` 27.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 27.b and ensure they now pass. Also verify that existing `planAllToOne` tests still pass.
    *   `[âœ…]` 27.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.ts` and `supabase/functions/dialectic-worker/strategies/planners/planAllToOne.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 28. **`[BE]` Fix planPairwiseByOrigin to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 28.a. `[DEPS]` The `planPairwiseByOrigin` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 56-86) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPairwiseByOrigin requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 28.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPairwiseByOrigin"` instead of `"planAllToOne"`.
    *   `[âœ…]` 28.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPairwiseByOrigin requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 28.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 28.b and ensure they now pass. Also verify that existing `planPairwiseByOrigin` tests still pass.
    *   `[âœ…]` 28.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPairwiseByOrigin.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 29. **`[BE]` Fix planPerModel to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 29.a. `[DEPS]` The `planPerModel` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 76-106) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerModel requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 29.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerModel"` instead of `"planAllToOne"`.
    *   `[âœ…]` 29.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerModel requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 29.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 29.b and ensure they now pass. Also verify that existing `planPerModel` tests still pass.
    *   `[âœ…]` 29.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerModel.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerModel.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 30. **`[BE]` Fix planPerSourceDocument to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 30.a. `[DEPS]` The `planPerSourceDocument` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 66-96) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceDocument requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 30.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceDocument"` instead of `"planAllToOne"`.
    *   `[âœ…]` 30.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceDocument requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 30.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 30.b and ensure they now pass. Also verify that existing `planPerSourceDocument` tests still pass.
    *   `[âœ…]` 30.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocument.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 31. **`[BE]` Fix planPerSourceDocumentByLineage to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 31.a. `[DEPS]` The `planPerSourceDocumentByLineage` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 54-84) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceDocumentByLineage requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 31.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceDocumentByLineage"` instead of `"planAllToOne"`.
    *   `[âœ…]` 31.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceDocumentByLineage requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 31.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 31.b and ensure they now pass. Also verify that existing `planPerSourceDocumentByLineage` tests still pass.
    *   `[âœ…]` 31.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceDocumentByLineage.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 32. **`[BE]` Fix planPerSourceGroup to Validate and Pass context_for_documents for PLAN Jobs and Validate files_to_generate for EXECUTE Jobs, and Tests**
    *   `[âœ…]` 32.a. `[DEPS]` The `planPerSourceGroup` planner function in `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` needs the same updates as `planAllToOne` from step 27. **CRITICAL ARCHITECTURAL REQUIREMENT: PLAN steps ALWAYS include `context_for_documents` in `recipe_step.outputs_required`. This is NEVER conditional. EXECUTE steps ALWAYS include `documents` and `files_to_generate` in `recipe_step.outputs_required`. This is NEVER conditional.** The function already extracts `document_key` from `outputs_required.documents[0].document_key` (lines 55-85) and sets it in the payload. Import `ContextForDocument` from `../../dialectic-service/dialectic.interface.ts` at the top of the file. All error messages must be prefixed with 'planPerSourceGroup requires...' instead of 'planAllToOne requires...'. The test file `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` must be updated to test both PLAN and EXECUTE job validation. Test and source files must be edited in the same step per Instructions for Agent block.
    *   `[âœ…]` 32.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts`, write the same eight test cases as step 27.b (four for PLAN jobs, four for EXECUTE jobs), but with error messages prefixed with `"planPerSourceGroup"` instead of `"planAllToOne"`.
    *   `[âœ…]` 32.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts`, add the same validation pattern as step 27.c (27.c.i through 27.c.vii), but with error messages prefixed with `"planPerSourceGroup requires"` instead of `"planAllToOne requires"`. Use `'PLAN'` and `'EXECUTE'` (uppercase) for job_type comparisons. Validate empty arrays explicitly (`array.length > 0`). Validate that `content_to_include` is an object, not an array at top level. Include `content_to_include` structure validation using the same `isContentToInclude` helper as step 27.c.iii.
    *   `[âœ…]` 32.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 32.b and ensure they now pass. Also verify that existing `planPerSourceGroup` tests still pass.
    *   `[âœ…]` 32.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.ts` and `supabase/functions/dialectic-worker/strategies/planners/planPerSourceGroup.test.ts` and resolve any warnings or errors.

*   `[ ]` 33. **`[TEST-INT]` Fix Integration Test to Verify Complete Cross-Document Coordination Flow**
    *   `[ ]` 33.a. `[DEPS]` After all planners are updated (steps 27-32), `assembleTurnPrompt` is fixed (step 25), and `assemblePlannerPrompt` is fixed (step 26), the existing integration test file `supabase/integration_tests/services/planner_output_type.integration.test.ts` is incorrect and incomplete. Update this test to verify the complete PLAN â†’ EXECUTE cross-document coordination flow: (1) PLAN job receives `context_for_documents` from recipe step, (2) PLAN job generates `header_context` with filled `content_to_include` objects, (3) EXECUTE jobs consume the `header_context` and use alignment details, (4) documents generated in parallel are aligned using the same `header_context`. The test must verify that `files_to_generate` is read from recipe step (not header context) and that `context_for_documents` alignment details are used correctly.
    *   `[ ]` 33.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts`, fix and expand the test to verify the complete cross-document coordination flow.
        *   `[ ]` 33.b.i. Update the existing test that verifies `document_key` flow to also verify that `assembleTurnPrompt` reads `files_to_generate` from `stage.recipe_step.outputs_required.files_to_generate` (not from `headerContext.files_to_generate`).
        *   `[ ]` 33.b.ii. Update the test to verify that `headerContext.context_for_documents` contains filled `content_to_include` objects (not empty models), and that `assembleTurnPrompt` uses these alignment details in the prompt context.
        *   `[ ]` 33.b.iii. Add a test case that verifies the complete PLAN â†’ EXECUTE flow: create a PLAN job, verify it generates `header_context` with filled `content_to_include`, create EXECUTE jobs that consume that `header_context`, and verify that `assembleTurnPrompt` successfully matches `from_document_key` to `document_key` and uses the alignment details.
        *   `[ ]` 33.b.iv. Add a test case that verifies structure matching: ensure that `files_to_generate[].from_document_key` values match `context_for_documents[].document_key` values, and that `content_to_include` structures match between PLAN and EXECUTE steps.
        *   `[ ]` 33.b.v. Remove or fix any incorrect test code that assumes `files_to_generate` is in `headerContext` (the test incorrectly creates `headerContext` with `files_to_generate` - this should be removed or updated to match the correct architecture where `files_to_generate` is in recipe step).
    *   `[ ]` 33.c. `[TEST-INT]` **GREEN**: Re-run all tests from step 33.b and ensure they now pass. This verifies that the complete end-to-end cross-document coordination flow works correctly: PLAN jobs generate `header_context` with alignment details, EXECUTE jobs consume it correctly, and documents are aligned.
    *   `[ ]` 33.d. `[LINT]` Run the linter for `supabase/integration_tests/services/planner_output_type.integration.test.ts` and resolve any warnings or errors.
    *   `[ ]` 33.e. `[CRITERIA]` The integration test verifies: (1) PLAN jobs generate `header_context` with `context_for_documents` containing filled `content_to_include` objects, (2) EXECUTE jobs read `files_to_generate` from recipe step (not header context), (3) `assembleTurnPrompt` matches `from_document_key` to `document_key` and uses alignment details from `context_for_documents`, (4) structure matching is validated at runtime, (5) documents generated in parallel use the same alignment details from `header_context`.

*   `[âœ…]` 34. **`[CRITERIA]` Verify All Gaps Are Resolved**
    *   `[âœ…]` 34.a. `[DEPS]` According to Recipe-Planner-Gap-Analysis.md, all identified gaps must be resolved. This step verifies that the implementation addresses every gap.
    *   `[âœ…]` 34.b. `[CRITERIA]` Verify that all gaps from Recipe-Planner-Gap-Analysis.md are resolved.
        *   `[âœ…]` 34.b.i. **Gap 1 (Location Mismatch)**: Verify `assembleTurnPrompt` reads `files_to_generate` from recipe step, not header context. âœ… Resolved by step 25.
        *   `[âœ…]` 34.b.ii. **Gap 2 (Field Name Mismatch)**: Verify `assembleTurnPrompt` uses `from_document_key` (not `document_key`) when matching. âœ… Resolved by step 25.
        *   `[âœ…]` 34.b.iii. **Gap 3 (Inconsistent Recipe Definitions)**: Verify Synthesis PLAN steps don't have `files_to_generate`, and Synthesis EXECUTE steps do have `files_to_generate`. âœ… Resolved by step 21.
        *   `[âœ…]` 34.b.iv. **Gap 4 (Missing Validation)**: Verify planners validate `context_for_documents` for PLAN jobs and `files_to_generate` for EXECUTE jobs. âœ… Resolved by steps 27-32.
        *   `[âœ…]` 34.b.v. **Gap 5 (Missing HeaderContext Type)**: Verify `HeaderContext` interface is defined and exported. âœ… Resolved by step 18.
        *   `[âœ…]` 34.b.vi. **Gap 6 (Header Context Generation)**: Verify PLAN jobs generate `header_context` with `context_for_documents` (not `files_to_generate`). âœ… Resolved by steps 21, 26.
        *   `[âœ…]` 34.b.vii. **Gap 7 (Inconsistent PLAN Step Structure)**: Verify Antithesis `non_functional_requirements` uses object structure. âœ… Resolved by step 20.
        *   `[âœ…]` 34.b.viii. **Gap 8 (Inconsistent EXECUTE Step Structure)**: Verify Paralysis EXECUTE step has `content_to_include`. âœ… Resolved by step 23.
        *   `[âœ…]` 34.b.ix. **Gap 9 (PLAN â†” EXECUTE Structure Mapping)**: Verify `context_for_documents[].document_key` matches `files_to_generate[].from_document_key` and structures match. âœ… Resolved by steps 19, 20, 21, 22, 23.
        *   `[âœ…]` 34.b.x. **Gap 10 (Missing Type Definitions)**: Verify `ContentToInclude` and `HeaderContext` types are defined. âœ… Resolved by step 18.

*   `[âœ…]` 35. **`[COMMIT]` Commit All Changes with Comprehensive Message**
    *   `[âœ…]` 35.a. `[DEPS]` After all work is complete, tested, and linted, commit all changes with a comprehensive commit message that describes all fixes.
    *   `[âœ…]` 35.b. `[COMMIT]` Commit message: "fix: resolve recipe-planner data architecture gaps for cross-document coordination - Define ContentToInclude type for consistent content_to_include structures across all stages - Define HeaderContext interface matching actual PLAN job output structure (no files_to_generate) - Update ContextForDocument to use ContentToInclude type - Fix all 5 stage migrations: verify and fix structure matching between PLAN and EXECUTE steps (context_for_documents[].document_key matches files_to_generate[].from_document_key, content_to_include structures match exactly) - Fix Synthesis stage migration: remove files_to_generate from PLAN steps, add to EXECUTE steps - Fix Antithesis stage migration: change non_functional_requirements from array to object structure - Fix Paralysis stage migration: add content_to_include to actionable_checklist EXECUTE step - Update isHeaderContext type guard to match new HeaderContext interface (remove files_to_generate validation) - Fix assembleTurnPrompt to read files_to_generate from recipe step (not header context) and use from_document_key - Fix assembleTurnPrompt to extract alignment details from header_context.context_for_documents - Fix assemblePlannerPrompt to include context_for_documents in PLAN prompts - Update all six planners to validate and pass context_for_documents for PLAN jobs - Update all six planners to validate files_to_generate for EXECUTE jobs - Add comprehensive unit tests for all changes - Fix integration test to verify complete cross-document coordination flow - Resolves all 10 gaps identified in Recipe-Planner-Gap-Analysis.md"


*   `[âœ…]` 36. **`[PROMPT]` Fix All Prompt Templates and Document Templates to Exactly Match Database Structures**
    *   `[âœ…]` 36.a. `[DEPS]` The prompt template files in `docs/prompts/` and document template files in `docs/templates/` are loaded from storage and used by `assemblePlannerPrompt` and `assembleTurnPrompt`. These files must EXACTLY match the database structures defined in the stage migration files (`supabase/migrations/*_stage.sql`). The database migration files define the expected structure for `context_for_documents`, `content_to_include`, `files_to_generate`, and document templates. Any mismatch between the file-based templates and database structures causes PLAN jobs to generate invalid `header_context` artifacts (as seen in the investigation where `feature_spec.content_to_include` was an array instead of an object). Every prompt template file must be FIXED to match its corresponding database structure in the migration file. Every document template file must be FIXED to match its corresponding database structure. The database migration files are the source of truth. The file-based templates must be corrected to exactly match. This correction ensures that the AI agent receives instructions that match the validated type structures, preventing structural validation failures at runtime. Each step must: (1) read the database structure from the migration file, (2) read the corresponding prompt/template file, (3) UPDATE the file to exactly match the database structure, (4) ensure all `content_to_include` structures are objects (not arrays at top level), (5) ensure all `document_key` values match, (6) ensure all `template_filename` values match.
    *   `[âœ…]` 36.b. **`[PROMPT]` Fix Thesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.b.i. `[DEPS]` The thesis stage planner prompt template file is `docs/prompts/thesis/thesis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194531_thesis_stage.sql` defines the expected `context_for_documents` structure for the PLAN step (lines 201-264). The planner prompt template contains a JSON schema example showing the expected `HeaderContext` structure, including `context_for_documents` array with `content_to_include` objects for each document. The template's schema example must EXACTLY match the database structure. The investigation identified that the template file currently shows `feature_spec.content_to_include` as an array (lines 84-90), but the database expects an object structure with `{"features": [...]}`. The database structure is the source of truth and the template must be corrected to match it exactly.
        *   `[âœ…]` 36.b.ii. `[PROMPT]` Read `supabase/migrations/20251006194531_thesis_stage.sql` PLAN step `context_for_documents` structure (lines 201-264) to get the exact database structure for all four documents: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`.
        *   `[âœ…]` 36.b.iii. `[PROMPT]` Read `docs/prompts/thesis/thesis_planner_header_v1.md` and locate the `HeaderContext` schema example (lines 36-111) that shows the JSON structure with `context_for_documents` array.
        *   `[âœ…]` 36.b.iv. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to fix `feature_spec.content_to_include`: change from array structure `[{...}]` (lines 84-90) to object structure `{"features": [{"feature_name": "", "feature_objective": "", "user_stories": [], "acceptance_criteria": [], "dependencies": [], "success_metrics": []}]}` exactly matching the database structure (lines 221-232).
        *   `[âœ…]` 36.b.v. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `business_case.content_to_include` matches the database structure exactly: include all fields (market_opportunity, user_problem_validation, competitive_analysis, differentiation_&_value_proposition, risks_&_mitigation, strengths, weaknesses, opportunities, threats, next_steps, proposal_references, executive_summary) with exact field names from database (lines 204-217).
        *   `[âœ…]` 36.b.vi. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `technical_approach.content_to_include` matches the database structure exactly: include all fields (architecture, components, data, deployment, sequencing, risk_mitigation, open_questions) with exact field names from database (lines 236-244).
        *   `[âœ…]` 36.b.vii. `[PROMPT]` Update `docs/prompts/thesis/thesis_planner_header_v1.md` to ensure `success_metrics.content_to_include` matches the database structure exactly: include all fields (outcome_alignment, north_star_metric, primary_kpis, leading_indicators, lagging_indicators, guardrails, measurement_plan, risk_signals, next_steps, data_sources, reporting_cadence, ownership, escalation_plan) with exact field names from database (lines 248-262).
        *   `[âœ…]` 36.b.viii. `[PROMPT]` Verify all `content_to_include` structures in the updated template are objects (not arrays at top level). Ensure the JSON schema example in the template exactly reflects the database structure. Ensure proper JSON formatting and indentation is maintained.
    *   `[âœ…]` 36.c. **`[PROMPT]` Fix Thesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.c.i. `[DEPS]` The thesis stage has four turn prompt template files: `docs/prompts/thesis/thesis_business_case_turn_v1.md`, `docs/prompts/thesis/thesis_feature_spec_turn_v1.md`, `docs/prompts/thesis/thesis_technical_approach_turn_v1.md`, `docs/prompts/thesis/thesis_success_metrics_turn_v1.md`. These turn prompts are used by `assembleTurnPrompt` for EXECUTE jobs. The database migration file defines the expected document structures in EXECUTE steps (lines 323-891). Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions. The database structure is the source of truth.
        *   `[âœ…]` 36.c.ii. `[PROMPT]` Read each thesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194531_thesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.d. **`[PROMPT]` Fix Thesis Stage Document Templates**
        *   `[âœ…]` 36.d.i. `[DEPS]` The thesis stage has four document template files: `docs/templates/thesis/thesis_business_case.md`, `docs/templates/thesis/thesis_feature_spec.md`, `docs/templates/thesis/thesis_technical_approach.md`, `docs/templates/thesis/thesis_success_metrics.md`. These document templates are referenced by `template_filename` in the database EXECUTE steps. The database migration file defines the expected `template_filename` values and `documents[].content_to_include` structures for each document. Template file content structure must exactly match the expected document structure from database.
        *   `[âœ…]` 36.d.ii. `[PROMPT]` In `supabase/migrations/20251006194531_thesis_stage.sql`, identify all `template_filename` values in EXECUTE steps. Ensure corresponding files exist in `docs/templates/thesis/`. If any files are missing, create them with the correct structure.
        *   `[âœ…]` 36.d.iii. `[PROMPT]` Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document from the EXECUTE step. Templates may use placeholder syntax (e.g., `{{field_name}}`) that must map to fields in `content_to_include`. Ensure all field names match exactly with the database structure.
        *   `[âœ…]` 36.d.iv. `[PROMPT]` Update each document template file to ensure the template structure exactly aligns with the database structure. If templates reference fields, ensure all field names match the database `content_to_include` structure exactly.
    *   `[âœ…]` 36.e. **`[PROMPT]` Fix Antithesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.e.i. `[DEPS]` The antithesis stage planner prompt template file is `docs/prompts/antithesis/antithesis_planner_review_v1.md`. The database migration file `supabase/migrations/20251006194542_antithesis_stage.sql` defines the expected `context_for_documents` structure for the PLAN step. According to step 20 of this checklist, `non_functional_requirements.content_to_include` was changed from an array to an object structure `{"categories": [...]}` in the database. The planner prompt template must match this corrected structure exactly.
        *   `[âœ…]` 36.e.ii. `[PROMPT]` Read `supabase/migrations/20251006194542_antithesis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents.
        *   `[âœ…]` 36.e.iii. `[PROMPT]` Read `docs/prompts/antithesis/antithesis_planner_review_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.e.iv. `[PROMPT]` Update `docs/prompts/antithesis/antithesis_planner_review_v1.md` to fix `non_functional_requirements.content_to_include`: change from array structure `[...]` to object structure `{"categories": [...]}` exactly matching the database structure.
        *   `[âœ…]` 36.e.v. `[PROMPT]` Update `docs/prompts/antithesis/antithesis_planner_review_v1.md` to ensure all other `context_for_documents` entries match database structures exactly: `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `dependency_map`, `comparison_vector`. Ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.f. **`[PROMPT]` Fix Antithesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.f.i. `[DEPS]` The antithesis stage has six turn prompt template files: `docs/prompts/antithesis/antithesis_business_case_critique_turn_v1.md`, `docs/prompts/antithesis/antithesis_feasibility_assessment_turn_v1.md`, `docs/prompts/antithesis/antithesis_risk_register_turn_v1.md`, `docs/prompts/antithesis/antithesis_non_functional_requirements_turn_v1.md`, `docs/prompts/antithesis/antithesis_dependency_map_turn_v1.md`, `docs/prompts/antithesis/antithesis_comparison_vector_turn_v1.md`. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.f.ii. `[PROMPT]` Read each antithesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194542_antithesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.g. **`[PROMPT]` Fix Antithesis Stage Document Templates**
        *   `[âœ…]` 36.g.i. `[DEPS]` The antithesis stage has six document template files: `docs/templates/antithesis/antithesis_business_case_critique.md`, `docs/templates/antithesis/antithesis_feasibility_assessment.md`, `docs/templates/antithesis/antithesis_risk_register.md`, `docs/templates/antithesis/antithesis_non_functional_requirements.md`, `docs/templates/antithesis/antithesis_dependency_map.md`, `docs/templates/antithesis/antithesis_comparison_vector.json`. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.g.ii. `[PROMPT]` In `supabase/migrations/20251006194542_antithesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.h. **`[PROMPT]` Fix Synthesis Stage Planner Prompt Templates**
        *   `[âœ…]` 36.h.i. `[DEPS]` The synthesis stage has two planner prompt template files: `docs/prompts/synthesis/synthesis_pairwise_header_planner_v1.md` (for pairwise planning) and `docs/prompts/synthesis/synthesis_final_header_planner_v1.md` (for final header planning). According to step 21 of this checklist, PLAN steps should NOT have `files_to_generate` (it was removed from database). The database migration file `supabase/migrations/20251006194549_synthesis_stage.sql` defines the expected structures. Planner prompt templates must not reference `files_to_generate` in their schema examples, and `context_for_documents` structures must match database definitions exactly.
        *   `[âœ…]` 36.h.ii. `[PROMPT]` Read `supabase/migrations/20251006194549_synthesis_stage.sql` PLAN step `context_for_documents` structure for pairwise planning to get the exact database structure.
        *   `[âœ…]` 36.h.iii. `[PROMPT]` Read `docs/prompts/synthesis/synthesis_pairwise_header_planner_v1.md` and locate the `HeaderContext` schema example. Remove any `files_to_generate` references if present. Update all `context_for_documents` entries to exactly match the database PLAN step structure.
        *   `[âœ…]` 36.h.iv. `[PROMPT]` Read `supabase/migrations/20251006194549_synthesis_stage.sql` PLAN step `context_for_documents` structure for final header planning to get the exact database structure.
        *   `[âœ…]` 36.h.v. `[PROMPT]` Read `docs/prompts/synthesis/synthesis_final_header_planner_v1.md` and locate the `HeaderContext` schema example. Remove any `files_to_generate` references if present. Update all `context_for_documents` entries to exactly match the database PLAN step structure.
    *   `[âœ…]` 36.i. **`[PROMPT]` Fix Synthesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.i.i. `[DEPS]` The synthesis stage has thirteen turn prompt template files covering pairwise, document-level, and final deliverables. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.i.ii. `[PROMPT]` Read each synthesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194549_synthesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.j. **`[PROMPT]` Fix Synthesis Stage Document Templates**
        *   `[âœ…]` 36.j.i. `[DEPS]` The synthesis stage has thirteen document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.j.ii. `[PROMPT]` In `supabase/migrations/20251006194549_synthesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.k. **`[PROMPT]` Fix Parenthesis Stage Planner Prompt Template**
        *   `[âœ…]` 36.k.i. `[DEPS]` The parenthesis stage planner prompt template file is `docs/prompts/parenthesis/parenthesis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194558_parenthesis_stage.sql` defines the expected `context_for_documents` structure. The planner prompt template must match the database structure exactly.
        *   `[âœ…]` 36.k.ii. `[PROMPT]` Read `supabase/migrations/20251006194558_parenthesis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents: `technical_requirements`, `master_plan`, `milestone_schema`.
        *   `[âœ…]` 36.k.iii. `[PROMPT]` Read `docs/prompts/parenthesis/parenthesis_planner_header_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.k.iv. `[PROMPT]` Update `docs/prompts/parenthesis/parenthesis_planner_header_v1.md` to ensure all `context_for_documents` entries exactly match database structures: ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.l. **`[PROMPT]` Fix Parenthesis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.l.i. `[DEPS]` The parenthesis stage has four turn prompt template files. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.l.ii. `[PROMPT]` Read each parenthesis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194558_parenthesis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.m. **`[PROMPT]` Fix Parenthesis Stage Document Templates**
        *   `[âœ…]` 36.m.i. `[DEPS]` The parenthesis stage has three document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.m.ii. `[PROMPT]` In `supabase/migrations/20251006194558_parenthesis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.n. **`[PROMPT]` Fix Paralysis Stage Planner Prompt Template**
        *   `[âœ…]` 36.n.i. `[DEPS]` The paralysis stage planner prompt template file is `docs/prompts/paralysis/paralysis_planner_header_v1.md`. The database migration file `supabase/migrations/20251006194605_paralysis_stage.sql` defines the expected `context_for_documents` structure. According to step 23 of this checklist, the EXECUTE step for `actionable_checklist` was updated to include `content_to_include` in the database. The planner prompt template must match the database structure exactly.
        *   `[âœ…]` 36.n.ii. `[PROMPT]` Read `supabase/migrations/20251006194605_paralysis_stage.sql` PLAN step `context_for_documents` structure to get the exact database structure for all documents: `actionable_checklist`, `updated_master_plan`, `advisor_recommendations`.
        *   `[âœ…]` 36.n.iii. `[PROMPT]` Read `docs/prompts/paralysis/paralysis_planner_header_v1.md` and locate the `HeaderContext` schema example.
        *   `[âœ…]` 36.n.iv. `[PROMPT]` Update `docs/prompts/paralysis/paralysis_planner_header_v1.md` to ensure all `context_for_documents` entries exactly match database structures: ensure all `document_key` values match and all `content_to_include` structures are objects (not arrays at top level), matching the database structure exactly.
    *   `[âœ…]` 36.o. **`[PROMPT]` Fix Paralysis Stage Turn Prompt Templates**
        *   `[âœ…]` 36.o.i. `[DEPS]` The paralysis stage has four turn prompt template files. Each turn prompt template must not contain hardcoded structure examples that conflict with database definitions.
        *   `[âœ…]` 36.o.ii. `[PROMPT]` Read each paralysis turn prompt template file. If any contain hardcoded structure examples that conflict with database definitions, remove or update them to match the database EXECUTE step structures from `supabase/migrations/20251006194605_paralysis_stage.sql`. Ensure all structure references align with database definitions.
    *   `[âœ…]` 36.p. **`[PROMPT]` Fix Paralysis Stage Document Templates**
        *   `[âœ…]` 36.p.i. `[DEPS]` The paralysis stage has three document template files. All `template_filename` values in database must match actual files, and template structures must align with database `documents[].content_to_include` structures.
        *   `[âœ…]` 36.p.ii. `[PROMPT]` In `supabase/migrations/20251006194605_paralysis_stage.sql`, identify all `template_filename` values. Ensure corresponding files exist. Read each document template file and update its structure to exactly match the database `documents[].content_to_include` structure for that document. Ensure all field names match exactly with the database structure.
    *   `[âœ…]` 36.q. **`[PROMPT]` Upload Updated Prompt and Document Templates to Storage**
        *   `[âœ…]` 36.q.i. `[DEPS]` After all prompt template files and document template files have been fixed to match database structures, they must be uploaded to storage using the `seed_prompt_templates.ts` script. The script uploads files from `docs/prompts/` and `docs/templates/` to the storage bucket referenced by `dialectic_document_templates` table. This ensures that when `assemblePlannerPrompt` and `assembleTurnPrompt` load templates from storage, they receive the corrected versions that match database structures.
        *   `[âœ…]` 36.q.ii. `[PROMPT]` Run the `seed_prompt_templates.ts` script to upload all updated prompt and document templates to storage. Ensure the script completes successfully and that all files are uploaded to the correct storage paths as defined in the database `dialectic_document_templates` table.
    *   `[âœ…]` 36.r. `[CRITERIA]` All prompt template files and document template files EXACTLY match their corresponding database structures in the stage migration files. All `content_to_include` structures are objects (not arrays at top level). All `document_key` values match between templates and database. All `template_filename` values match between database and actual files. All updated templates have been uploaded to storage. No structural mismatches exist between file-based templates and database definitions.

*   `[âœ…]` 37. **`[STORE]` Fix Notification Store Payload Extraction and Validation to Systematically Match Type Definitions**
    *   `[âœ…]` 37.a. `[DEPS]` The `handleIncomingNotification` function in `packages/store/src/notificationStore.ts` has systematic architectural flaws that cause notifications to be silently ignored or incorrectly processed: (1) **Inconsistent payload extraction**: Event payload construction (lines 76-269) does not systematically extract all optional fields defined in type definitions. For example, `document_started` (lines 152-171) does not extract `latestRenderedResourceId` even though `DocumentStartedPayload` extends `DocumentLifecyclePayload` which includes `latestRenderedResourceId?: string | null` (defined in `packages/types/src/dialectic.types.ts` line 765), while `document_completed` (lines 196-216) correctly extracts it (line 214). This inconsistency causes `handleDocumentStartedLogic` in `packages/store/src/dialecticStore.documents.ts` (lines 500-508) to ignore events for documents requiring rendering when `latestRenderedResourceId` is missing, even though the backend may have sent it in the notification data. (2) **Validation failures still add notifications**: For `WALLET_TRANSACTION` events (lines 285-296), when validation fails (line 287 check fails), the function logs a warning (line 294) but still adds the notification to the list (line 298) without calling the wallet handler (line 289), causing invalid notifications to appear in the UI while the actual wallet update is ignored. The backend sends `WALLET_TRANSACTION` notifications with `walletId` and `newBalance` in the data field (see `supabase/functions/_shared/services/tokenWalletService.ts` lines 312-318), but the validation may fail if the data structure is malformed or if fields are missing. (3) **No systematic type-to-extraction mapping**: There is no systematic approach ensuring that all fields defined in type definitions (`packages/types/src/dialectic.types.ts` and `supabase/functions/_shared/types/notification.service.types.ts`) are extracted from notification data during payload construction. Each event type has ad-hoc validation and extraction logic, leading to inconsistencies where some optional fields are extracted for some events but not others, even when they share the same base type. The fix requires: (1) creating a systematic payload extraction pattern that extracts ALL optional fields defined in type definitions for each event type, (2) ensuring validation failures prevent invalid notifications from being added to the notification list, (3) documenting the mapping between type definitions and extraction logic to prevent future inconsistencies, (4) fixing all existing event types to extract all optional fields from their type definitions.
    *   `[âœ…]` 37.b. `[TYPES]` Audit all notification event type definitions to create a complete mapping of required and optional fields.
        *   `[âœ…]` 37.b.i. Read `packages/types/src/dialectic.types.ts` and identify all `DialecticLifecycleEvent` union members and their type definitions: `ContributionGenerationStartedPayload`, `DialecticContributionStartedPayload`, `ContributionGenerationRetryingPayload`, `DialecticContributionReceivedPayload`, `ContributionGenerationFailedPayload`, `ContributionGenerationContinuedPayload`, `ContributionGenerationCompletePayload`, `DialecticProgressUpdatePayload`, `PlannerStartedPayload`, `DocumentStartedPayload`, `DocumentChunkCompletedPayload`, `DocumentCompletedPayload`, `RenderCompletedPayload`, `JobFailedPayload`. Document all required fields and all optional fields (marked with `?`) for each type.
        *   `[âœ…]` 37.b.ii. Read `supabase/functions/_shared/types/notification.service.types.ts` and identify all notification payload type definitions, including `DocumentPayload` base interface and all extending interfaces. Document all required and optional fields.
        *   `[âœ…]` 37.b.iii. Create a comprehensive mapping document (or inline comments) that lists for each event type: (a) the type definition location, (b) all required fields, (c) all optional fields, (d) the current extraction status in `notificationStore.ts` (which fields are extracted, which are missing), (e) the validation requirements. This mapping will serve as the specification for the systematic fix.
        *   `[âœ…]` 37.b.iv. Identify all base types and interfaces that are extended by multiple event types (e.g., `DocumentLifecyclePayload` is extended by `DocumentStartedPayload`, `DocumentCompletedPayload`, `DocumentChunkCompletedPayload`, `RenderCompletedPayload`, `JobFailedPayload`). Document which optional fields from base types are currently extracted inconsistently across extending types.
    *   `[âœ…]` 37.c. `[TEST-UNIT]` **RED**: In `packages/store/src/notificationStore.test.ts`, write comprehensive tests that verify all optional fields are extracted from notification data for all event types.
        *   `[âœ…]` 37.c.i. For each event type in the `DialecticLifecycleEvent` union, create a test case that mocks a notification with ALL optional fields present in the notification data (as sent by the backend), and assert that the constructed `eventPayload` includes all optional fields that are defined in the type definition. For example, for `document_started`, create a test that mocks `notification.data` with `latestRenderedResourceId: "resource-123"` and assert that `eventPayload.latestRenderedResourceId === "resource-123"`. These tests must fail because the current implementation does not extract all optional fields.
        *   `[âœ…]` 37.c.ii. Create test cases that verify validation failures prevent invalid notifications from being added to the notification list. For `WALLET_TRANSACTION`, create a test that mocks a notification with invalid data (missing `walletId` or `newBalance`), and assert that: (a) the wallet handler is NOT called, (b) the notification is NOT added to the notification list (verify `addNotification` is not called or the notification list does not contain the invalid notification), (c) an error is logged. This test must fail because the current implementation adds invalid notifications to the list.
        *   `[âœ…]` 37.c.iii. Create test cases that verify optional fields are correctly extracted when present and set to `undefined` when missing (not `null` unless the type allows `null`). For each event type with optional fields, create two test cases: (1) notification data includes the optional field, assert it's extracted, (2) notification data omits the optional field, assert it's `undefined` in the payload (or `null` if the type allows it).
        *   `[âœ…]` 37.c.iv. Create test cases that verify base type optional fields are extracted consistently across all extending types. For example, verify that `latestRenderedResourceId` is extracted for `document_started`, `document_completed`, `document_chunk_completed`, and `render_completed` (all extend `DocumentLifecyclePayload`). This test must fail because `document_started` currently does not extract `latestRenderedResourceId`.
    *   `[âœ…]` 37.d. `[STORE]` **GREEN**: In `packages/store/src/notificationStore.ts`, systematically fix payload extraction to match type definitions and fix validation failure handling.
        *   `[âœ…]` 37.d.i. For each event type case in the switch statement (lines 76-269), update the payload construction to extract ALL optional fields defined in the corresponding type definition. Use the mapping from step 37.b.iii as the specification. For example, for `document_started` (lines 152-171), add extraction of `latestRenderedResourceId` after line 169: `latestRenderedResourceId: typeof data['latestRenderedResourceId'] === 'string' ? data['latestRenderedResourceId'] : (data['latestRenderedResourceId'] === null ? null : undefined),` (matching the pattern used in `document_completed` line 214, but also handling `null` if the type allows it). Ensure the extraction logic matches the type definition exactly (if the type allows `string | null | undefined`, extract accordingly; if it only allows `string | undefined`, do not set `null`).
        *   `[âœ…]` 37.d.ii. For `document_chunk_completed` (lines 173-195), verify that all optional fields from `DocumentChunkCompletedPayload` are extracted: `step_key`, `isFinalChunk`, `continuationNumber`, and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.iii. For `planner_started` (lines 131-151), verify that all optional fields from `PlannerStartedPayload` are extracted: `step_key` and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.iv. For `job_failed` (lines 241-263), verify that all optional fields from `JobFailedPayload` are extracted: `step_key` and `latestRenderedResourceId` (from base `DocumentLifecyclePayload`). Add extraction for `latestRenderedResourceId` if missing.
        *   `[âœ…]` 37.d.v. For `contribution_generation_retrying` (lines 87-91), verify that all optional fields are extracted. The current implementation extracts `error` as optional (line 89), which appears correct, but verify against the type definition.
        *   `[âœ…]` 37.d.vi. For `contribution_generation_failed` (lines 103-107), verify that all optional fields are extracted: `job_id` and `modelId` are already extracted as optional (line 105), which appears correct, but verify against the type definition.
        *   `[âœ…]` 37.d.vii. For `WALLET_TRANSACTION` handling (lines 285-296), fix the validation failure behavior: when validation fails (line 287 check fails), do NOT call `get().addNotification(notification)` (line 298). Instead, log an error (not just a warning) with the invalid data structure, and return early without adding the notification to the list. The notification should only be added to the list if validation passes AND the wallet handler is called (or if the wallet handler call fails, still add it but log the failure). This ensures invalid notifications do not appear in the UI while the actual update is ignored.
        *   `[âœ…]` 37.d.viii. Add comprehensive JSDoc comments above the switch statement (around line 75) documenting the systematic approach: "This switch statement constructs type-safe payloads from notification data. Each case must extract ALL optional fields defined in the corresponding type definition. Base type optional fields (e.g., `latestRenderedResourceId` from `DocumentLifecyclePayload`) must be extracted consistently across all extending types. Validation failures must prevent invalid notifications from being added to the notification list."
        *   `[âœ…]` 37.d.ix. Verify that all extraction logic uses the same pattern: `fieldName: typeof data['fieldName'] === 'expectedType' ? data['fieldName'] : (data['fieldName'] === null && typeAllowsNull ? null : undefined)`. This ensures type safety and consistent handling of missing vs null vs undefined values.
    *   `[âœ…]` 37.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 37.c and ensure they now pass. Also verify that existing `notificationStore` tests still pass (update any tests that mock notifications to include all optional fields if they test payload construction).
    *   `[âœ…]` 37.f. `[TEST-INT]` **RED**: In `packages/store/src/dialecticStore.notifications.integration.test.ts` or create a new integration test file, write tests that verify end-to-end notification handling with optional fields.
        *   `[âœ…]` 37.f.i. Create a test case that simulates a `document_started` notification with `latestRenderedResourceId` in the notification data, and verify that `handleDocumentStartedLogic` receives the `latestRenderedResourceId` and processes the event correctly (does not ignore it). This test must fail initially if `document_started` extraction is not fixed, but should pass after step 37.d.i.
        *   `[âœ…]` 37.f.ii. Create a test case that simulates a `WALLET_TRANSACTION` notification with invalid data (missing `walletId`), and verify that: (a) the notification is NOT added to the notification list, (b) the wallet handler is NOT called, (c) an error is logged. This test must fail initially because invalid notifications are currently added to the list.
        *   `[âœ…]` 37.f.iii. Create a test case that simulates a `WALLET_TRANSACTION` notification with valid data, and verify that: (a) the wallet handler IS called with the correct data, (b) the notification IS added to the notification list. This test should pass initially but must continue to pass after the validation fix.
    *   `[âœ…]` 37.g. `[TEST-INT]` **GREEN**: Re-run all tests from step 37.f and ensure they now pass. This verifies that the end-to-end notification flow works correctly with the systematic payload extraction and validation fixes.
    *   `[âœ…]` 37.h. `[LINT]` Run the linter for `packages/store/src/notificationStore.ts` and all test files, and resolve any warnings or errors.
    *   `[âœ…]` 37.i. `[CRITERIA]` All notification event types extract ALL optional fields defined in their type definitions. Base type optional fields (e.g., `latestRenderedResourceId` from `DocumentLifecyclePayload`) are extracted consistently across all extending types. Validation failures prevent invalid notifications from being added to the notification list. All optional fields are extracted using a consistent pattern that handles `undefined`, `null`, and missing values according to type definitions. The mapping between type definitions and extraction logic is documented and systematic, preventing future inconsistencies. All tests pass, including tests that verify optional field extraction and validation failure handling.

*   `[âœ…]` 38. **`[UI]` Fix SessionContributionsDisplayCard to Filter Documents Based on StageRunChecklist Highlighting**
    *   `[âœ…]` 38.a. `[DEPS]` The `SessionContributionsDisplayCard` component in `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` currently renders ALL documents from `documentGroups` (lines 552-647) as Card components without filtering. However, it should only render cards for documents that are highlighted in `StageRunChecklist`. A document is highlighted in `StageRunChecklist` when `focusedStageDocumentMap[focusKey]?.documentKey === entry.documentKey`, where `focusKey = `${sessionId}:${stageSlug}:${modelId}`` (as determined in `apps/web/src/components/dialectic/StageRunChecklist.tsx` lines 349-358). The highlighting logic is currently duplicated: `buildFocusedDocumentKey` is a private function in `StageRunChecklist.tsx` (lines 75-76), and the highlighting check is inline (lines 349-358). To ensure DRY compliance and consistent behavior, the highlighting logic must be extracted to a shared utility in `@paynless/utils` package. The component needs to: (1) access `focusedStageDocument` from the store (similar to how `GeneratedContributionCard` accesses it at line 95), (2) use the shared utility function to filter `documentGroups` to only include documents that are highlighted, (3) ensure documents that are not highlighted are not rendered at all (not just hidden). The shared utility functions (`buildFocusedDocumentKey` and `isDocumentHighlighted`) are pure business logic functions, not UI-specific, so they belong in `packages/utils/src/dialecticUtils.ts` and must be exported from `packages/utils/src/index.ts`. Both `StageRunChecklist.tsx` and `SessionContributionsDisplayCard.tsx` will import from `@paynless/utils`. The type `FocusedStageDocumentState` is already defined in `packages/types/src/dialectic.types.ts` (lines 470-473) and exported, so no type changes are required.
    *   `[âœ…]` 38.b. `[TYPES]` Verify that `FocusedStageDocumentState` type is available from `@paynless/types` package. The type is defined in `packages/types/src/dialectic.types.ts` (lines 470-473) as `export interface FocusedStageDocumentState { modelId: string; documentKey: string; }`. Confirm it is exported from `packages/types/src/index.ts` (or equivalent export file). If not exported, add export. No type changes are required if already exported.
    *   `[âœ…]` 38.c. `[TEST-UNIT]` **RED**: In `packages/utils/src/dialecticUtils.test.ts`, write tests that verify the shared utility functions work correctly.
        *   `[âœ…]` 38.c.i. Create a test file following the pattern of `packages/utils/src/type_guards.test.ts` using vitest (`describe`, `it`, `expect`).
        *   `[âœ…]` 38.c.ii. Import `buildFocusedDocumentKey` and `isDocumentHighlighted` from `./dialecticUtils` (the functions don't exist yet, so this will fail initially).
        *   `[âœ…]` 38.c.iii. Write test cases for `buildFocusedDocumentKey`: (1) returns correct format `${sessionId}:${stageSlug}:${modelId}` for valid inputs, (2) handles empty strings correctly, (3) handles special characters in IDs correctly.
        *   `[âœ…]` 38.c.iv. Write test cases for `isDocumentHighlighted`: (1) returns `true` when `focusedStageDocumentMap[focusKey]?.documentKey === documentKey` matches, (2) returns `false` when `focusKey` doesn't exist in map, (3) returns `false` when `documentKey` doesn't match, (4) returns `false` when `sessionId`, `stageSlug`, or `modelId` are empty/missing, (5) returns `false` when `focusedStageDocumentMap` is `undefined` or `null`, (6) returns `false` when `focusedStageDocumentMap[focusKey]` is `null`. Import `FocusedStageDocumentState` from `@paynless/types` for test fixtures. These tests must fail because the functions don't exist yet.
    *   `[âœ…]` 38.d. `[UTILS]` **GREEN**: In `packages/utils/src/dialecticUtils.ts`, create the shared utility functions.
        *   `[âœ…]` 38.d.i. Create the file following the pattern of `packages/utils/src/stringUtils.ts` with JSDoc comments.
        *   `[âœ…]` 38.d.ii. Import `FocusedStageDocumentState` from `@paynless/types`.
        *   `[âœ…]` 38.d.iii. Export `buildFocusedDocumentKey` function: `export const buildFocusedDocumentKey = (sessionId: string, stageSlug: string, modelId: string): string => `${sessionId}:${stageSlug}:${modelId}`;` with JSDoc explaining the format and parameters.
        *   `[âœ…]` 38.d.iv. Export `isDocumentHighlighted` function that: (1) checks if `sessionId`, `stageSlug`, and `modelId` are truthy (returns `false` if any are missing), (2) calls `buildFocusedDocumentKey` to construct `focusKey`, (3) checks if `focusedStageDocumentMap` exists and `focusedStageDocumentMap[focusKey]?.documentKey === documentKey`, (4) returns boolean result. Include JSDoc explaining the function matches StageRunChecklist logic and parameters.
        *   `[âœ…]` 38.d.v. Ensure both functions are exported (not just defined) so they can be imported by consumers.
    *   `[âœ…]` 38.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.c and ensure they now pass. The tests should verify that both utility functions work correctly with all test cases.
    *   `[âœ…]` 38.f. `[UTILS]` In `packages/utils/src/index.ts`, export the new utility functions.
        *   `[âœ…]` 38.f.i. Add `export * from './dialecticUtils';` to the list of exports (after line 6, following the existing pattern).
        *   `[âœ…]` 38.f.ii. Verify the export is added correctly and follows the same pattern as other exports.
    *   `[âœ…]` 38.g. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/StageRunChecklist.test.tsx`, update existing tests to verify the component uses the shared utility functions.
        *   `[âœ…]` 38.g.i. Read the existing test file to understand current test structure and what tests exist for highlighting behavior.
        *   `[âœ…]` 38.g.ii. Add test cases that verify `StageRunChecklist` correctly uses `isDocumentHighlighted` from `@paynless/utils` (mock the import if needed, or verify the behavior matches the shared utility logic). The tests should verify that highlighting behavior matches the shared utility function behavior.
        *   `[âœ…]` 38.g.iii. These tests should pass after `StageRunChecklist.tsx` is updated to use the shared utility, but may fail initially if the component still uses private logic.
    *   `[âœ…]` 38.h. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/StageRunChecklist.tsx`, replace private implementation with shared utility functions.
        *   `[âœ…]` 38.h.i. Remove the private `buildFocusedDocumentKey` function (lines 75-76).
        *   `[âœ…]` 38.h.ii. Add import at the top of the file: `import { buildFocusedDocumentKey, isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 38.h.iii. Replace the inline highlighting logic (lines 349-358) with a call to `isDocumentHighlighted(activeSessionId ?? '', activeStageSlug ?? '', documentModelId ?? '', entry.documentKey, effectiveFocusedStageDocumentMap)`. Store the result in `isActive` variable.
        *   `[âœ…]` 38.h.iv. Verify that the `buildFocusedDocumentKey` function is no longer used directly in this file (it's now only used internally by `isDocumentHighlighted`).
        *   `[âœ…]` 38.h.v. Ensure all existing functionality is preserved - only the implementation source changes, not the behavior.
    *   `[âœ…]` 38.i. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.g and ensure they now pass. Also verify that all existing `StageRunChecklist` tests still pass (the behavior should be identical, just using shared utilities).
    *   `[âœ…]` 38.j. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`, write tests that verify the component only renders cards for documents that are highlighted in StageRunChecklist.
        *   `[âœ…]` 38.j.i. Create a test case that mocks store state with multiple documents in `documentsByModel` for a model (e.g., `'business_case'` and `'feature_spec'`), but only one document is highlighted in `focusedStageDocument` (e.g., `focusedStageDocument['session1:thesis:model1'] = { modelId: 'model1', documentKey: 'business_case' }`). Use the same `focusKey` format (`${sessionId}:${stageSlug}:${modelId}`) as the shared utility.
        *   `[âœ…]` 38.j.ii. Render `SessionContributionsDisplayCard` and assert that only the highlighted document (`'business_case'`) has a Card component rendered (check for `data-testid={`stage-document-card-${modelId}-business_case`}`).
        *   `[âœ…]` 38.j.iii. Assert that the non-highlighted document (`'feature_spec'`) does NOT have a Card component rendered (verify `data-testid={`stage-document-card-${modelId}-feature_spec`}` is not present in the DOM).
        *   `[âœ…]` 38.j.iv. Create a test case where no documents are highlighted (`focusedStageDocument` is empty object `{}` or `undefined`), and assert that no document cards are rendered (only the "No documents generated yet" message or similar).
        *   `[âœ…]` 38.j.v. Create a test case with multiple models, where each model has different highlighted documents, and verify that only highlighted documents are rendered for each model. For example, `model-a` has `business_case` highlighted, `model-b` has `feature_spec` highlighted, and verify only those specific cards are rendered. This test must fail because `SessionContributionsDisplayCard` currently renders all documents regardless of highlighting status.
    *   `[âœ…]` 38.k. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx`, add filtering logic to only render documents that are highlighted in StageRunChecklist using the shared utility.
        *   `[âœ…]` 38.k.i. Add import at the top: `import { isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 38.k.ii. Add type import: `import type { FocusedStageDocumentState } from '@paynless/types';` (if not already imported).
        *   `[âœ…]` 38.k.iii. Access `focusedStageDocument` from the store after line 114 (after `modelCatalog` access): `const focusedStageDocument = useDialecticStore((state) => state.focusedStageDocument);`
        *   `[âœ…]` 38.k.iv. Create a `useMemo` hook (before line 285, before `documentGroups` definition) that filters `documentGroups` to only include highlighted documents: `const filteredDocumentGroups = useMemo(() => { if (!session || !activeStage) { return []; } return documentGroups.map(([modelId, documents]) => { const filteredDocuments = documents.filter((document) => isDocumentHighlighted(session.id, activeStage.slug, modelId, document.documentKey, focusedStageDocument)); return filteredDocuments.length > 0 ? [modelId, filteredDocuments] as const : null; }).filter((group): group is [string, StageDocumentChecklistEntry[]] => group !== null); }, [documentGroups, session, activeStage, focusedStageDocument]);`
        *   `[âœ…]` 38.k.v. Update the `hasDocuments` check (line 312) to use `filteredDocumentGroups` instead of `documentGroups`: `const hasDocuments = useMemo(() => filteredDocumentGroups.some(([, documents]) => documents.length > 0), [filteredDocumentGroups]);`
        *   `[âœ…]` 38.k.vi. Update the rendering logic (line 552) to use `filteredDocumentGroups.map` instead of `documentGroups.map`. Ensure that if a model has no highlighted documents after filtering, the entire model section (including the model name heading) is not rendered (the filter already handles this by returning `null` and filtering it out).
        *   `[âœ…]` 38.k.vii. Handle edge cases: the `isDocumentHighlighted` function already handles `undefined`/`null` `focusedStageDocument` (returns `false`), and the `useMemo` handles missing `session`/`activeStage` (returns empty array). No additional edge case handling needed beyond what the shared utility provides.
    *   `[âœ…]` 38.l. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 38.j and ensure they now pass. Also verify that existing `SessionContributionsDisplayCard` tests still pass (update any tests that expect all documents to be rendered to account for the new filtering behavior - they should mock `focusedStageDocument` appropriately).
    *   `[âœ…]` 38.m. `[LINT]` Run the linter for all modified files and resolve any warnings or errors.
        *   `[âœ…]` 38.m.i. Lint `packages/utils/src/dialecticUtils.ts` and `packages/utils/src/dialecticUtils.test.ts`.
        *   `[âœ…]` 38.m.ii. Lint `packages/utils/src/index.ts`.
        *   `[âœ…]` 38.m.iii. Lint `apps/web/src/components/dialectic/StageRunChecklist.tsx` and `apps/web/src/components/dialectic/StageRunChecklist.test.tsx`.
        *   `[âœ…]` 38.m.iv. Lint `apps/web/src/components/dialectic/SessionContributionsDisplayCard.tsx` and `apps/web/src/components/dialectic/SessionContributionsDisplayCard.test.tsx`.
    *   `[âœ…]` 38.n. `[CRITERIA]` All requirements are met: (1) Shared utility functions `buildFocusedDocumentKey` and `isDocumentHighlighted` are in `@paynless/utils` package and exported, (2) `StageRunChecklist` uses the shared utility functions instead of private implementation, (3) `SessionContributionsDisplayCard` filters documents using the shared utility function, (4) Only highlighted documents are rendered in `SessionContributionsDisplayCard`, (5) All tests pass, including utility function tests, `StageRunChecklist` tests, and `SessionContributionsDisplayCard` tests, (6) All files are lint-clean, (7) DRY compliance is achieved - highlighting logic exists in one place and is reused by all consumers.
    
*   `[âœ…]` 39. **`[UI]` Verify GeneratedContributionCard Only Renders Documents That Are Highlighted in StageRunChecklist**
    *   `[âœ…]` 39.a. `[DEPS]` The `GeneratedContributionCard` component in `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` already checks `focusedDocument` before rendering document content (line 345: `{focusedDocument && isValidMarkdownDocument ? (...)`). However, it should verify that the `focusedDocument` check aligns with the highlighting logic used in `StageRunChecklist`. The component uses `selectFocusedStageDocument` (line 88) which queries `focusedStageDocument` from the store using the same `focusKey` pattern (`${sessionId}:${stageSlug}:${modelId}`). The component should ensure that document content is only rendered when the document is actually highlighted in the checklist (i.e., when `focusedDocument` exists and matches the highlighting criteria). Currently, the component may render document content if `focusedDocument` exists but the document might not be highlighted in the checklist (edge case scenario). The shared utility function `isDocumentHighlighted` from `@paynless/utils` (created in step 38) provides the exact highlighting logic used by `StageRunChecklist`. The fix requires: (1) importing `isDocumentHighlighted` from `@paynless/utils`, (2) updating the `focusedDocument` check (line 345) to also verify the document is highlighted using the shared utility, (3) ensuring that if `focusedDocument` exists but the document is not actually highlighted (mismatch in `focusedStageDocumentMap`), the content should not render, (4) using the same highlighting validation logic as `StageRunChecklist` and `SessionContributionsDisplayCard` to maintain consistency.
    *   `[âœ…]` 39.b. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write tests that verify the component only renders document content when the document is highlighted in StageRunChecklist using the shared utility.
        *   `[âœ…]` 39.b.i. Create a test case that mocks store state where `selectFocusedStageDocument` returns a `focusedDocument` (e.g., `{ documentKey: 'business_case' }`), but `focusedStageDocumentMap` does NOT have an entry for that document (the document is not highlighted). Use the same `focusKey` format (`${sessionId}:${stageSlug}:${modelId}`) as the shared utility. Assert that the document content section (the div starting at line 346) is NOT rendered, and only the "Select a document to view its content and provide feedback." message is shown.
        *   `[âœ…]` 39.b.ii. Create a test case that mocks store state where `focusedStageDocumentMap` has an entry for a document (e.g., `focusedStageDocumentMap['session1:thesis:model1'] = { modelId: 'model1', documentKey: 'business_case' }`), and `selectFocusedStageDocument` returns a matching `focusedDocument`, and assert that the document content section IS rendered.
        *   `[âœ…]` 39.b.iii. Create a test case where `selectFocusedStageDocument` returns a `focusedDocument`, but `focusedStageDocumentMap` has an entry with a different `documentKey` (mismatch), and assert that the document content is NOT rendered. This test verifies that the component validates the document is actually highlighted using the shared utility, not just that `focusedDocument` exists.
        *   `[âœ…]` 39.b.iv. Create a test case where `focusedDocument` is `null`, and assert that no document content is rendered (only the "Select a document" message). This test should pass initially but must continue to pass after any fixes.
        *   `[âœ…]` 39.b.v. These tests must fail if the component currently renders document content when `focusedDocument` exists but the document is not highlighted in the checklist.
    *   `[âœ…]` 39.c. `[UI]` **GREEN**: In `apps/web/src/components/dialectic/GeneratedContributionCard.tsx`, add explicit validation that the focused document is actually highlighted in the checklist before rendering content using the shared utility.
        *   `[âœ…]` 39.c.i. Add import at the top of the file: `import { isDocumentHighlighted } from '@paynless/utils';`
        *   `[âœ…]` 39.c.ii. Add type import if not already present: `import type { FocusedStageDocumentState } from '@paynless/types';` (verify it's already imported or add it).
        *   `[âœ…]` 39.c.iii. Update the condition at line 345 from `{focusedDocument && isValidMarkdownDocument ? (` to `{focusedDocument && isValidMarkdownDocument && isDocumentHighlighted(sessionId ?? '', stageSlug ?? '', modelId, focusedDocument.documentKey, focusedStageDocumentMap) ? (`. This ensures that document content is only rendered when: (1) `focusedDocument` exists, (2) the document is a valid markdown document, AND (3) the document is actually highlighted in the checklist (verified using the shared utility function).
        *   `[âœ…]` 39.c.iv. Handle edge cases: the `isDocumentHighlighted` function from `@paynless/utils` already handles `undefined`/`null` `focusedStageDocumentMap` (returns `false`), and handles missing `sessionId`/`stageSlug`/`modelId` (returns `false`). The `?? ''` fallbacks ensure strings are passed even if values are null/undefined. No additional edge case handling needed beyond what the shared utility provides.
        *   `[âœ…]` 39.c.v. Ensure the existing logic for displaying "Select a document to view its content and provide feedback." (lines 469-473) still works correctly when no document is highlighted.
    *   `[âœ…]` 39.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 39.b and ensure they now pass. Also verify that existing `GeneratedContributionCard` tests still pass (update any tests that expect document content to be rendered to account for the new highlighting validation - they should mock `focusedStageDocumentMap` appropriately).
    *   `[âœ…]` 39.e. `[TEST-UNIT]` **RED**: In `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx`, write a test that verifies the component correctly uses the shared utility function from `@paynless/utils`.
        *   `[âœ…]` 39.e.i. Create a test case that verifies `GeneratedContributionCard` correctly uses `isDocumentHighlighted` from `@paynless/utils` by mocking the import and verifying it's called with the correct parameters (`sessionId`, `stageSlug`, `modelId`, `documentKey`, `focusedStageDocumentMap`) when checking if document content should be rendered. Alternatively, verify the behavior matches the shared utility by testing with known inputs that should pass/fail according to the utility's logic.
        *   `[âœ…]` 39.e.ii. Create a test case that verifies the component's highlighting behavior matches `StageRunChecklist` behavior when using the same `focusedStageDocumentMap` state. This ensures consistency across all three components (`StageRunChecklist`, `SessionContributionsDisplayCard`, and `GeneratedContributionCard`).
        *   `[âœ…]` 39.e.iii. This test ensures the highlighting logic is consistent between `GeneratedContributionCard` and `StageRunChecklist` by verifying both use the same shared utility function.
    *   `[âœ…]` 39.f. `[TEST-UNIT]` **GREEN**: Re-run the test from step 39.e and ensure it passes. This verifies that `GeneratedContributionCard` correctly integrates with the shared utility function.
    *   `[âœ…]` 39.g. `[LINT]` Run the linter for `apps/web/src/components/dialectic/GeneratedContributionCard.tsx` and `apps/web/src/components/dialectic/GeneratedContributionCard.test.tsx` and resolve any warnings or errors.
    *   `[âœ…]` 39.h. `[CRITERIA]` All requirements are met: (1) `GeneratedContributionCard` imports and uses `isDocumentHighlighted` from `@paynless/utils` (shared utility from step 38), (2) Document content is only rendered when the document is actually highlighted in the checklist (verified using the shared utility), (3) All three components (`StageRunChecklist`, `SessionContributionsDisplayCard`, and `GeneratedContributionCard`) use the same shared utility function, ensuring consistent highlighting behavior, (4) All tests pass, including tests that verify the component uses the shared utility correctly, (5) The file is lint-clean, (6) DRY compliance is maintained - no duplicate highlighting logic exists.

*   `[âœ…]` 40. **`[BE]` Fix Path Construction to Require ALL Required Values and Remove Fallback Chain**
    *   `[âœ…]` 40.a. `[DEPS]` The `constructStoragePath` function in `supabase/functions/_shared/utils/path_constructor.ts` (line 214) uses a fallback chain `documentKey ?? contributionType ?? fileType` to determine `effectiveContributionType` for document file types. This violates project coding standards that forbid defaults, fallbacks, or silent healing. For document file types (FileType.business_case, FileType.feature_spec, etc.), the function MUST require ALL required values to be present and non-empty, and MUST throw an error immediately if ANY required value is missing, undefined, null, or empty string. The function requires the following values for document file types: (1) `projectId: string` (required in PathContext interface), (2) `fileType: FileType` (required in PathContext interface), (3) `sessionId: string` (required for `stageRootPath` construction on line 80), (4) `iteration: number` (required for `stageRootPath` construction on line 80), (5) `stageSlug: string` (required for `stageRootPath` construction on line 80 via `mappedStageDir`), (6) `modelSlug: string` (required for document file types, validated on line 218), (7) `attemptCount: number` (required for document file types, validated on line 218), (8) `documentKey: string` (required for document file types, currently missing validation). All documentKey values are members of FileType enum. The function is called by `FileManagerService.uploadAndRegisterFile` which receives `pathContext` from `executeModelCallAndSave` (step 41 will fix the caller to provide ALL required values). The fix requires: (1) defining a `DocumentKey` type that represents FileType values that are valid documentKey values, (2) creating an `isDocumentKey` type guard to check if a FileType is a DocumentKey, (3) validating that ALL required values are present and non-empty for document file types BEFORE any path construction logic, (4) throwing descriptive errors immediately if ANY validation fails, (5) removing the fallback chain and using `documentKey` directly for document file types, (6) ensuring error messages clearly identify which file type failed and which specific value is missing or invalid.
    *   `[âœ…]` 40.b. `[TYPES]` In `supabase/functions/_shared/types/file_manager.types.ts`, define `DocumentKey` type as a union of FileType enum members that are valid documentKey values. Add the type definition after `ModelContributionFileTypes` (around line 165). The type should include all FileType values that are documentKeys: `business_case`, `feature_spec`, `technical_approach`, `success_metrics`, `business_case_critique`, `technical_feasibility_assessment`, `risk_register`, `non_functional_requirements`, `dependency_map`, `comparison_vector`, `product_requirements`, `system_architecture`, `tech_stack`, `technical_requirements`, `master_plan`, `milestone_schema`, `updated_master_plan`, `actionable_checklist`, `advisor_recommendations`. Export the type so it can be imported by type guard files.
    *   `[âœ…]` 40.c. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.test.ts`, write tests that verify `isDocumentKey` type guard correctly identifies DocumentKey file types.
        *   `[âœ…]` 40.c.i. Create test cases that assert `isDocumentKey` returns `true` for all DocumentKey file types (e.g., `FileType.business_case`, `FileType.feature_spec`, `FileType.technical_approach`, etc.).
        *   `[âœ…]` 40.c.ii. Create test cases that assert `isDocumentKey` returns `false` for non-DocumentKey file types (e.g., `FileType.HeaderContext`, `FileType.ModelContributionRawJson`, `FileType.PairwiseSynthesisChunk`, `FileType.ReducedSynthesis`, `FileType.Synthesis`, `FileType.header_context_pairwise`, `FileType.SynthesisHeaderContext`, `FileType.synthesis_pairwise_business_case`, `FileType.synthesis_document_business_case`, etc.). These tests must fail because the type guard doesn't exist yet.
    *   `[âœ…]` 40.d. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, implement the `isDocumentKey` type guard function.
        *   `[âœ…]` 40.d.i. Import `DocumentKey` type from `../../types/file_manager.types.ts`.
        *   `[âœ…]` 40.d.ii. Create a compile-time enforced map `DOCUMENT_KEY_MAP: { [K in DocumentKey]: true }` that includes all DocumentKey file types, following the same pattern as `MODEL_CONTRIBUTION_FILE_TYPES_MAP` and `OUTPUT_TYPES_MAP` in the same file.
        *   `[âœ…]` 40.d.iii. Export the function `isDocumentKey(value: FileType): value is DocumentKey` that checks if the value exists in `DOCUMENT_KEY_MAP` using `Object.prototype.hasOwnProperty.call(DOCUMENT_KEY_MAP, value)`, following the same pattern as `isModelContributionFileType` and `isOutputType` in the same file.
    *   `[âœ…]` 40.e. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 40.c and ensure they now pass. Also verify that existing type guard tests still pass.
    *   `[âœ…]` 40.f. `[TEST-UNIT]` **RED**: In `supabase/functions/_shared/utils/path_constructor.test.ts`, write tests that verify `constructStoragePath` requires ALL required values for document file types and throws errors when ANY value is missing.
        *   `[âœ…]` 40.f.i. Create a test case that calls `constructStoragePath` with a document file type (e.g., `FileType.business_case`) and ALL required values present and non-empty: `projectId: 'project-123'`, `fileType: FileType.business_case`, `sessionId: 'session-123'`, `iteration: 1`, `stageSlug: 'thesis'`, `modelSlug: 'claude-opus'`, `attemptCount: 0`, `documentKey: 'business_case'`. Assert that the function returns a valid path with the `documentKey` in the filename (e.g., `claude-opus_0_business_case.md`). This test should pass after implementation.
        *   `[âœ…]` 40.f.ii. Create test cases that call `constructStoragePath` with a document file type and each required value missing individually: (a) `documentKey: undefined`, (b) `documentKey: null`, (c) `documentKey: ''` (empty string), (d) `sessionId: undefined`, (e) `iteration: undefined`, (f) `stageSlug: undefined`, (g) `modelSlug: undefined`, (h) `attemptCount: undefined`, (i) `projectId: undefined`. Each test must assert that the function throws an error with a message indicating which specific value is required and missing. These tests must fail because the function currently does not validate all required values.
        *   `[âœ…]` 40.f.iii. Create a test case that calls `constructStoragePath` with a non-document file type (e.g., `FileType.HeaderContext`) and `documentKey: undefined`, and assert that the function does NOT throw an error (non-document file types may not require `documentKey`). This test should pass if non-document file types are handled separately.
        *   `[âœ…]` 40.f.iv. Create a test case that verifies `constructStoragePath` requires ALL required values for document file types by testing the behavior: (1) Call `constructStoragePath` with a representative document file type (e.g., `FileType.business_case`) and ALL required values present, assert it succeeds, (2) Call `constructStoragePath` with the same document file type but missing `documentKey`, assert it throws an error indicating `documentKey (string, non-empty)` is required, (3) Call `constructStoragePath` with a non-document file type (e.g., `FileType.HeaderContext`) and missing `documentKey`, assert it does NOT throw an error (proving document file types are handled differently). This test verifies the behavior without enumerating or redefining which file types are document types - it tests that the function correctly distinguishes document file types from non-document file types and enforces the requirement.
    *   `[âœ…]` 40.g. `[BE]` **GREEN**: In `supabase/functions/_shared/utils/path_constructor.ts`, remove the fallback chain and require ALL required values for document file types.
        *   `[âœ…]` 40.g.i. Import `isDocumentKey` from `../type-guards/type_guards.file_manager.ts` at the top of the file.
        *   `[âœ…]` 40.g.ii. At the start of the function (after destructuring `context` on line 70), before any path construction logic, add comprehensive validation for document file types. Use `isDocumentKey(fileType)` to check if the file type requires documentKey. If it is a document file type, validate ALL required values in a single validation block: (1) `projectId` must exist, be a string, and be non-empty, (2) `fileType` must exist (already validated by TypeScript), (3) `sessionId` must exist, be a string, and be non-empty, (4) `iteration` must exist and be a number, (5) `stageSlug` must exist, be a string, and be non-empty, (6) `modelSlug` must exist, be a string, and be non-empty, (7) `attemptCount` must exist and be a number, (8) `documentKey` must exist, be a string, and be non-empty. If ANY validation fails, throw an error immediately: `constructStoragePath requires all of the following values for document file type '${fileType}': projectId (string, non-empty), sessionId (string, non-empty), iteration (number), stageSlug (string, non-empty), modelSlug (string, non-empty), attemptCount (number), documentKey (string, non-empty). Missing or invalid: [list of missing/invalid values]`. This ensures ALL required values are validated BEFORE any path construction logic executes.
        *   `[âœ…]` 40.g.iii. For document file types, remove the fallback chain on line 214: replace `const effectiveContributionType = documentKey ?? contributionType ?? fileType;` with conditional logic that uses `documentKey` directly when `isDocumentKey(fileType)` is true, and keeps the fallback chain for non-document file types. For document file types, `documentKey` is now guaranteed to be present and non-empty after validation, so no fallback is needed. For non-document file types, preserve the existing fallback chain behavior.
        *   `[âœ…]` 40.g.iv. Update the `baseFileName` construction (line 273) to use `documentKey` directly for document file types: in the default case, check if `isDocumentKey(fileType)` and if so, use `${modelSlugSanitized}_${attemptCount}_${sanitizeForPath(documentKey)}` instead of `${modelSlugSanitized}_${attemptCount}_${contributionTypeSanitized}`. This ensures the filename uses the actual document key (e.g., `business_case`) instead of the fallback value (e.g., `thesis`). For non-document file types, preserve the existing logic.
        *   `[âœ…]` 40.g.v. Replace the inline `isDocument` array check (lines 293-299) with `isDocumentKey(fileType)`. Update the `storagePath` logic (lines 301-313) to use `isDocumentKey(fileType)` instead of `isDocument || documentKey`. The check `isDocumentKey(fileType) || documentKey` on line 308 should continue to work correctly, but now `documentKey` will always be present for document file types after validation.
        *   `[âœ…]` 40.g.vi. Ensure non-document file types (e.g., `FileType.HeaderContext`, `FileType.PairwiseSynthesisChunk`, `FileType.ReducedSynthesis`, `FileType.antithesis`) continue to work correctly with their existing logic. These file types may use `contributionType` or other fields, and should continue to use the fallback chain for `effectiveContributionType` determination.
    *   `[âœ…]` 40.h. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 40.f and ensure they now pass. Also verify that existing `path_constructor` tests still pass (update any tests that rely on the fallback chain to provide `documentKey` for document file types).
    *   `[âœ…]` 40.i. `[LINT]` Run the linter for all modified files (`supabase/functions/_shared/types/file_manager.types.ts`, `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.ts`, `supabase/functions/_shared/utils/type-guards/type_guards.file_manager.test.ts`, `supabase/functions/_shared/utils/path_constructor.ts`, `supabase/functions/_shared/utils/path_constructor.test.ts`) and resolve any warnings or errors.

*   `[âœ…]` 41. **`[BE]` Fix executeModelCallAndSave to Pass ALL Required Values to Path Context and Use Correct document_key in Notifications**
    *   `[âœ…]` 41.a. `[DEPS]` The `executeModelCallAndSave` function in `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` constructs `pathContext` for file uploads (lines 1074-1086) but does not include `documentKey` from `job.payload.document_key`, even though planners set this field (per checklist items 11-16). Additionally, the function must ensure ALL required values for `pathContext` are present and validated BEFORE constructing the `pathContext` object. The `constructStoragePath` function (fixed in step 40) requires the following values for document file types: `projectId`, `fileType`, `sessionId`, `iteration`, `stageSlug`, `modelSlug`, `attemptCount`, `documentKey`. The function currently extracts: `projectId: job.payload.projectId` (line 1076), `sessionId` (line 1078), `iteration: iterationNumber` (line 1079), `modelSlug: providerDetails.api_identifier` (line 1080), `attemptCount: job.attempt_count` (line 1081), `...restOfCanonicalPathParams` (line 1082, which includes `stageSlug` from `canonicalPathParams`). However, `documentKey` is NOT included, and there is NO validation that ALL required values are present before constructing `pathContext`. Additionally, the `document_completed` notification (line 1284) uses `String(output_type)` as `document_key`, but should use `job.payload.document_key` if present, as this matches the actual document key from the recipe step that the frontend expects. The fix requires: (1) validating that `job.payload.canonicalPathParams` exists and has `stageSlug` (required for `pathContext.stageSlug`), (2) validating that `job.payload.projectId` exists and is non-empty, (3) validating that `job.payload.sessionId` exists and is non-empty, (4) validating that `job.payload.iterationNumber` exists and is a number, (5) validating that `job.attempt_count` exists and is a number, (6) validating that `providerDetails.api_identifier` exists and is non-empty (for `modelSlug`), (7) validating that `job.payload.document_key` is present and non-empty for EXECUTE jobs that output documents, (8) adding `documentKey: job.payload.document_key` to `pathContext` (after line 1082, before `contributionType`), (9) updating the `document_completed` notification (line 1284) to use `job.payload.document_key` instead of `String(output_type)`, (10) ensuring ALL validation errors clearly identify which specific value is missing or invalid.
    *   `[âœ…]` 41.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.pathContext.test.ts`, write tests that verify `executeModelCallAndSave` validates ALL required values and passes them to path context, and uses correct `document_key` in notifications.
        *   `[âœ…]` 41.b.i. Create a test case that mocks a job payload with ALL required values present: `document_key: 'business_case'`, `output_type: 'business_case'`, `projectId: 'project-123'`, `sessionId: 'session-123'`, `iterationNumber: 1`, `canonicalPathParams: { stageSlug: 'thesis', ... }`, `attempt_count: 0`, and mocks `providerDetails.api_identifier: 'claude-opus'`. Mock `fileManager.uploadAndRegisterFile` to capture the `pathContext` argument. Assert that `pathContext` contains ALL required values: `documentKey === 'business_case'`, `projectId === 'project-123'`, `sessionId === 'session-123'`, `iteration === 1`, `stageSlug === 'thesis'`, `modelSlug === 'claude-opus'`, `attemptCount === 0`. This test must fail because `documentKey` is not currently passed to `pathContext`.
        *   `[âœ…]` 41.b.ii. Create a test case that mocks a job payload with `document_key: 'feature_spec'` and verifies that the `document_completed` notification uses `document_key: 'feature_spec'` (not `String(output_type)`). Mock `notificationService.sendDocumentCentricNotification` and assert the notification payload has `document_key: 'feature_spec'`. This test must fail because the notification currently uses `String(output_type)`.
        *   `[âœ…]` 41.b.iii. Create test cases that mock an EXECUTE job with `output_type: 'business_case'` (a document file type) but each required value missing individually: (a) `document_key: undefined`, (b) `document_key: ''` (empty string), (c) `projectId: undefined`, (d) `sessionId: undefined`, (e) `iterationNumber: undefined`, (f) `canonicalPathParams: undefined`, (g) `canonicalPathParams.stageSlug: undefined`, (h) `attempt_count: undefined`, (i) `providerDetails.api_identifier: undefined`. Each test must assert that `executeModelCallAndSave` throws an error indicating which specific value is required and missing. These tests must fail because the function currently does not validate all required values.
        *   `[âœ…]` 41.b.iv. Create a test case that mocks an EXECUTE job with `output_type: 'HeaderContext'` (a non-document file type) and `document_key: undefined`, and assert that `executeModelCallAndSave` does NOT throw an error (non-document file types may not require `document_key`). This test should pass if non-document file types are handled separately.
    *   `[âœ…]` 41.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/executeModelCallAndSave.ts`, validate ALL required values and add `documentKey` to path context, and fix notification to use correct `document_key`.
        *   `[âœ…]` 41.c.i. Before constructing `uploadContext` (line 1074), add comprehensive validation logic that checks if `output_type` is a document file type (using the same list as `path_constructor.ts` lines 293-299). If it is a document file type, validate ALL required values in a single validation block: (1) `job.payload.projectId` must exist, be a string, and be non-empty, (2) `job.payload.sessionId` must exist, be a string, and be non-empty, (3) `job.payload.iterationNumber` must exist and be a number, (4) `job.payload.canonicalPathParams` must exist and be an object, (5) `job.payload.canonicalPathParams.stageSlug` must exist, be a string, and be non-empty, (6) `job.attempt_count` must exist and be a number, (7) `providerDetails.api_identifier` must exist, be a string, and be non-empty, (8) `job.payload.document_key` must exist, be a string, and be non-empty. If ANY validation fails, throw an error immediately: `executeModelCallAndSave requires all of the following values for document file type '${output_type}': job.payload.projectId (string, non-empty), job.payload.sessionId (string, non-empty), job.payload.iterationNumber (number), job.payload.canonicalPathParams.stageSlug (string, non-empty), job.attempt_count (number), providerDetails.api_identifier (string, non-empty), job.payload.document_key (string, non-empty). Missing or invalid: [list of missing/invalid values]`. This ensures ALL required values are validated BEFORE constructing `pathContext`.
        *   `[âœ…]` 41.c.ii. In the `pathContext` object (line 1075), add `documentKey: job.payload.document_key` after line 1082 (after `...restOfCanonicalPathParams`, before `contributionType`). This ensures `constructStoragePath` receives the `documentKey` it requires. Verify that ALL other required values are already present: `projectId`, `sessionId`, `iteration`, `stageSlug` (from `restOfCanonicalPathParams`), `modelSlug`, `attemptCount`.
        *   `[âœ…]` 41.c.iii. Update the `document_completed` notification (line 1284) to use `job.payload.document_key` instead of `String(output_type)`: change `document_key: String(output_type)` to `document_key: job.payload.document_key`. This ensures the notification uses the correct document key that matches the frontend's expectations.
        *   `[âœ…]` 41.c.iv. Ensure non-document file types (e.g., `FileType.HeaderContext`) continue to work correctly. If `output_type` is not a document file type, `document_key` may be undefined, and the function should not throw an error for these cases.
    *   `[âœ…]` 41.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 41.b and ensure they now pass. Also verify that existing `executeModelCallAndSave` tests still pass (update any tests that mock job payloads to include `document_key` for document file types).
    *   `[âœ…]` 41.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/executeModelCallAndSave.ts` and `supabase/functions/dialectic-worker/executeModelCallAndSave.test.ts` and resolve any warnings or errors.

*   `[âœ…]` 42. **`[BE]` Fix processComplexJob to Filter Out Already-Completed Steps from Ready Steps and Validate planner_metadata**
    *   `[âœ…]` 42.a. `[DEPS]` The `processComplexJob` function in `supabase/functions/dialectic-worker/processComplexJob.ts` tracks completed steps by looking up `step_slug` from `planner_metadata.recipe_step_id` in completed child jobs (lines 154-166), and stores them in `completedStepSlugs` Set. However, the function does NOT validate that `planner_metadata` exists and has `recipe_step_id` for completed child jobs. If `planner_metadata` is missing or incomplete, the step cannot be tracked as completed, causing it to be re-planned. Additionally, when building `readySteps` (lines 192-217), the function only checks if predecessors are completed, but does not filter out steps that are already completed themselves. This causes the system to re-plan steps that have already been completed, leading to the recipe restarting from the beginning (e.g., re-planning `build-stage-header` even though documents have already been generated). The fix requires: (1) validating that ALL completed child jobs have `planner_metadata` with `recipe_step_id` present and non-empty, (2) throwing an error immediately if a completed child job is missing `planner_metadata` or `recipe_step_id` (cannot track completion without it), (3) filtering `readySteps` to exclude steps whose `step_slug` is already in `completedStepSlugs`, (4) ensuring this filter is applied after building the `readySteps` array but before planning, (5) verifying that the completion check uses the same `step_slug` matching logic that is used to populate `completedStepSlugs`.
    *   `[âœ…]` 42.b. `[TEST-UNIT]` **RED**: In `supabase/functions/dialectic-worker/processComplexJob.test.ts`, write tests that verify `processComplexJob` validates `planner_metadata` and filters out already-completed steps from ready steps.
        *   `[âœ…]` 42.b.i. Create a test case that mocks a PLAN job with multiple recipe steps, where one step (`build-stage-header`) has a completed child job (status `'completed'` with `planner_metadata.recipe_step_id` matching the step's `id`), and another step (`generate-business-case`) has no completed children. Assert that `processComplexJob` only plans the step that is not yet completed (`generate-business-case`), and does NOT plan the already-completed step (`build-stage-header`). This test must fail because the function currently does not filter out completed steps.
        *   `[âœ…]` 42.b.ii. Create a test case that mocks a PLAN job where all steps have completed child jobs, and assert that `processComplexJob` completes the parent job (does not plan any steps, marks parent as `'completed'`). This test should pass if the existing logic (lines 220-229) works correctly, but must continue to pass after the filter is added.
        *   `[âœ…]` 42.b.iii. Create a test case that mocks a PLAN job where a step has a completed child job but the step's `step_slug` does not match the `completedStepSlugs` Set (simulating a mismatch in step tracking), and assert that the step is still planned (the filter only excludes steps that are correctly tracked as completed). This test verifies the filter uses the correct matching logic.
        *   `[âœ…]` 42.b.iv. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: undefined`, and assert that `processComplexJob` throws an error indicating `planner_metadata` is required for completed child jobs. This test must fail because the function currently does not validate `planner_metadata` presence.
        *   `[âœ…]` 42.b.v. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: {}` (empty object, missing `recipe_step_id`), and assert that `processComplexJob` throws an error indicating `planner_metadata.recipe_step_id` is required for completed child jobs. This test must fail because the function currently does not validate `recipe_step_id` presence.
        *   `[âœ…]` 42.b.vi. Create a test case that mocks a PLAN job with a completed child job that has `planner_metadata: { recipe_step_id: '' }` (empty string), and assert that `processComplexJob` throws an error indicating `planner_metadata.recipe_step_id` must be non-empty. This test must fail because the function currently does not validate `recipe_step_id` is non-empty.
    *   `[âœ…]` 42.c. `[BE]` **GREEN**: In `supabase/functions/dialectic-worker/processComplexJob.ts`, validate `planner_metadata` and filter out already-completed steps from `readySteps` before planning.
        *   `[âœ…]` 42.c.i. In the loop that tracks completed steps (lines 156-166), BEFORE attempting to look up `step_slug`, add validation logic that checks if `planner_metadata` exists and has `recipe_step_id`. For each completed child job, validate: (1) `child.payload` must exist and be an object, (2) `child.payload.planner_metadata` must exist and be an object, (3) `child.payload.planner_metadata.recipe_step_id` must exist, be a string, and be non-empty. If ANY validation fails for a completed child job, throw an error immediately: `processComplexJob cannot track completion for child job ${child.id} because planner_metadata.recipe_step_id is missing or invalid. Completed child jobs MUST have planner_metadata with a non-empty recipe_step_id to enable step completion tracking.`. This ensures ALL completed child jobs have the required metadata for tracking completion.
        *   `[âœ…]` 42.c.ii. After building the `readySteps` array (line 217), add a filter that excludes steps whose `step_slug` is in the `completedStepSlugs` Set. Replace `readySteps` with the filtered array: `const filteredReadySteps = readySteps.filter(step => !completedStepSlugs.has(step.step_slug));` and use `filteredReadySteps` for all subsequent operations (planning, logging, etc.).
        *   `[âœ…]` 42.c.iii. Update the check for "no ready steps remain" (line 220) to use the filtered array: change `if (readySteps.length === 0)` to `if (filteredReadySteps.length === 0)`. This ensures the function correctly detects when all remaining steps are completed.
        *   `[âœ…]` 42.c.iv. Update the sorting logic (line 233) to use the filtered array: change `readySteps.sort(...)` to `filteredReadySteps.sort(...)`. This ensures only non-completed steps are sorted and planned.
        *   `[âœ…]` 42.c.v. Update the logging (lines 244-249) to use the filtered array: change all references to `readySteps` to `filteredReadySteps` in the logging statements. This ensures logs accurately reflect which steps are being planned.
        *   `[âœ…]` 42.c.vi. Update the planning loop (line 281) to use the filtered array: change `readySteps.map(...)` to `filteredReadySteps.map(...)`. This ensures only non-completed steps are planned.
        *   `[âœ…]` 42.c.vii. Update the critical validation check (line 253) to use the filtered array: change `if (completedStepSlugs.size === 0 && readySteps.length > 1)` to `if (completedStepSlugs.size === 0 && filteredReadySteps.length > 1)`. This ensures the validation check uses the correct array.
    *   `[âœ…]` 42.d. `[TEST-UNIT]` **GREEN**: Re-run all tests from step 42.b and ensure they now pass. Also verify that existing `processComplexJob` tests still pass (the filter should not break existing functionality, only prevent re-planning of completed steps).
    *   `[âœ…]` 42.e. `[LINT]` Run the linter for `supabase/functions/dialectic-worker/processComplexJob.ts` and `supabase/functions/dialectic-worker/processComplexJob.test.ts` and resolve any warnings or errors.
    *   `[âœ…]` 42.f. `[CRITERIA]` All requirements are met: (1) `processComplexJob` validates that ALL completed child jobs have `planner_metadata` with `recipe_step_id` present and non-empty, (2) `processComplexJob` throws an error immediately if a completed child job is missing `planner_metadata` or `recipe_step_id`, (3) `processComplexJob` filters out already-completed steps from `readySteps` before planning, (4) The filter uses the same `step_slug` matching logic that populates `completedStepSlugs`, (5) All references to `readySteps` after filtering use the filtered array, (6) The function does not re-plan steps that have already been completed, (7) All tests pass, including tests that verify `planner_metadata` validation and completed steps are excluded from planning, (8) The file is lint-clean.

*   `[ ]` 43. **`[TEST-INT]` Fix Integration Test to Verify Complete End-to-End Flow with Correct File Paths and Step Tracking**
    *   `[ ]` 43.a. `[DEPS]` After all fixes are complete (steps 40-42), write an integration test that verifies the complete end-to-end flow: (1) PLAN job creates EXECUTE jobs with `document_key` in payload, (2) EXECUTE jobs save contributions with correct file paths using `documentKey`, (3) `document_completed` notifications use correct `document_key`, (4) Frontend receives notifications and updates state correctly, (5) PLAN job does not re-plan already-completed steps when woken up with `pending_next_step` status. The test file `supabase/integration_tests/services/planner_output_type.integration.test.ts` or a new integration test file should verify this flow.
    *   `[ ]` 43.b. `[TEST-INT]` **RED**: In `supabase/integration_tests/services/planner_output_type.integration.test.ts` or create a new integration test file, write a test that verifies the complete end-to-end flow with correct file paths and step tracking.
        *   `[ ]` 43.b.i. Create a test case that sets up a PLAN job, calls a planner function (e.g., `planPerSourceDocument`) with a recipe step that has `outputs_required.documents[0].document_key: 'business_case'`, and creates child EXECUTE jobs. Assert that the created EXECUTE job payloads have `document_key: 'business_case'` set correctly.
        *   `[ ]` 43.b.ii. Execute one of the EXECUTE jobs by calling `executeModelCallAndSave` with the job payload, and assert that the file is saved with the correct path containing `documentKey: 'business_case'` (e.g., filename should be `model_0_business_case.md`, not `model_0_thesis.md`). Verify the file path is constructed correctly by checking the `pathContext` passed to `fileManager.uploadAndRegisterFile`.
        *   `[ ]` 43.b.iii. Assert that the `document_completed` notification sent by `executeModelCallAndSave` uses `document_key: 'business_case'` (not `String(output_type)`). Mock or capture the notification and verify the payload.
        *   `[ ]` 43.b.iv. After the EXECUTE job completes, simulate the database trigger setting the parent PLAN job to `pending_next_step` status, and call `processComplexJob` again. Assert that `processComplexJob` does NOT re-plan the already-completed step, and only plans remaining steps that are not yet completed.
        *   `[ ]` 43.b.v. This test must fail initially if any of the fixes (steps 40-42) are not complete, but should pass after all fixes are implemented.
    *   `[ ]` 43.c. `[TEST-INT]` **GREEN**: Re-run the test from step 43.b and ensure it now passes. This verifies that the complete end-to-end flow works correctly: planners set `document_key`, file paths use correct `documentKey`, notifications use correct `document_key`, and completed steps are not re-planned.
    *   `[ ]` 43.d. `[LINT]` Run the linter for the integration test file and resolve any warnings or errors.
    *   `[ ]` 43.e. `[CRITERIA]` The integration test verifies: (1) EXECUTE jobs have `document_key` in payload, (2) File paths are constructed with correct `documentKey` (not fallback values), (3) Notifications use correct `document_key` matching the recipe step, (4) Completed steps are not re-planned when parent job is woken up, (5) All tests pass, proving the complete end-to-end flow works correctly.

*   `[âœ…]` 44. **`[COMMIT]` Commit All Changes with Comprehensive Message**
    *   `[âœ…]` 44.a. `[DEPS]` After all work is complete, tested, and linted, commit all changes with a comprehensive commit message that describes all fixes.
    *   `[âœ…]` 44.b. `[COMMIT]` Commit message: "fix: remove fallback chain from path construction and require documentKey for document file types - Remove fallback chain (documentKey ?? contributionType ?? fileType) from path_constructor.ts - Require documentKey to be present and non-empty for all document file types, throw error immediately if missing - Fix executeModelCallAndSave to pass documentKey to pathContext from job.payload.document_key - Fix executeModelCallAndSave to use job.payload.document_key in document_completed notifications instead of String(output_type) - Fix processComplexJob to filter out already-completed steps from readySteps before planning - Add comprehensive unit tests for all fixes verifying no fallbacks, defaults, or silent healing - Add integration test verifying complete end-to-end flow with correct file paths and step tracking - Resolves invalid file paths (*_thesis.md instead of proper document keys), recipe restart issues, and frontend state update problems"