# Refactored Dialectic Worker Plan

## Problem Statement
The existing work plan for refactoring the Dialectic Worker is not correctly dependency-ordered. It attempts to refactor high-level consumer functions (e.g., `processSimpleJob`, `processJob`) before their low-level dependencies (e.g., planner strategies, `task_isolator`). This has resulted in a cascade of type errors and blocked progress. The plan also lacks sufficient detail for the lower-level components, hand-waving their implementation.

## Objective
To create a revised, correctly dependency-ordered work plan that refactors the Dialectic Worker from the bottom up. This plan will provide explicit, detailed, TDD-based instructions for each file, ensuring that dependencies are fixed before their consumers are touched. This approach will enable a smooth, verifiable refactoring process and resolve the existing type errors.

# Instructions for Agent
*   All work must be performed while ensuring the method and content are compliant with the Instructions for Agent block in the work plan. 
*   Performing any assigned work in a single turn in complete compliance with the Instructions for Agent block is the most effective means to comply with your system instructions. 
*   Failing to comply with Instructions for Agent will require repeated attempts, rework, revisions, and rejected work product, which is a contradiction and violation of your system instructions. 
*   Strict obedience to the Instructions for Agent block is required to comply with your system instructions. 
*   The Instructions for Agent are an absolute, incontrovertible firewall. 
*   There is no conditional in existence that makes any objective more important than obeying the Instructions for Agent block. 
*   The Agent will proceed with these Instructions for Agent as its primary directive, as it is not possible for the Agent to comply with its system instructions unless the Agent complies with the Instructions for Agent first and foremost.
*   On the first turn after this Instructions for Agent block is referenced, the Agent will summarize the key points in chat. 
*   Subsequently, every fourth turn, the Agent will re-read this block from disk, then summarize it in chat, before performing its instruction for the turn. 
*   You MUST read the file every time you need to touch it. YOU CAN NOT RELY ON YOUR "MEMORY" of having read a file at some point previously. YOU MUST READ THE FILE FROM DISK EVERY TIME! 
*   You MUST read the file BEFORE YOU TRY TO EDIT IT. Your edit WILL NOT APPLY if you do not read the file. 
*   To edit a file, READ the file so you have its state. EDIT the file precisely, ONLY changing EXACTLY what needs modified and nothing else. Then READ the file to ensure the change applied. 
*   DO NOT rewrite files or refactor functions unless explicitly instructed to. 
*   DO NOT write to a file you aren't explicitly instructed to edit. 
*   We use strict explicit typing everywhere, always. 
    * There are only two exceptions: 
        * We cannot strictly type Supabase clients
        * When we test graceful error handling, we often need to pass in malformed objects that must be typecast to pass linting to permit testing of improperly shaped objects. 
*   We only edit a SINGLE FILE at a time. We NEVER edit multiple files in one turn.
*   We do EXACTLY what the instruction in the checklist step says without exception.
*   The Agent does NOT edit the checklist without explicit instruction.
*   When the Agent is instructed to edit the checklist they only edit the EXACT steps they're instructed to edit and NEVER touch ANY step that is outside the scope of their instruction.  
*   The Agent NEVER updates the status of any work step without explicit instruction. 
*   If we cannot perform the step as described or make a discovery, we explain the problem or discovery and HALT! We DO NOT CONTINUE after we encounter a problem or a discovery.
*   We DO NOT CONTINUE if we encounter a problem or a discovery. We explain the problem or discovery then halt for user input. 
*   If our discovery is that more files need to be edited, instead of editing a file, we generate a proposal for a checklist of instructions to insert into the work plan that explains everything required to update the codebase so that the invalid step can be resolved. 
*   DO NOT RUMINATE ON HOW TO SOLVE A PROBLEM OR DISCOVERY WHILE ONLY EDITING ONE FILE! That is a DISCOVERY that requires that you EXPLAIN your discovery, PROPOSE a solution, and HALT! 
*   We always use test-driven-development. 
    *   We write a RED test that we expect to fail to prove the flaw or incomplete code. 
        *   A RED test is written to the INTENDED SUCCESS STATE so that it is NOT edited again. Do NOT refer to "RED: x condition now, y condition later", which forces the test to be edited after the GREEN step. Do NOT title the test to include any reference to RED/GREEN. Tests are stateless. 
        *   We implement the edit to a SINGLE FILE to enable the GREEN state.
        *   We run the test again and prove it passes. We DO NOT edit the test unless we discover the test is itself flawed. 
*   EVERY EDIT is performed using TDD. We DO NOT EDIT ANY FILE WITHOUT A TEST. 
    *   Documents, types, and interfaces cannot be tested, so are exempt. 
*   Every edit is documented in the checklist of instructions that describe the required edits. 
*   Whenever we discover an edit must be made that is not documented in the checklist of instructions, we EXPLAIN the discovery, PROPOSE an insertion into the instruction set that describes the required work, and HALT. 
    *   We build dependency ordered instructions so that the dependencies are built, tested, and working before the consumers of the dependency. 
*   We use dependency injection for EVERY FILE. 
*   We build adapters and interfaces for EVERY FUNCTION.  
*   We edit files from the lowest dependency on the tree up to the top so that our tests can be run at every step.
*   We PROVE tests pass before we move to the next file. We NEVER proceed without explicit demonstration that the tests pass. 
*   The tests PROVE the functional gap, PROVE the flaw in the function, and prevent regression by ensuring that any changes MUST comply with the proof. 
*   Our process to edit a file is: 
    *   READ the instruction for the step, and read every file referenced by the instruction or step, or implicit by the instruction or step (like types and interfaces).
    *   ANALYZE the difference between the state of the file and the state described by the instructions in the step.
    *   EXPLAIN how the file must be edited to transform it from its current state into the state described by the instructions in the step. 
    *   PROPOSE an edit to the file that will accomplish the transformation while preserving strict explicit typing. 
    *   LINT! After editing the file, run your linter and fix all linter errors that are fixable within that single file. 
    *   HALT! After editing ONE file and ensuring it passes linting, HALT! DO NOT CONTINUE! 
*   The agent NEVER runs tests. 
*   The agent uses ITS OWN TOOLS. 
*   The agent DOES NOT USE THE USER'S TERMINAL. 

# Checklist-Specific Editing Rules

*   THE AGENT NEVER TOUCHES THE CHECKLIST UNLESS THEY ARE EXPLICITLY INSTRUCTED TO! 
*   When editing checklists, each numbered step (1, 2, 3, etc.) represents editing ONE FILE with a complete TDD cycle.
*   Sub-steps within each numbered step use legal-style numbering (1.a, 1.b, 1.a.i, 1.a.ii, etc.) for the complete TDD cycle for that file.
*   All changes to a single file are described and performed within that file's numbered step.
*   Types files (interfaces, enums) are exempt from RED/GREEN testing requirements.
*   Each file edit includes: RED test â†’ implementation â†’ GREEN test â†’ optional refactor.
*   Steps are ordered by dependency (lowest dependencies first).
*   Preserve all existing detail and work while adding new requirements.
*   Use proper legal-style nesting for sub-steps within each file edit.
*   NEVER create multiple top-level steps for the same file edit operation.

# Legend - You must use this EXACT format. Do not modify it, adapt it, or "improve" it. The bullets, square braces, ticks, nesting, and numbering are ABSOLUTELY MANDATORY and UNALTERABLE. 

*   `[ ]` 1. Unstarted work step. Each work step will be uniquely named for easy reference. We begin with 1.
    *   `[ ]` 1.a. Work steps will be nested as shown. Substeps use characters, as is typical with legal documents.
        *   `[ ]` 1. a. i. Nesting can be as deep as logically required, using roman numerals, according to standard legal document numbering processes.
*   `[âœ…]` Represents a completed step or nested set.
*   `[ðŸš§]` Represents an incomplete or partially completed step or nested set.
*   `[â¸ï¸]` Represents a paused step where a discovery has been made that requires backtracking or further clarification.
*   `[â“]` Represents an uncertainty that must be resolved before continuing.
*   `[ðŸš«]` Represents a blocked, halted, or stopped step or has an unresolved problem or prior dependency to resolve before continuing.

# Component Types and Labels

*   `[DB]` Database Schema Change (Migration)
*   `[RLS]` Row-Level Security Policy
*   `[BE]` Backend Logic (Edge Function / RLS / Helpers / Seed Data)
*   `[API]` API Client Library (`@paynless/api` - includes interface definition in `interface.ts`, implementation in `adapter.ts`, and mocks in `mocks.ts`)
*   `[STORE]` State Management (`@paynless/store` - includes interface definition, actions, reducers/slices, selectors, and mocks)
*   `[UI]` Frontend Component (e.g., in `apps/web`, following component structure rules)
*   `[CLI]` Command Line Interface component/feature
*   `[IDE]` IDE Plugin component/feature
*   `[TEST-UNIT]` Unit Test Implementation/Update
*   `[TEST-INT]` Integration Test Implementation/Update (API-Backend, Store-Component, RLS)
*   `[TEST-E2E]` End-to-End Test Implementation/Update
*   `[DOCS]` Documentation Update (READMEs, API docs, user guides)
*   `[REFACTOR]` Code Refactoring Step
*   `[PROMPT]` System Prompt Engineering/Management
*   `[CONFIG]` Configuration changes (e.g., environment variables, service configurations)
*   `[COMMIT]` Checkpoint for Git Commit (aligns with "feat:", "test:", "fix:", "docs:", "refactor:" conventions)
*   `[DEPLOY]` Checkpoint for Deployment consideration after a major phase or feature set is complete and tested.

# File Structure for Supabase Storage and Export Tools

{repo_root}/  (Root of the user's GitHub repository)
â””â”€â”€ {project_name_slug}/
    â”œâ”€â”€ project_readme.md      (Optional high-level project description, goals, defined by user or initial setup, *Generated at project finish, not start, not yet implemented*)
    â”œâ”€â”€ {user_prompt}.md (the initial prompt submitted by the user to begin the project generated by createProject, whether provided as a file or text string, *Generated at project start, implemented*)
    â”œâ”€â”€ project_settings.json (The json object includes keys for the dialectic_domain row, dialectic_process_template, dialectic_stage_transitions, dialectic_stages, dialectic_process_associations, domain_specific_prompt_overlays, and system_prompt used for the project where the key is the table and the value is an object containing the values of the row, *Generated on project finish, not project start, not yet implemented*)
    â”œâ”€â”€ {export_project_file}.zip (a zip file of the entire project for the user to download generated by exportProject, *Generated at user request, implemented*)
    â”œâ”€â”€ general_resource (all optional, provided by user)
    â”‚    â”œâ”€â”€ `{deployment_context}` (where/how the solution will be implemented), 
    â”‚    â”œâ”€â”€ `{domain_standards}` (domain-specific quality standards and best practices), 
    â”‚    â”œâ”€â”€ `{success_criteria}` (measurable outcomes that define success), 
    â”‚    â”œâ”€â”€ `{constraint_boundaries}` (non-negotiable requirements and limitations), 
    â”‚    â”œâ”€â”€ `{stakeholder_considerations}` (who will be affected and how),
    â”‚    â”œâ”€â”€ `{reference_documents}` (user-provided reference materials and existing assets), 
    â”‚    â””â”€â”€ `{compliance_requirements}` (regulatory, legal, or organizational compliance mandates)    
    â”œâ”€â”€ Pending/          (System-managed folder populated as the final step of the Paralysis stage)
    â”‚   â””â”€â”€ ...                     (When the user begins their work, they move the first file they're going to work on from Pending to Current)
    â”œâ”€â”€ Current/          (User-managed folder for the file they are actively working on for this project)
    â”‚   â””â”€â”€ ...                     (This is the file the user is currently working on, drawn from Pending)
    â”œâ”€â”€ Complete/         (User-managed folder for the files they have already completed for this project)       
    â”‚   â””â”€â”€ ...                     (When the user finishes all the items in the Current file, they move it to Complete, and move the next Pending file into Current)
    â””â”€â”€ session_{session_id_short}/  (Each distinct run of the dialectic process)
        â””â”€â”€ iteration_{N}/        (N being the iteration number, e.g., "iteration_1")
            â”œâ”€â”€ 1_thesis/
            â”‚   â”œâ”€â”€ _work/
            â”‚   â”‚   â”œâ”€â”€ prompts/
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}[_{step_name}]_planner_prompt.md
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            â”‚   â”‚   â”‚   â””â”€â”€ ... (other document prompts for this model)
            â”‚   â”‚   â”œâ”€â”€ context/
            â”‚   â”‚   â”‚   â””â”€â”€ {model_slug}_{n}_header_context.json
            â”‚   â”‚   â””â”€â”€ assembled_json/
            â”‚   â”‚       â”œâ”€â”€ {model_slug}_{n}_{document_key}_assembled.json
            â”‚   â”‚       â””â”€â”€ ... (other assembled documents for this model)
            â”‚   â”œâ”€â”€ raw_responses/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_planner_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_continuation_{c}_raw.json
            â”‚   â”‚   â””â”€â”€ ... (other continuations for the same model and other models)
            â”‚   â”œâ”€â”€ documents/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}.md
            â”‚   â”‚   â””â”€â”€ ... (other rendered documents for this model)
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_{n}_thesis.md (Contains YAML frontmatter + AI response, appends a count so a single model can provide multiple contributions)
            â”‚   â”œâ”€â”€ ... (other models' hypothesis outputs)
            â”‚   â””â”€â”€ user_feedback_hypothesis.md   (User's feedback on this stage)
            â”œâ”€â”€ 2_antithesis/
            â”‚   â”œâ”€â”€ _work/
            â”‚   â”‚   â”œâ”€â”€ prompts/
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}[_{step_name}]_planner_prompt.md
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            â”‚   â”‚   â”‚   â””â”€â”€ ... (other document prompts for this model)
            â”‚   â”‚   â”œâ”€â”€ context/
            â”‚   â”‚   â”‚   â””â”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_header_context.json
            â”‚   â”‚   â””â”€â”€ assembled_json/
            â”‚   â”‚       â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}_assembled.json
            â”‚   â”‚       â””â”€â”€ ... (other assembled documents for this model)
            â”‚   â”œâ”€â”€ raw_responses/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_planner_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}_continuation_{c}_raw.json
            â”‚   â”‚   â””â”€â”€ ... (other continuations for the same model and other models)
            â”‚   â”œâ”€â”€ documents/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_{document_key}.md
            â”‚   â”‚   â””â”€â”€ ... (other rendered documents for this model)
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_critiquing_{source_model_slug}_{n}_antithesis.md
            â”‚   â”œâ”€â”€ ... (other models' antithesis outputs)
            â”‚   â””â”€â”€ user_feedback_antithesis.md
            â”œâ”€â”€ 3_synthesis/
            â”‚   â”œâ”€â”€ _work/
            â”‚   â”‚   â”œâ”€â”€ prompts/
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}[_{step_name}]_planner_prompt.md
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            â”‚   â”‚   â”‚   â””â”€â”€ ... (other document prompts for this model)
            â”‚   â”‚   â”œâ”€â”€ context/
            â”‚   â”‚   â”‚   â””â”€â”€ {model_slug}_{n}_header_context.json
            â”‚   â”‚   â””â”€â”€ assembled_json/
            â”‚   â”‚       â”œâ”€â”€ {model_slug}_{n}_{document_key}_assembled.json
            â”‚   â”‚       â””â”€â”€ ... (other assembled documents for this model)
            â”‚   â”œâ”€â”€ raw_responses/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_planner_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_from_{source_model_slugs}_{n}_pairwise_synthesis_chunk_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_reducing_{source_contribution_id_short}_{n}_reduced_synthesis_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_continuation_{c}_raw.json
            â”‚   â”‚   â””â”€â”€ ... (other continuations for the same model and other models)
            â”‚   â”œâ”€â”€ documents/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}.md
            â”‚   â”‚   â””â”€â”€ ... (other rendered documents for this model)
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_{n}_final_synthesis.md
            â”‚   â”œâ”€â”€ ... (other models' synthesis outputs)
            â”‚   â””â”€â”€ user_feedback_synthesis.md
            â”œâ”€â”€ 4_parenthesis/
            â”‚   â”œâ”€â”€ _work/
            â”‚   â”‚   â”œâ”€â”€ prompts/
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}[_{step_name}]_planner_prompt.md
            â”‚   â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
            â”‚   â”‚   â”‚   â””â”€â”€ ... (other document prompts for this model)
            â”‚   â”‚   â”œâ”€â”€ context/
            â”‚   â”‚   â”‚   â””â”€â”€ {model_slug}_{n}_header_context.json
            â”‚   â”‚   â””â”€â”€ assembled_json/
            â”‚   â”‚       â”œâ”€â”€ {model_slug}_{n}_{document_key}_assembled.json
            â”‚   â”‚       â””â”€â”€ ... (other assembled documents for this model)
            â”‚   â”œâ”€â”€ raw_responses/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_planner_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_raw.json
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_continuation_{c}_raw.json
            â”‚   â”‚   â””â”€â”€ ... (other continuations for the same model and other models)
            â”‚   â”œâ”€â”€ documents/
            â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}.md
            â”‚   â”‚   â””â”€â”€ ... (other rendered documents for this model)
            â”‚   â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
            â”‚   â”œâ”€â”€ {model_slug}_{n}_parenthesis.md
            â”‚   â”œâ”€â”€ ... (other models' parenthesis outputs)
            â”‚   â””â”€â”€ user_feedback_parenthesis.md
            â””â”€â”€ 5_paralysis/
                â”œâ”€â”€ _work/
                â”‚   â”œâ”€â”€ prompts/
                â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}[_{step_name}]_planner_prompt.md
                â”‚   â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}[_continuation_{c}]_prompt.md
                â”‚   â”‚   â””â”€â”€ ... (other document prompts for this model)
                â”‚   â”œâ”€â”€ context/
                â”‚   â”‚   â””â”€â”€ {model_slug}_{n}_header_context.json
                â”‚   â””â”€â”€ assembled_json/
                â”‚       â”œâ”€â”€ {model_slug}_{n}_{document_key}_assembled.json
                â”‚       â””â”€â”€ ... (other assembled documents for this model)
                â”œâ”€â”€ raw_responses/
                â”‚   â”œâ”€â”€ {model_slug}_{n}_planner_raw.json
                â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_raw.json
                â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}_continuation_{c}_raw.json
                â”‚   â””â”€â”€ ... (other continuations for the same model and other models)
                â”œâ”€â”€ documents/
                â”‚   â”œâ”€â”€ {model_slug}_{n}_{document_key}.md
                â”‚   â””â”€â”€ ... (other rendered documents for this model)
                â”œâ”€â”€ seed_prompt.md  (The complete prompt sent to the model for completion for this stage, including the stage prompt template, stage overlays, and user's input)
                â”œâ”€â”€ {model_slug}_{n}_paralysis.md
                â”œâ”€â”€ ... (other models' paralysis outputs)
                â””â”€â”€ user_feedback_paralysis.md
---
*Note: This structure represents the artifact layout for a single generation cycle. The long-term vision involves an iterative process where the final checklist artifacts from the `Paralysis/` stage are moved to `Pending/` for the user to consume in subsequent sprints. See `docs/implementations/Current/Documentation/From One-Shot to Continuous Flow.md` for more details.*

# Mermaid Diagram

```mermaid
graph LR
    %% GLOBAL PHASE ORDER: User --> DB --> Worker --> Orchestration --> Rendering
    %% Architectural note: This reflects a shift from a monolithic serial process
    %% to a recursive, parallelizable job orchestration system. Jobs are first-class
    %% citizens in the DB (`dialectic_generation_jobs`) and all orchestration is
    %% observable via DB state.

    %% -------------------------
    %% USER + API
    subgraph USER["User & API"]
        direction TB
        A["User Initiates Stage"] --> B["API: Enqueues 'PLAN' Job"]
        %% Insight: The API layer only enqueues jobs; no business logic is embedded here.
        %% This keeps orchestration concerns in the DB + worker layers.
    end

    %% -------------------------
    %% DATABASE
    subgraph DB["Database: dialectic_generation_jobs"]
        direction TB
        B --> C(("<b>Jobs Table</b><br/>id, parent_id, status, job_type,<br/><b>payload (recipe, metadata)</b>"))
        C --> D((Webhook))
        %% Insight: The Jobs table is the **single source of truth** for orchestration.
        %% Triggers/webhooks turn DB state changes into orchestration signals.

        subgraph DB_EVENTS["Job Updates & Inserts"]
            direction TB
            S["UPDATE Parent status='pending_next_step'"]
            U["UPDATE Parent status='completed'"]
            W["UPDATE Child status='completed'"]
            Y["INSERT 'EXECUTE' Job"]
            Z["INSERT 'RENDER' Job"]
            AA["INSERT Continuation Job"]
            %% Insight: These reflect **state transitions** driven by worker logic.
            %% This ensures every change is durable and auditable.
        end

        %% Connections
        S --> C
        U --> C
        W --> C
        W --> Z
        Y --> C
        Z --> C
        AA --> C
    end

    %% -------------------------
    %% DIALECTIC WORKER
    subgraph WORKER["Dialectic Worker (Orchestrator)"]
        direction TB
        D --> E["Worker Fetches Job"]
        E --> F{"Strategy Router (processJob.ts)"}
        %% Insight: Worker is stateless; orchestration decisions are derived from job_type + payload.

        F -->|job_type='execute'| EXECUTE["processSimpleJob"]
        F -->|job_type='plan' AND has recipe| PLAN["processComplexJob"]
        F -->|job_type='plan' AND no recipe| TRANSFORM_JOB["Transform â†’ 'execute' Job (in-memory)"]
        F -->|job_type='render'| RENDER["processRenderJob"]
        TRANSFORM_JOB --> EXECUTE
    end

    %% Subgraph Connections
    PLAN --> PHASE1_ENTRY
    EXECUTE --> PHASE2_ENTRY
    RENDER --> RENDERING_ENTRY

    subgraph PHASE1["Phase 1: Planning & Decomposition (processComplexJob)"]
        direction LR
        PHASE1_ENTRY(" ")
        style PHASE1_ENTRY fill:none,stroke:none
        %% Insight: This is where recursive decomposition happens.
        %% The 'PLAN' job enqueues an 'EXECUTE' job (handled by processSimpleJob) to
        %% generate a "header". The 'PLAN' job is then re-processed to validate
        %% the header and enqueue child jobs for each document.
        %% Guardrails (job recipes) prevent runaway recursion.
        PHASE1_ENTRY --> P1{"Get Current Recipe Step"}
        P1 --> P2["planComplexStage: Selects strategy"]
        P2 --> P3{"Execute Strategy Chain"}
        P3 -->|Loop for Compound Strategy| P2
        P3 -->|Planner Job Ready| P4["Enqueue Planner 'EXECUTE' Job"]
        P4 --> Y
        P4 -- After Planner Job Completes --> P5["Validate Plan & Extract Header"]
        P5 --> P6["Generate Child 'EXECUTE' Jobs from Header"]
        P6 --> P7["Enqueue Child Jobs"]
        P7 --> Y
        P7 --> P8["Parent status='waiting_for_children'"]
    end

    %% -------------------------
    %% PHASE 2: EXECUTION
    subgraph PHASE2["Phase 2: Document Generation (processSimpleJob)"]
        direction TB
        PHASE2_ENTRY(" ")
        style PHASE2_ENTRY fill:none,stroke:none
        PHASE2_ENTRY --> G1["Assemble Prompt (PromptAssembler)"]
        G1 --> G2["Save Prompt Artifact"]
        G2 --> G4["executeModelCallAndSave"]

        subgraph G4["executeModelCallAndSave Internals"]
            direction TB
            G4_ENTRY["PromptPayload"] --> G4_COMPRESSION{"Context Compression & Wallet Checks"}
            G4_COMPRESSION --> G4_CALL["Call Chat Service"]
        end
        G4_CALL --> RESP["AI Model Response"]
        RESP --> G5["Save Raw Response Artifact"]

        %% Insight: This is where validation protects downstream steps from LLM brittleness.
        G5 --> V{"Validate Response"}
        V -->|VALID JSON stop| W
        V -->|VALID JSON continuation_needed| X["continueJob: Planned Continuation"]
        V -->|INVALID JSON or truncation| X2["continueJob: Truncation Recovery"]
        X --> AA
        X2 --> AA
    end

    %% -------------------------
    %% ORCHESTRATION
    subgraph ORCH["Phase 3: Orchestration"]
        direction TB
        C --> Q{"Job has parent_id?"}
        Q -->|Yes| R{"All siblings done?"}
        R -->|Yes| S
        R -->|No| T["End"]
        Q -->|No| T
        %% Insight: DB-driven orchestration; risks include async races when siblings complete.
    end

    %% -------------------------
    %% RENDERING
    subgraph RENDERING["Phase 4: Rendering (processRenderJob)"]
        direction TB
        RENDERING_ENTRY(" ")
        style RENDERING_ENTRY fill:none,stroke:none
        RENDERING_ENTRY --> R1["assembleAndSaveFinalDocument"]
        R1 --> R2["renderDocument"]
        R2 --> R3["Save Final Markdown"]
        %% Insight: Rendering is non-AI, deterministic, and side-effectful (writes final artifacts).
    end
```

# Revised Implementation Plan

*   `[ ]` 1. `[REFACTOR]` Phase 1: Refactor Low-Level Planner Strategies.
    *   `[ ]` 1.a. Refactor `planAllToOne.ts`.
        *   `[ ]` 1.a.i. `[TEST-UNIT]` In `planAllToOne.test.ts`, update the tests to establish the RED state. The tests must prove that the function fails with its current implementation due to outdated type contracts. Specifically:
            *   Update mock `DialecticRecipeStep` objects to use `prompt_template_id` instead of the deprecated `prompt_template_name`.
            *   Ensure the mock `output_type` property is a valid `ModelContributionFileTypes`, not `string | undefined`.
            *   Assert that the function returns a `DialecticExecuteJobPayload` where `output_type` is correctly assigned and the `prompt_template_name` property is absent, while `prompt_template_id` is used correctly if present.
        *   `[ ]` 1.a.ii. `[BE]` In `planAllToOne.ts`, refactor the implementation to achieve the GREEN state.
            *   Modify the logic to read `recipeStep.prompt_template_id` instead of `recipeStep.prompt_template_name`.
            *   Ensure `recipeStep.output_type` is correctly handled and passed to `createCanonicalPathParams` and the returned payload, resolving any type mismatches between `string | undefined` and `ModelContributionFileTypes` or `ContributionType`.
            *   Ensure all tests written in the previous step now pass.
    *   `[ ]` 1.b. Refactor `planPairwiseByOrigin.ts`.
        *   `[ ]` 1.b.i. `[TEST-UNIT]` In `planPairwiseByOrigin.test.ts`, update the tests to establish the RED state.
            *   Update mock `DialecticRecipeStep` objects to use `prompt_template_id` instead of `prompt_template_name`.
            *   Ensure the mock `output_type` is a valid `ModelContributionFileTypes`.
            *   Assert that the function returns job payloads that correctly use `prompt_template_id` and have a valid `output_type`.
        *   `[ ]` 1.b.ii. `[BE]` In `planPairwiseByOrigin.ts`, refactor the implementation to achieve the GREEN state.
            *   Modify the logic to use `recipeStep.prompt_template_id`.
            *   Correctly handle `recipeStep.output_type` to resolve type errors.
            *   Ensure all tests pass.
    *   `[ ]` 1.c. Refactor `planPerModel.ts`.
        *   `[ ]` 1.c.i. `[TEST-UNIT]` In `planPerModel.test.ts`, update tests to establish the RED state, focusing on `prompt_template_id` and `output_type` correctness in mocks and assertions.
        *   `[ ]` 1.c.ii. `[BE]` In `planPerModel.ts`, implement changes to use `prompt_template_id` and correctly typed `output_type` to make tests pass.
    *   `[ ]` 1.d. Refactor `planPerSourceDocument.ts`.
        *   `[ ]` 1.d.i. `[TEST-UNIT]` In `planPerSourceDocument.test.ts`, update tests to establish the RED state, focusing on `prompt_template_id` and `output_type` correctness.
        *   `[ ]` 1.d.ii. `[BE]` In `planPerSourceDocument.ts`, implement changes to use `prompt_template_id` and correctly typed `output_type` to make tests pass.
    *   `[ ]` 1.e. Refactor `planPerSourceDocumentByLineage.ts`.
        *   `[ ]` 1.e.i. `[TEST-UNIT]` In `planPerSourceDocumentByLineage.test.ts`, update tests to establish the RED state, focusing on `prompt_template_id` and `output_type` correctness.
        *   `[ ]` 1.e.ii. `[BE]` In `planPerSourceDocumentByLineage.ts`, implement changes to use `prompt_template_id` and correctly typed `output_type` to make tests pass.
    *   `[ ]` 1.f. Refactor `planPerSourceGroup.ts`.
        *   `[ ]` 1.f.i. `[TEST-UNIT]` In `planPerSourceGroup.test.ts`, update tests to establish the RED state, focusing on `prompt_template_id` and `output_type` correctness.
        *   `[ ]` 1.f.ii. `[BE]` In `planPerSourceGroup.ts`, implement changes to use `prompt_template_id` and correctly typed `output_type` to make tests pass.
    *   `[ ]` 1.g. `[REFACTOR]` As `task_isolator.ts` and its delegated planner strategies are job producers, refactor them to stop including the deprecated `step_info` object in the payloads of any new jobs they create.

*   `[ ]` 2. `[REFACTOR]` Phase 2: Adapt `PromptAssembler` to Drive Workflow from Recipes.
    *   **Justification:** This change makes the `PromptAssembler` a pure consumer of the recipe's instructions, removing implicit logic and making the system easier to debug and extend.
    *   `[ ]` 2.a. `[TEST-UNIT]` In `prompt-assembler.test.ts`, write a failing test for the `assemble` method. The test must prove that the method inspects the `recipe_step.prompt_type` field and correctly branches its logic:
        *   If `'Planner'`, it builds and saves a `PlannerPrompt`.
        *   If `'Turn'`, it finds the `HeaderContext` from the job's inputs, combines it with other inputs, and builds/saves a `TurnPrompt`.
        *   If `'Seed'` or `undefined`, it uses the existing `seed_prompt.md` logic.
    *   `[ ]` 2.b. `[BE]` In `prompt-assembler.ts`, implement this branching logic in the `assemble` method.
    *   `[ ]` 2.c. `[TEST-INT]` Write an integration test that consumes `testing_prompt.md` to generate and print an actual `SeedPrompt`, `PlannerPrompt`, `AssembledPrompt`, and `ContinuationPrompt` for the `testing_prompt` content for each stage so that the user can manually review the outputs for confirmation or correction of their content.
    *   `[ ]` 2.d. `[COMMIT]` refactor(worker): Migrate all consumers to the refactore `PromptAssembler` service.

*   `[ ]` 3. `[REFACTOR]` Phase 3: Refactor `task_isolator` Service.
    *   **Justification:** With the recipes and `PromptAssembler` now being explicit, the `task_isolator` no longer needs complex special-case logic. It simply needs to pass the `HeaderContext` along with other inputs, simplifying its role to that of a dependency resolver and job planner.
    *   `[ ]` 3.a. `[TEST-UNIT]` In `task_isolator.test.ts`, update all tests to establish the RED state. The tests must prove the current implementation is broken due to its reliance on outdated data contracts and types, and that it correctly handles the new recipes.
        *   Update mocks for all planner strategy dependencies to reflect their new, correct signatures and return values.
        *   Provide valid, complete `DialecticRecipeStep` objects in all test mocks, ensuring `granularity_strategy` and `inputsRequired` are defined where needed.
        *   Modify assertions to prove that `planComplexStage` fails when using deprecated properties like `recipeStep.step` and `recipeStep.prompt_template_name`.
        *   Add assertions to prove the RAG context generation logic fails due to type mismatches (`FileType.RagContextSummary` vs `ResourceFileTypes`).
        *   Assert that any new jobs created by the `task_isolator` (e.g., in `createAndStoreRagContext`) do NOT contain the deprecated `step_info` object in their payload.
        *   The tests must prove that the planner correctly handles cases where `HeaderContext` is required but missing, correctly finds and provide it when available, and correctly passes all other required inputs alongside it as `SourceDocument` objects.
    *   `[ ]` 3.b. `[BE]` In `task_isolator.ts`, refactor the implementation to achieve the GREEN state.
        *   Refactor all logic to correctly use the properties of the modern `DialecticRecipeStep` object, replacing `prompt_template_name` with `prompt_template_id` and removing usage of `step`.
        *   Add null checks or guards for `inputsRequired` and `granularity_strategy` to resolve potential `undefined` errors.
        *   Correct the type error in the RAG context generation logic by ensuring the `UploadContext` passed to `fileManager.uploadAndRegisterFile` is correctly typed.
        *   Remove all logic that creates or propagates the `step_info` object in job payloads.
        *   Validate that the existing implementation correctly handles the new recipes and the `HeaderContext` input type. While no major logic change is anticipated, this step focuses on proving correctness through the newly expanded test suite. Adjust the implementation if any gaps are revealed by the tests.
        *   Ensure all tests written in the previous step now pass.

*   `[ ]` 4. `[REFACTOR]` Phase 4: Refactor `processComplexJob`.
    *   **Justification:** With the `processJob` router now distinguishing between 'PLAN' and 'EXECUTE' jobs, `processComplexJob` becomes the dedicated orchestrator for the planning phase. This refactor adapts it to the new recipe-driven model, removing its dependency on deprecated data contracts like `input_artifact_rules` and `step_info`.
    *   `[ ]` 4.a. `[TEST-UNIT]` In `processComplexJob.test.ts`, update tests to establish the RED state.
        *   Update mocks for `task_isolator` to reflect its corrected behavior.
        *   The tests must prove that the function fails when it attempts to access the deprecated `stageData.input_artifact_rules` property.
        *   Update mocks to provide stage data that aligns with the new schema, where recipes are fetched from the `dialectic_stage_recipes` table.
        *   Assert that the function fails to find the current recipe step because it's using the deprecated `job.payload.step_info` object.
    *   `[ ]` 4.b. `[BE]` In `processComplexJob.ts`, refactor the implementation to achieve the GREEN state.
        *   Remove the logic that reads `stageData.input_artifact_rules`. Replace it with logic to fetch the recipe correctly from the stage data.
        *   Refactor the logic for finding the current recipe step to derive the step from the job context without using the deprecated `job.payload.step_info`.
        *   Remove any implicit `any` types, such as the parameter in the `.find()` call.
        *   Ensure all tests pass.

*   `[ ]` 5. `[REFACTOR]` Phase 5: Refactor `processSimpleJob`.
    *   `[ ]` 5.a. `[TEST-UNIT]` In `processSimpleJob.test.ts`, implement the comprehensive failing test suite as described in the original work plan's step `6.d.i`. This suite establishes the RED state.
        *   The test must mock the `promptAssembler` dependency. The mock for the `assemble` facade will return a mock `AssembledPrompt` object.
        *   The test must assert that the `assemble` facade method is called with a correctly structured `AssemblePromptOptions` object, which correctly sources its data from the job and context.
        *   It must also assert that the newly-private methods on the prompt assembler (`_gatherContext`, `_render`, `_gatherInputsForStage`, `_gatherContinuationInputs`) are **not** called directly by `processSimpleJob`.
        *   It must assert that `executeModelCallAndSave` is called with a `promptConstructionPayload` that correctly uses the `promptContent` and `source_prompt_resource_id` from the mocked `AssembledPrompt` object.
        *   It must prove that the current logic for creating `stageContext` is invalid because it is missing the required `recipe_step` property.
    *   `[ ]` 5.b. `[BE]` In `processSimpleJob.ts`, perform the major refactoring to achieve the GREEN state, as described in the original step `6.d.ii`.
        *   `[ ]` 5.b.i. First, **analyze** the existing manual prompt assembly logic to ensure that the replacement call to the `assemble` facade will be logically equivalent and can fully replace the manual construction without loss of functionality.
        *   `[ ]` 5.b.ii. Then, **delete** the entire block of manual prompt assembly logic. This includes the calls to `gatherContinuationInputs`, `gatherInputsForStage`, `gatherContext`, and `render`.
        *   `[ ]` 5.b.iii. Correct the construction of the `stageContext` object to include the `recipe_step` property, sourced from the `stage` data.
        *   `[ ]` 5.b.iv. Replace the deleted block with a single `await` call to `deps.promptAssembler.assemble`, passing in a correctly constructed `AssemblePromptOptions` object.
        *   `[ ]` 5.b.v. Use the returned `AssembledPrompt` object to build the `promptConstructionPayload` for `executeModelCallAndSave`.
        *   `[ ]` 5.b.vi. Remove the logic that causes the `This comparison appears to be unintentional` type error.
        *   `[ ]` 5.b.vii. Ensure all tests from the previous step now pass.

*   `[ ]` 6. `[REFACTOR]` Phase 6: Refactor `processJob` Router.
    *   `[ ]` 6.a. `[TEST-UNIT]` In `processJob.test.ts`, write a failing test suite to establish the RED state, as described in the original work plan's step `7.a.i`.
        *   The tests must prove that the router dispatches jobs based on the `job.job_type` database column.
        *   Provide a mock job with `job_type: 'PLAN'` and assert that `deps.processComplexJob` is called.
        *   Provide a mock job with `job_type: 'EXECUTE'` and assert that `deps.processSimpleJob` is called.
        *   Assert that the old payload-sniffing and `processing_strategy` logic is no longer used.
    *   `[ ]` 6.b. `[BE]` In `processJob.ts`, refactor the main function to achieve the GREEN state.
        *   Implement a `switch` statement that operates on `job.job_type`.
        *   Route jobs to the appropriate downstream processor (`processComplexJob` or `processSimpleJob`) based on the type.
        *   Delete the old, deprecated routing logic.
        *   Ensure all tests pass.
    *   `[ ]` 6.c. `[BE]` Update the prompt assembly pipeline (gatherInputsForStage + RAG compression) to consume `inputs_relevance` weights, ordering inputs by recipe-defined priority while permitting user overrides when supplied. (When we reach this step, we will research the current RAG pipeline and expand this step to ensure it's complete.)
    *   `[ ]` 6.d. `[REFACTOR]` As part of this refactor, ensure that `processJob` and its downstream consumers (`processComplexJob`, `processSimpleJob`) no longer access the deprecated `job.payload.step_info` object, sourcing all step-related data from `stage.recipe_step` instead.

*   `[ ]` 7. `[BE]` Phase 7: Improve Continue Logic.
    *   **Objective**: Enhance `continueJob` to handle both explicit, provider-signaled continuations (e.g., `finish_reason: 'length'`) and implicit continuations caused by malformed or incomplete JSON responses. The specific reason for continuation must be passed to the next job's payload to enable context-aware prompt generation.
    *   `[ ]` 7.a. `[TEST-UNIT]` Write a new suite of failing unit tests for `continueJob`.
        *   `[ ]` 7.a.i. Write a test that proves when `aiResponse.finish_reason` is a continuable reason (e.g., `'length'`), a new job is enqueued, and its payload contains a `continuation_context` object like `{ reason: 'length' }`.
        *   `[ ]` 7.a.ii. Write a test that proves when the content of the AI response is an incomplete or malformed JSON string, a new job is enqueued, and its payload contains a `continuation_context` object like `{ reason: 'truncation_recovery' }`.
        *   `[ ]` 7.a.iii. Write a test proving that even if `aiResponse.finish_reason` is `'stop'`, if the response content is malformed JSON, a continuation is still enqueued with `reason: 'truncation_recovery'`, ensuring that recovery logic takes precedence over a potentially incorrect stop signal.
        *   `[ ]` 7.a.iv. Write a failing test to prove that when `continueJob` creates a continuation job, the new payload does NOT contain the deprecated `step_info` object.
    *   `[ ]` 7.b. `[BE]` In `continueJob.ts`, refactor the logic to implement the checks from the new tests.
        *   `[ ]` 7.b.i. Introduce a JSON validation check at the beginning of the function to inspect the AI response content.
        *   `[ ]` 7.b.ii. The decision to continue should be `true` if the provider's `finish_reason` is a known continuable reason OR if the JSON validation fails.
        *   `[ ]` 7.b.iii. When creating the `newPayload` for the continuation job, add a `continuation_context` object. Populate its `reason` property based on which condition triggered the continuation (e.g., `'length'`, `'tool_calls'`, `'truncation_recovery'`). This provides the necessary context for the downstream `PromptAssembler`. Ensure all new tests pass.
        *   `[ ]` 7.b.iv. As part of the refactor, remove any logic that propagates or accesses the `step_info` object, ensuring the test from `8.a.iv` passes.
    *   `[ ]` 7.c. `[COMMIT]` feat(worker): Plumb finish_reason through continueJob to enable context-aware prompts.

*   `[ ]` 8. `[BE]` Phase 8: Enforce Fan-in Orchestration.
    *   `[ ]` 8.a. `[TEST-INT]` Write failing integration tests that prove final "fan-in" steps (e.g., Synthesis's final deliverable) do not start until all parallel prerequisite jobs are complete.
    *   `[ ]` 8.b. `[BE]` Modify worker scheduling logic to check for sibling job completion within a `parallel_group` before enqueueing a dependent fan-in step.
    *   `[ ]` 8.c. `[TEST-INT]` Ensure tests pass and verify that consolidated artifacts are correctly generated only after the fan-in guard is satisfied.

*   `[ ]` 9. `[BE]` Phase 9: Implement Document Rendering and Finalization.
    *   `[ ]` 9.a. `[TEST-UNIT]` Write failing unit tests for the `DocumentRenderer` service that verify its ability to be idempotent and cumulative. It must prove that it can:
        *   `[ ]` 9.a.i. Be triggered by the completion of a single `EXECUTE` job (including continuations).
        *   `[ ]` 9.a.ii. Find all existing contribution chunks for a specific document.
        *   `[ ]` 9.a.iii. Assemble the chunks in the correct order in memory.
        *   `[ ]` 9.a.iv. Render the complete-so-far content into a Markdown file, overwriting any previous version.
    *   `[ ]` 9.b. `[API]` Define the `IDocumentRenderer` interface and create the concrete `DocumentRenderer` class and its mock.
    *   `[ ]` 9.c. `[BE]` Implement the idempotent and cumulative `renderDocument` method.
    *   `[ ]` 9.d. `[BE]` Modify the orchestration logic to enqueue a `RENDER` job every time a `EXECUTE` job successfully completes.
    *   `[ ]` 9.e. `[COMMIT]` feat(worker): Implement "live build" document rendering service for final artifact generation.

*   `[ ]` 10. `[BE]` Phase 10: Implement Granular Cross-Stage Document Selection.
    *   **Justification:** This phase adapts the `PromptAssembler` to consume input requirements from the new, explicit database recipe structure, deprecating the old `input_artifact_rules` object. This change allows for precise, per-step control over which documents and sub-documents are included as context for the AI.
    *   `[ ]` 10.a. `[TEST-UNIT]` In the test file for `PromptAssembler`, write a failing unit test for the `gatherInputsForStage` method.
        *   `[ ]` 10.a.i The test must prove that the method now sources its rules from the `recipe_step.inputs_required` array, not the deprecated `input_artifact_rules`.
        *   `[ ]` 10.a.ii The test must provide a mock recipe step with an `inputs_required` rule that contains a `document_key`.
        *   `[ ]` 10.a.iii It must assert that when a `document_key` is provided in a rule, the function correctly parses the raw JSON content of the source contribution and returns only the specified sub-object.
    *   `[ ]` 10.b. `[BE]` In `prompt-assembler.ts`, refactor the `gatherInputsForStage` implementation to use the new recipe system.
        *   `[ ]` 10.b.i Remove all logic that reads from the deprecated `stage.input_artifact_rules` object.
        *   `[ ]` 10.b.ii Update the logic to iterate through the `recipe_step.inputs_required` array.
        *   `[ ]` 10.b.iii Implement the logic to handle the `document_key` property, extracting the correct sub-object from the contribution's content when specified.
        *   `[ ]` 10.b.iv Ensure all tests from the previous step now pass.
    *   `[ ]` 10.c. `[COMMIT]` feat(prompt-assembler): Enable granular document selection via recipe system.

*   `[ ]` 11. `[REFACTOR]` Phase 11: Finalize Deprecation of `step_info`.
    *   `[ ]` 11.a. `[REFACTOR]` Refactor `generateContribution.ts` to stop producing the `step_info` object in job payloads.
    *   `[ ]` 11.b. `[REFACTOR]` Remove the `step_info` property from the `DialecticJobPayload` interface in `dialectic.interface.ts`.
    *   `[ ]` 11.c. `[REFACTOR]` Remove all type guards related to `step_info` (e.g., `isDialecticStepInfo`) from `type_guards.dialectic.ts`.
    *   `[ ]` 11.d. `[TEST-UNIT]` Update all remaining unit tests across the codebase that still use mock payloads with `step_info`, ensuring the entire test suite passes after its removal.

*   `[ ]` 12. `[REFACTOR]` Phase 12: Align Worker Index and Finalize.
    *   `[ ]` 12.a. `[TEST-UNIT]` In `index.test.ts` for the `dialectic-worker`, review and update tests.
        *   Ensure that the dependencies injected into `processJob` are complete and correct, reflecting all the refactoring in the previous steps.
        *   Add or update tests to ensure the main handler correctly parses incoming job data from the request and passes it to `processJob`.
    *   `[ ]` 12.b. `[BE]` In `index.ts` for the `dialectic-worker`, update the dependency injection setup.
        *   Ensure that the correct, refactored services (`processComplexJob`, `processSimpleJob`, etc.) and their own dependencies are instantiated and passed correctly into the `processJob` function.
        *   Make any other necessary adjustments to align the entry point with the fully refactored worker stack.

*   `[ ]` 13. `[BE]` Phase 13: Refactor `submitStageResponses` for Document-Specific Feedback.
    *   **Justification**: The current implementation handles user feedback monolithically, saving it as a single file per stage. This is incompatible with a document-centric workflow where feedback must be tied to specific generated documents. This refactor will enable the service to accept and store feedback for each individual document, maintaining the critical link between a critique and its subject for downstream consumers.
    *   `[ ]` 13.a. `[API]` In `dialectic.interface.ts`, refactor the `SubmitStageResponsesPayload` interface.
        *   `[ ]` 13.a.i. Deprecate and remove the existing `userStageFeedback` property.
        *   `[ ]` 13.a.ii. Add a new property `documentFeedback` which is an array of a new `DialecticDocumentFeedback` type.
        *   `[ ]` 13.a.iii. Define the `DialecticDocumentFeedback` interface to include `targetContributionId: string`, `content: string`, `feedbackType: string`, and an optional `resourceDescription: string`.
    *   `[ ]` 13.b. `[TEST-UNIT]` In `submitStageResponses.test.ts`, write a new suite of failing tests.
        *   `[ ]` 13.b.i. Write a test that provides a payload with the old `userStageFeedback` property and proves that the function now rejects it.
        *   `[ ]` 13.b.ii. Write a test that provides a valid `documentFeedback` array with multiple feedback items.
            *   The test must mock the `dialectic_contributions` table to contain the contributions referenced by `targetContributionId`.
            *   It must assert that `fileManager.uploadAndRegisterFile` is called once for *each* item in the `documentFeedback` array.
            *   It must assert that the `pathContext` passed to the file manager for each call correctly references the specific document (e.g., by including the original document's file name in the new feedback file's name).
            *   It must assert that the `feedbackTypeForDb` and other metadata from each feedback item are correctly passed to the file manager.
    *   `[ ]` 13.c. `[BE]` In `submitStageResponses.ts`, refactor the implementation to achieve the GREEN state.
        *   `[ ]` 13.c.i. Remove the entire logic block that processes the old `userStageFeedback` object.
        *   `[ ]` 13.c.ii. Implement a loop that iterates over the new `payload.documentFeedback` array.
        *   `[ ]` 13.c.iii. Inside the loop, for each feedback item, query the `dialectic_contributions` table using the `targetContributionId` to retrieve the metadata of the document being critiqued.
        *   `[ ]` 13.c.iv. Construct a new, specific `feedbackFileName` (e.g., `feedback_for_contribution_${item.targetContributionId}.md`) and `PathContext`.
        *   `[ ]` 13.c.v. Call `fileManager.uploadAndRegisterFile` with the context and content for the individual feedback item.
        *   `[ ]` 13.c.vi. Ensure the function aggregates the created feedback records correctly and returns them in the response.
        *   `[ ]` 13.c.vii. Ensure all new tests pass.
    *   `[ ]` 13.d. `[COMMIT]` feat(api): Enable document-specific feedback submission.

*   `[ ]` 14. `[COMMIT]` feat(worker): Refactor dialectic worker for document-centric generation.
